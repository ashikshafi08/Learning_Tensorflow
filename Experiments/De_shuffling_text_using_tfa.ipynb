{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "De_shuffling_text_using_tfa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnFoWWPnHXTFQRzFNQQ4Gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Experiments/De_shuffling_text_using_tfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT8hEzTT94gW"
      },
      "source": [
        "# De-Scrambling the text with Sequence-to-Sequence with Attention Mechanism. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgl_1hz4kC_k",
        "outputId": "5505b82e-4a7b-473a-80a1-3a19536fb99d"
      },
      "source": [
        "# Downloading tensorflow addons \n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOykrFDM-DQ5",
        "outputId": "bd933b69-ce3e-4cf7-dcbd-48ec1347aab5"
      },
      "source": [
        "!pip install aicrowd-cli\n",
        "API_KEY = '' \n",
        "!aicrowd login --api-key $API_KEY\n",
        "\n",
        "# Downloading the Dataset\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "!aicrowd dataset download --challenge de-shuffling-text -j 3 -o data\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aicrowd-cli in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: toml<1,>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (0.10.2)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.12 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (3.1.17)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5,>=4.56.0 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (4.61.0)\n",
            "Requirement already satisfied: requests<3,>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (2.25.1)\n",
            "Requirement already satisfied: click<8,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (7.1.2)\n",
            "Requirement already satisfied: rich<11,>=10.0.0 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (10.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython<4,>=3.1.12->aicrowd-cli) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython<4,>=3.1.12->aicrowd-cli) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2020.12.5)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (0.4.4)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (2.6.1)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.12->aicrowd-cli) (4.0.0)\n",
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[32mSaved API Key successfully!\u001b[0m\n",
            "val.csv:   0% 0.00/714k [00:00<?, ?B/s]\n",
            "val.csv: 100% 714k/714k [00:00<00:00, 1.91MB/s]\n",
            "\n",
            "train.csv: 100% 7.00M/7.00M [00:00<00:00, 10.9MB/s]\n",
            "test.csv: 100% 1.83M/1.83M [00:00<00:00, 3.17MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8si-SfNIbJWX"
      },
      "source": [
        "# Importing all the packages we need \n",
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLSRI0T35mVK",
        "outputId": "725e1d3e-385a-4fe9-936f-bc5f1104357e"
      },
      "source": [
        "# Importing the data \n",
        "\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "val_data = pd.read_csv('data/val.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Printing out all shapes of our data \n",
        "print(f'Shape of the train data: {train_data.shape}')\n",
        "print(f'Shape of the validation data: {val_data.shape}')\n",
        "print(f'Shape of the test data: {test_data.shape}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train data: (40001, 2)\n",
            "Shape of the validation data: (4001, 2)\n",
            "Shape of the test data: (10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "z2erEbQ95oQN",
        "outputId": "bf49dd7f-e010-414d-f022-f222d60ed0a4"
      },
      "source": [
        "# How does our train data looks like? \n",
        "train_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>presented here Furthermore, naive improved. im...</td>\n",
              "      <td>Furthermore, the naive implementation presente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vector a in a form vector multidimensional spa...</td>\n",
              "      <td>Those coefficients form a vector in a multidim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>compatible of The model with recent is model s...</td>\n",
              "      <td>The model is compatible with a recent model of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>but relevance outlined. hemodynamics its based...</td>\n",
              "      <td>The model is based on electrophysiology, but i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of transitions lever-like involve reorientatio...</td>\n",
              "      <td>Conformational transitions in macromolecular c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  presented here Furthermore, naive improved. im...  Furthermore, the naive implementation presente...\n",
              "1  vector a in a form vector multidimensional spa...  Those coefficients form a vector in a multidim...\n",
              "2  compatible of The model with recent is model s...  The model is compatible with a recent model of...\n",
              "3  but relevance outlined. hemodynamics its based...  The model is based on electrophysiology, but i...\n",
              "4  of transitions lever-like involve reorientatio...  Conformational transitions in macromolecular c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoKo2MLk5p-s",
        "outputId": "6f39b1d0-177f-4f7e-fe12-9b15a2001e95"
      },
      "source": [
        "# Shuffling our train data \n",
        "train_data_shuffled = train_data.sample(frac = 1 , random_state = 42)\n",
        "train_data_shuffled.head() , train_data_shuffled.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                    text                                              label\n",
              " 32824  on work, supervised label image the segmentati...  In our work, we focus on the weakly supervised...\n",
              " 16298  we small of a for set work, In this features i...  In this work, we propose a small set of featur...\n",
              " 30180  ($G_h^{Der}$ to factors the contributes $\\tau_...  The increment of both factors ($G_h^{Der}$ and...\n",
              " 6689   new precise particular, for entailment. bounds...  In particular, we provide new precise analytic...\n",
              " 26893  a these causation Incorporating features, defi...  Incorporating these three features, a definiti...,\n",
              " (40001, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcHm9gXk5tZ3",
        "outputId": "ec9dab71-e014-4d07-a738-599016e2e283"
      },
      "source": [
        "# Splitting sentences and labels\n",
        "train_sentences = train_data_shuffled['text'].to_numpy()\n",
        "train_labels = train_data_shuffled['label'].to_numpy()\n",
        "\n",
        "val_sentences = val_data['text'].to_numpy()\n",
        "val_labels = val_data['label'].to_numpy()\n",
        "\n",
        "test_sentences = test_data['text'].to_numpy()\n",
        "test_labels = test_data['label'].to_numpy()\n",
        "\n",
        "\n",
        "# Checking the shapes \n",
        "print(f'Shape of the train sentences: {train_sentences.shape}')\n",
        "print(f'Shape of the validation sentences: {val_sentences.shape}')\n",
        "print(f'Shape of the train labels: {train_labels.shape}')\n",
        "print(f'Shape of the validation labels: {val_labels.shape}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train sentences: (40001,)\n",
            "Shape of the validation sentences: (4001,)\n",
            "Shape of the train labels: (40001,)\n",
            "Shape of the validation labels: (4001,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIUeh9OA6pEv",
        "outputId": "75169d03-4f52-4fa6-abb6-c7ac5f47b4d6"
      },
      "source": [
        "# Creating a tf.data.dataset of our sentences and labels \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences , train_labels)).shuffle(1000)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences , val_labels))\n",
        "\n",
        "# Adding a batch \n",
        "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>,\n",
              " <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3SUYiC6y-a",
        "outputId": "ba4dbe6f-cac4-41d3-a245-f206370e3ba7"
      },
      "source": [
        "# Looking into our train_dataset just a batch (only 5 first texts in a batch)\n",
        "for scrambled_text , unscrambled_text in train_dataset.take(1):\n",
        "  print(f'Below is the Scrambled version:\\n {scrambled_text[:5]}')\n",
        "  print('\\n----------\\n')\n",
        "  print(f'Below is the Un-Scrambled version:\\n {unscrambled_text[:5]}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below is the Scrambled version:\n",
            " [b'are pignistic through the two subsystems transformation. connected The'\n",
            " b'contribute of layer. a all previous to equally the way, layers output This'\n",
            " b'combine and we subset algorithm popular this In (Au the article, Probab. simulation Beck,'\n",
            " b'valuable this in are situation. particularly Diagnostics'\n",
            " b'Duality (for observations and between observables pixels) established. example, is']\n",
            "\n",
            "----------\n",
            "\n",
            "Below is the Un-Scrambled version:\n",
            " [b'The two subsystems are connected through the pignistic transformation.'\n",
            " b'This way, all previous layers contribute equally to the output of a layer.'\n",
            " b'In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab.'\n",
            " b'Diagnostics are particularly valuable in this situation.'\n",
            " b'Duality between observables (for example, pixels) and observations is established.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZzz0M5DkxIA",
        "outputId": "aa88ce5d-0ba0-41b9-ce41-09b152c769c2"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYV59Klu63r0",
        "outputId": "d95c1a82-34e9-4d7b-a7e6-aef732074b51"
      },
      "source": [
        "# Getting the example input batch annd target batch \n",
        "\n",
        "example_input_batch , example_target_batch = next(iter(train_dataset))\n",
        "\n",
        "example_input_batch.shape , example_target_batch.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64]), TensorShape([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaqJ1YqUkozQ"
      },
      "source": [
        "# Creating text vectorization layer for the scrambled words \n",
        "max_vocab_length = 10000\n",
        "\n",
        "input_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2 , \n",
        "    max_tokens = max_vocab_length \n",
        ")\n",
        "\n",
        "# Fitting on our train sentences (scrambled words )\n",
        "input_text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OikOCXklruc",
        "outputId": "667c01d9-d3e5-4a90-98f6-8c5f086f7433"
      },
      "source": [
        "# First 10 words from the vocabulary \n",
        "input_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH5QvkNMnB9e"
      },
      "source": [
        "# Creating a text vectorization layer for the unscrambled words \n",
        "output_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2, \n",
        "    max_tokens = max_vocab_length\n",
        ")\n",
        "\n",
        "# Fitting on our train labels (unscrambled words)\n",
        "output_text_vectorizer.adapt(train_labels)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWS-W2wdnFLc",
        "outputId": "e8032a32-0fbe-4e94-f343-c4873b799c33"
      },
      "source": [
        "# First 10 words from the vocab \n",
        "output_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWDBxtFInH4L",
        "outputId": "ab32a879-7aa0-4c1b-b0b3-a374a7937131"
      },
      "source": [
        "# Passing a scrambled text (strings) into our layer \n",
        "scrambled_tokens = input_text_vectorizer(scrambled_text)\n",
        "scrambled_tokens[:3]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 29), dtype=int64, numpy=\n",
              "array([[  12,    1,  181,    2,   42,    1,  911, 1226,    2,    1,    1,\n",
              "        3174, 1114,    1,    1,    1,    1,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0],\n",
              "       [2086,    3,  789,    4,  111,  212,    6, 3853,    2,  309,  623,\n",
              "         798,   11,    1,    1,    1,    1,    1, 7779,    1,    1, 8485,\n",
              "           1,    1,    1,    0,    0,    0,    0],\n",
              "       [1632,    8,    9, 2042,   39,  419,   11,    5, 4709,    2,  792,\n",
              "           1,  345,    1,    1,  444,    1,    1,    1,    1,  296,    1,\n",
              "           1, 8553,    1,    1,    1,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMUbzVMPnKIt"
      },
      "source": [
        "# Creating a numpy array of the vocabulary\n",
        "input_vocab = np.array(input_text_vectorizer.get_vocabulary())\n",
        "output_vocab = np.array(output_text_vectorizer.get_vocabulary())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6ybyUAnPfb",
        "outputId": "759d9e34-4e09-4200-bedd-c6b38df164f5"
      },
      "source": [
        "# Indexing our scrambled tokens into the array of vocbulary\n",
        "tokens = input_vocab[scrambled_tokens.numpy()]\n",
        "print(f'Actual sequence:\\n\\n {scrambled_text[:3]}\\n')\n",
        "print(f'\\nThe sequence in tokens:\\n\\n {tokens[:3]}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sequence:\n",
            "\n",
            " [b'are pignistic through the two subsystems transformation. connected The'\n",
            " b'contribute of layer. a all previous to equally the way, layers output This'\n",
            " b'combine and we subset algorithm popular this In (Au the article, Probab. simulation Beck,']\n",
            "\n",
            "\n",
            "The sequence in tokens:\n",
            "\n",
            " [['are' '[UNK]' 'through' 'the' 'two' '[UNK]' 'transformation'\n",
            "  'connected' 'the' '[UNK]' '[UNK]' 'through the' 'the two' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '[UNK]' '' '' '' '' '' '' '' '' '' '' '' '']\n",
            " ['contribute' 'of' 'layer' 'a' 'all' 'previous' 'to' 'equally' 'the'\n",
            "  'way' 'layers' 'output' 'this' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
            "  'previous to' '[UNK]' '[UNK]' 'the way' '[UNK]' '[UNK]' '[UNK]' '' ''\n",
            "  '' '']\n",
            " ['combine' 'and' 'we' 'subset' 'algorithm' 'popular' 'this' 'in' 'au'\n",
            "  'the' 'article' '[UNK]' 'simulation' '[UNK]' '[UNK]' 'and we' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '[UNK]' 'this in' '[UNK]' '[UNK]' 'the article' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '' '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jegfy3ZngCl"
      },
      "source": [
        "# Defining some important parameters \n",
        "inp_vocab_size = len(input_vocab)\n",
        "lab_vocab_size = len(output_vocab) # this will be our label \n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "max_length = 15\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RSpPQi6oVcL"
      },
      "source": [
        "#### Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZAF01-omIW"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self , vocab_size , embedding_dim , enc_units , batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size # batch size\n",
        "    self.enc_units = enc_units # Encoder units / units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim) # the embedding layer\n",
        "\n",
        "    ## LSTM layer in our Encoder \n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units , \n",
        "                                           return_sequences = True , \n",
        "                                           return_state = True , \n",
        "                                           recurrent_initializer = 'glorot_uniform')\n",
        "    \n",
        "  def call(self , x , hidden):\n",
        "    x = self.embedding(x) # \n",
        "    output , h , c = self.lstm_layer(x , initial_state = hidden)\n",
        "    return output , h , c\n",
        "\n",
        "  def initialize_hidden_state(self): \n",
        "    return [tf.zeros((self.batch_size , self.enc_units)) , tf.zeros((self.batch_size , self.enc_units))]\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdYAwu4-Cp92"
      },
      "source": [
        "- hidden_state --> output of the lstm layer\n",
        "\n",
        "[Difference between return state and return sequence](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/#:~:text=The%20output%20of%20an%20LSTM,the%20cell%20state%2C%20or%20c.&text=The%20LSTM%20hidden%20state%20output,last%20time%20step%20(again).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lYYBx49COYb"
      },
      "source": [
        "dum = tf.zeros((64 , 1024))\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLN9FhMuCTiJ"
      },
      "source": [
        "# Test the Encoder layer we built \n",
        "encoder = Encoder(vocab_size = inp_vocab_size ,\n",
        "                  embedding_dim = embedding_dim , \n",
        "                  enc_units = units , \n",
        "                  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9e08fMGDvOs",
        "outputId": "4d985c07-e149-4744-af57-504ec64edf53"
      },
      "source": [
        "# This will initialize the hidden state of our lstm layer \n",
        "encoder.initialize_hidden_state()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKtOO4r1D3zs",
        "outputId": "8bfff507-deb6-4bcc-c9d5-d53f60c37f62"
      },
      "source": [
        "sample_hidden = encoder.initialize_hidden_state()  # Sample input\n",
        "\n",
        "# Apply the text vectorizer and turn the sequence into tokens before passing into Encoder\n",
        "sample_output , sample_h , sample_c = encoder(input_text_vectorizer(example_input_batch) , sample_hidden)\n",
        "\n",
        "print(f'Encoder output shape: (batch size , sequence length , units) --> {sample_output.shape}')\n",
        "print(f'Encoder h vector shape: (batch_size , units) --> {sample_h.shape}')\n",
        "print(f'Encoder c vector shape: (batch_size , units) --> {sample_c.shape}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size , sequence length , units) --> (64, 29, 1024)\n",
            "Encoder h vector shape: (batch_size , units) --> (64, 1024)\n",
            "Encoder c vector shape: (batch_size , units) --> (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v36CFEq3HBkT"
      },
      "source": [
        "#### Decoder \n",
        "\n",
        "Some threads I used to refer: \n",
        "- https://stackoverflow.com/questions/48187283/whats-the-difference-between-lstm-and-lstmcell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r8AKpqQHPFF"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size , embedding_dim , dec_units , batch_size , attention_type = 'luong'):\n",
        "    super(Decoder , self).__init__()\n",
        "    self.batch_size = batch_size \n",
        "    self.dec_units = dec_units \n",
        "    self.attention_type = attention_type \n",
        "\n",
        "    # Embedding layer \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim)\n",
        "\n",
        "    # Final Dense layer where softmax will be applied (fully connected layer)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "\n",
        "    # Sampler \n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler() \n",
        " \n",
        "  def build_attention_mechanism(self , dec_units , memory , memory_sequence_length , attention_type = 'luong'):\n",
        "    '''\n",
        "    1. attention_type --> Which sort of attention (Bahdanau , Luong)\n",
        "    2. dec_units: Final dimension of attention outputs (Decoder units)\n",
        "    3. memory: Encoder hidden states of shape (batch_size , max_length_inputs , enc_units)\n",
        "    4. memory_sequence_length: 1D array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
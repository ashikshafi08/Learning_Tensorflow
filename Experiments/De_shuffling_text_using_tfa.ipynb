{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "De_shuffling_text_using_tfa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgoVEqhkqkkY3ddSe+tA+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Experiments/De_shuffling_text_using_tfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT8hEzTT94gW"
      },
      "source": [
        "# De-Scrambling the text with Sequence-to-Sequence with Attention Mechanism. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgl_1hz4kC_k",
        "outputId": "01a06831-46db-46ea-9e72-baa0788211e7"
      },
      "source": [
        "# Downloading tensorflow addons \n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOykrFDM-DQ5",
        "outputId": "125af2d2-bcc5-4dd5-e28e-9808ce9cff7a"
      },
      "source": [
        "!pip install aicrowd-cli\n",
        "API_KEY = 'b0fd3331ed02024ed40b448baf316d82' \n",
        "!aicrowd login --api-key $API_KEY\n",
        "\n",
        "# Downloading the Dataset\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "!aicrowd dataset download --challenge de-shuffling-text -j 3 -o data\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting aicrowd-cli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/57/59b5a00c6e90c9cc028b3da9dff90e242ad2847e735b1a0e81a21c616e27/aicrowd_cli-0.1.7-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hCollecting tqdm<5,>=4.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/20/9f1e974bb4761128fc0d0a32813eaa92827309b1756c4b892d28adfb4415/tqdm-4.61.1-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (7.1.2)\n",
            "Collecting requests-toolbelt<1,>=0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml<1,>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (0.10.2)\n",
            "Collecting gitpython<4,>=3.1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 17.8MB/s \n",
            "\u001b[?25hCollecting rich<11,>=10.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/32/eb8aadb1ed791081e5c773bd1dfa15f1a71788fbeda37b12f837f2b1999b/rich-10.3.0-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 38.0MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython<4,>=3.1.12->aicrowd-cli) (3.7.4.3)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/92/dfd892312d822f36c55366118b95d914e5f16de11044a27cf10a7d71bbbf/commonmark-0.9.1-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (1.24.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm, requests, requests-toolbelt, smmap, gitdb, gitpython, colorama, commonmark, rich, aicrowd-cli\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed aicrowd-cli-0.1.7 colorama-0.4.4 commonmark-0.9.1 gitdb-4.0.7 gitpython-3.1.17 requests-2.25.1 requests-toolbelt-0.9.1 rich-10.3.0 smmap-4.0.0 tqdm-4.61.1\n",
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[32mSaved API Key successfully!\u001b[0m\n",
            "test.csv:   0% 0.00/1.83M [00:00<?, ?B/s]\n",
            "train.csv:   0% 0.00/7.00M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "test.csv: 100% 1.83M/1.83M [00:00<00:00, 3.71MB/s]\n",
            "\n",
            "\n",
            "val.csv: 100% 714k/714k [00:00<00:00, 1.86MB/s]\n",
            "\n",
            "train.csv: 100% 7.00M/7.00M [00:00<00:00, 10.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8si-SfNIbJWX"
      },
      "source": [
        "# Importing all the packages we need \n",
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLSRI0T35mVK",
        "outputId": "8192e712-6347-4400-e9e4-885380e85023"
      },
      "source": [
        "# Importing the data \n",
        "\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "val_data = pd.read_csv('data/val.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Printing out all shapes of our data \n",
        "print(f'Shape of the train data: {train_data.shape}')\n",
        "print(f'Shape of the validation data: {val_data.shape}')\n",
        "print(f'Shape of the test data: {test_data.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train data: (40001, 2)\n",
            "Shape of the validation data: (4001, 2)\n",
            "Shape of the test data: (10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "z2erEbQ95oQN",
        "outputId": "d530e304-30b7-4784-89e6-45d640f4fd8e"
      },
      "source": [
        "# How does our train data looks like? \n",
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>presented here Furthermore, naive improved. im...</td>\n",
              "      <td>Furthermore, the naive implementation presente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vector a in a form vector multidimensional spa...</td>\n",
              "      <td>Those coefficients form a vector in a multidim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>compatible of The model with recent is model s...</td>\n",
              "      <td>The model is compatible with a recent model of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>but relevance outlined. hemodynamics its based...</td>\n",
              "      <td>The model is based on electrophysiology, but i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of transitions lever-like involve reorientatio...</td>\n",
              "      <td>Conformational transitions in macromolecular c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  presented here Furthermore, naive improved. im...  Furthermore, the naive implementation presente...\n",
              "1  vector a in a form vector multidimensional spa...  Those coefficients form a vector in a multidim...\n",
              "2  compatible of The model with recent is model s...  The model is compatible with a recent model of...\n",
              "3  but relevance outlined. hemodynamics its based...  The model is based on electrophysiology, but i...\n",
              "4  of transitions lever-like involve reorientatio...  Conformational transitions in macromolecular c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoKo2MLk5p-s",
        "outputId": "c0dd70e1-57ef-4d6b-b4fe-02f7f9b9adb2"
      },
      "source": [
        "# Shuffling our train data \n",
        "train_data_shuffled = train_data.sample(frac = 1 , random_state = 42)\n",
        "train_data_shuffled.head() , train_data_shuffled.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                    text                                              label\n",
              " 32824  on work, supervised label image the segmentati...  In our work, we focus on the weakly supervised...\n",
              " 16298  we small of a for set work, In this features i...  In this work, we propose a small set of featur...\n",
              " 30180  ($G_h^{Der}$ to factors the contributes $\\tau_...  The increment of both factors ($G_h^{Der}$ and...\n",
              " 6689   new precise particular, for entailment. bounds...  In particular, we provide new precise analytic...\n",
              " 26893  a these causation Incorporating features, defi...  Incorporating these three features, a definiti...,\n",
              " (40001, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcHm9gXk5tZ3",
        "outputId": "c1c4f786-51e8-42cd-fe20-8d84a7def15d"
      },
      "source": [
        "# Splitting sentences and labels\n",
        "train_sentences = train_data_shuffled['text'].to_numpy()\n",
        "train_labels = train_data_shuffled['label'].to_numpy()\n",
        "\n",
        "val_sentences = val_data['text'].to_numpy()\n",
        "val_labels = val_data['label'].to_numpy()\n",
        "\n",
        "test_sentences = test_data['text'].to_numpy()\n",
        "test_labels = test_data['label'].to_numpy()\n",
        "\n",
        "\n",
        "# Checking the shapes \n",
        "print(f'Shape of the train sentences: {train_sentences.shape}')\n",
        "print(f'Shape of the validation sentences: {val_sentences.shape}')\n",
        "print(f'Shape of the train labels: {train_labels.shape}')\n",
        "print(f'Shape of the validation labels: {val_labels.shape}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train sentences: (40001,)\n",
            "Shape of the validation sentences: (4001,)\n",
            "Shape of the train labels: (40001,)\n",
            "Shape of the validation labels: (4001,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIUeh9OA6pEv",
        "outputId": "7cbbc26d-8735-4cbc-a5c2-bd788330a8a6"
      },
      "source": [
        "# Creating a tf.data.dataset of our sentences and labels \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences , train_labels)).shuffle(1000)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences , val_labels))\n",
        "\n",
        "# Adding a batch \n",
        "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>,\n",
              " <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3SUYiC6y-a",
        "outputId": "e3622e9f-5cbe-419c-9331-2405c7498c70"
      },
      "source": [
        "# Looking into our train_dataset just a batch (only 5 first texts in a batch)\n",
        "for scrambled_text , unscrambled_text in train_dataset.take(1):\n",
        "  print(f'Below is the Scrambled version:\\n {scrambled_text[:5]}')\n",
        "  print('\\n----------\\n')\n",
        "  print(f'Below is the Un-Scrambled version:\\n {unscrambled_text[:5]}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below is the Scrambled version:\n",
            " [b'introns. depends that of assume the length the exonization process We on'\n",
            " b'types combines proposed The approach of information. two different'\n",
            " b'(SPIC). algorithm, this We of SPI SPI call Continuous the variation'\n",
            " b'data), could and personalized. be estimation/tracking which human such as highly pose small behavior'\n",
            " b'problem. enhancement computer resolution classical is image vision Low a']\n",
            "\n",
            "----------\n",
            "\n",
            "Below is the Un-Scrambled version:\n",
            " [b'We assume that the exonization process depends on the length of introns.'\n",
            " b'The proposed approach combines two different types of information.'\n",
            " b'We call this variation of the SPI algorithm, SPI Continuous (SPIC).'\n",
            " b'small data), such as human pose and behavior estimation/tracking which could be highly personalized.'\n",
            " b'Low resolution image enhancement is a classical computer vision problem.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZzz0M5DkxIA",
        "outputId": "d00ac11c-e3c6-4f7e-9484-61a40a45d670"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYV59Klu63r0",
        "outputId": "7438cf20-abe0-4c6c-b7d1-7d91322dcb4d"
      },
      "source": [
        "# Getting the example input batch annd target batch \n",
        "\n",
        "example_input_batch , example_target_batch = next(iter(train_dataset))\n",
        "\n",
        "example_input_batch.shape , example_target_batch.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64]), TensorShape([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaqJ1YqUkozQ"
      },
      "source": [
        "# Creating text vectorization layer for the scrambled words \n",
        "max_vocab_length = 10000\n",
        "\n",
        "input_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2 , \n",
        "    max_tokens = max_vocab_length \n",
        ")\n",
        "\n",
        "# Fitting on our train sentences (scrambled words )\n",
        "input_text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OikOCXklruc",
        "outputId": "62665907-c837-4222-e601-3d85c7a0fb1f"
      },
      "source": [
        "# First 10 words from the vocabulary \n",
        "input_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH5QvkNMnB9e"
      },
      "source": [
        "# Creating a text vectorization layer for the unscrambled words \n",
        "output_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2, \n",
        "    max_tokens = max_vocab_length\n",
        ")\n",
        "\n",
        "# Fitting on our train labels (unscrambled words)\n",
        "output_text_vectorizer.adapt(train_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWS-W2wdnFLc",
        "outputId": "7989a641-0228-4bd7-fc63-5b941412d65d"
      },
      "source": [
        "# First 10 words from the vocab \n",
        "output_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWDBxtFInH4L",
        "outputId": "6457e20c-a0d6-4467-bbeb-2cdda7a5f6a8"
      },
      "source": [
        "# Passing a scrambled text (strings) into our layer \n",
        "scrambled_tokens = input_text_vectorizer(scrambled_text)\n",
        "scrambled_tokens[:3]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 29), dtype=int64, numpy=\n",
              "array([[   1, 1036,   14,    3, 1605,    2, 1822,    2,    1,  117,    9,\n",
              "          13,    1,    1, 1583,    1,    1,    1,    1,    1,    1, 7771,\n",
              "         745,    0,    0,    0,    0,    0,    0],\n",
              "       [ 468, 1517,   23,    2,   31,    3,   66,   42,   56,    1,    1,\n",
              "         406,  857, 1451, 2884,    1, 6242,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0],\n",
              "       [   1,   39,   11,    9,    3,    1,    1,  980, 1066,    2,  761,\n",
              "           1, 2837,  378,  297,    1,    1,    1,    1,    1,    1,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMUbzVMPnKIt"
      },
      "source": [
        "# Creating a numpy array of the vocabulary\n",
        "input_vocab = np.array(input_text_vectorizer.get_vocabulary())\n",
        "output_vocab = np.array(output_text_vectorizer.get_vocabulary())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6ybyUAnPfb",
        "outputId": "23c61612-20d6-4337-b822-a05881939f8b"
      },
      "source": [
        "# Indexing our scrambled tokens into the array of vocbulary\n",
        "tokens = input_vocab[scrambled_tokens.numpy()]\n",
        "print(f'Actual sequence:\\n\\n {scrambled_text[:3]}\\n')\n",
        "print(f'\\nThe sequence in tokens:\\n\\n {tokens[:3]}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sequence:\n",
            "\n",
            " [b'introns. depends that of assume the length the exonization process We on'\n",
            " b'types combines proposed The approach of information. two different'\n",
            " b'(SPIC). algorithm, this We of SPI SPI call Continuous the variation']\n",
            "\n",
            "\n",
            "The sequence in tokens:\n",
            "\n",
            " [['[UNK]' 'depends' 'that' 'of' 'assume' 'the' 'length' 'the' '[UNK]'\n",
            "  'process' 'we' 'on' '[UNK]' '[UNK]' 'that of' '[UNK]' '[UNK]' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '[UNK]' 'process we' 'we on' '' '' '' '' '' '']\n",
            " ['types' 'combines' 'proposed' 'the' 'approach' 'of' 'information' 'two'\n",
            "  'different' '[UNK]' '[UNK]' 'proposed the' 'the approach' 'approach of'\n",
            "  'of information' '[UNK]' 'two different' '' '' '' '' '' '' '' '' '' ''\n",
            "  '' '']\n",
            " ['[UNK]' 'algorithm' 'this' 'we' 'of' '[UNK]' '[UNK]' 'call'\n",
            "  'continuous' 'the' 'variation' '[UNK]' 'algorithm this' 'this we'\n",
            "  'we of' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '' '' '' '' ''\n",
            "  '' '' '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jegfy3ZngCl"
      },
      "source": [
        "# Defining some important parameters \n",
        "inp_vocab_size = len(input_vocab)\n",
        "lab_vocab_size = len(output_vocab) # this will be our label \n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "max_length = 15\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RSpPQi6oVcL"
      },
      "source": [
        "#### Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZAF01-omIW"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self , vocab_size , embedding_dim , enc_units , batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size # batch size\n",
        "    self.enc_units = enc_units # Encoder units / units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim) # the embedding layer\n",
        "\n",
        "    ## LSTM layer in our Encoder \n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units , \n",
        "                                           return_sequences = True , \n",
        "                                           return_state = True , \n",
        "                                           recurrent_initializer = 'glorot_uniform')\n",
        "    \n",
        "  def call(self , x , hidden):\n",
        "    x = self.embedding(x) # \n",
        "    output , h , c = self.lstm_layer(x , initial_state = hidden)\n",
        "    return output , h , c\n",
        "\n",
        "  def initialize_hidden_state(self): \n",
        "    return [tf.zeros((self.batch_size , self.enc_units)) , tf.zeros((self.batch_size , self.enc_units))]\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdYAwu4-Cp92"
      },
      "source": [
        "- hidden_state --> output of the lstm layer\n",
        "\n",
        "[Difference between return state and return sequence](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/#:~:text=The%20output%20of%20an%20LSTM,the%20cell%20state%2C%20or%20c.&text=The%20LSTM%20hidden%20state%20output,last%20time%20step%20(again).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lYYBx49COYb"
      },
      "source": [
        "dum = tf.zeros((64 , 1024))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLN9FhMuCTiJ"
      },
      "source": [
        "# Test the Encoder layer we built \n",
        "encoder = Encoder(vocab_size = inp_vocab_size ,\n",
        "                  embedding_dim = embedding_dim , \n",
        "                  enc_units = units , \n",
        "                  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9e08fMGDvOs",
        "outputId": "ac94ee94-5154-427d-fab4-a0537fafc0db"
      },
      "source": [
        "# This will initialize the hidden state of our lstm layer \n",
        "encoder.initialize_hidden_state()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKtOO4r1D3zs",
        "outputId": "1ca20964-db95-40bf-80f1-d6fad726d9e3"
      },
      "source": [
        "sample_hidden = encoder.initialize_hidden_state()  # Sample input\n",
        "\n",
        "# Apply the text vectorizer and turn the sequence into tokens before passing into Encoder\n",
        "sample_output , sample_h , sample_c = encoder(input_text_vectorizer(example_input_batch) , sample_hidden)\n",
        "\n",
        "print(f'Encoder output shape: (batch size , sequence length , units) --> {sample_output.shape}')\n",
        "print(f'Encoder h vector shape: (batch_size , units) --> {sample_h.shape}')\n",
        "print(f'Encoder c vector shape: (batch_size , units) --> {sample_c.shape}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size , sequence length , units) --> (64, 29, 1024)\n",
            "Encoder h vector shape: (batch_size , units) --> (64, 1024)\n",
            "Encoder c vector shape: (batch_size , units) --> (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v36CFEq3HBkT"
      },
      "source": [
        "#### Decoder \n",
        "\n",
        "Some threads I used to refer: \n",
        "- https://stackoverflow.com/questions/48187283/whats-the-difference-between-lstm-and-lstmcell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r8AKpqQHPFF"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size , embedding_dim , dec_units , batch_size , attention_type = 'luong'):\n",
        "    super(Decoder , self).__init__()\n",
        "    self.batch_size = batch_size \n",
        "    self.dec_units = dec_units \n",
        "    self.attention_type = attention_type \n",
        "\n",
        "    # Embedding layer \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size , embedding_dim)\n",
        "\n",
        "    # Final Dense layer where softmax will be applied (fully connected layer)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "\n",
        "    # Sampler \n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler() \n",
        "\n",
        "    # Create attention mechanism with the memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units , \n",
        "                                                              None , \n",
        "                                                              self.batch_size*[max_length], self.attention_type)\n",
        "    \n",
        "    # Define the decoder with respect to fundamental rnn cell \n",
        "    self.rnn_cell = self.build_rnn_cell(batch_size)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell \n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell , sampler = self.sampler, \n",
        "                                            output_layer = self.fc)\n",
        "    \n",
        " \n",
        "  def build_attention_mechanism(self , dec_units , memory , memory_sequence_length , attention_type = 'luong'):\n",
        "    '''\n",
        "    1. attention_type --> Which sort of attention (Bahdanau , Luong)\n",
        "    2. dec_units: Final dimension of attention outputs (Decoder units)\n",
        "    3. memory: Encoder hidden states of shape (batch_size , max_length_inputs , enc_units)\n",
        "    4. memory_sequence_length: 1D array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "    '''\n",
        "\n",
        "    if (attention_type == 'bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units = dec_units , \n",
        "                                           memory = memory , \n",
        "                                           memory_sequence_length = memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units = dec_units , memory = memory , memory_sequence_length= memory_sequence_length)  \n",
        "\n",
        "  def build_rnn_cell(self , batch_size):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell , \n",
        "                                            self.attention_mechanism, \n",
        "                                            attention_layer_size = self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_initial_state(self , batch_size , encoder_state , Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size , dtype = Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "  def call(self , inputs , initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _ , _ = self.decoder(x , initial_state = initial_state , sequence_length = self.batch_size*[max_length -1])\n",
        "    return outputs"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InTEBeOkIhYn"
      },
      "source": [
        "# Setting up our decoder \n",
        "decoder = Decoder(vocab_size= lab_vocab_size , \n",
        "                  embedding_dim = embedding_dim , \n",
        "                  dec_units = units , \n",
        "                  batch_size = BATCH_SIZE , \n",
        "                  attention_type = 'luong')\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiLlU1DJJVb6",
        "outputId": "83a5ba1c-9c3b-4468-ae9b-e0fe139cb4d9"
      },
      "source": [
        "# Sample sequence to be passed inside the decoder\n",
        "sample_x = tf.random.uniform((BATCH_SIZE , max_length))\n",
        "sample_x[:1]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=float32, numpy=\n",
              "array([[0.5982404 , 0.5174072 , 0.7417691 , 0.98033535, 0.20018566,\n",
              "        0.58651006, 0.58131933, 0.4040811 , 0.8046969 , 0.95714545,\n",
              "        0.75586677, 0.83635545, 0.7425797 , 0.15451884, 0.5775627 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrsMgpzvJd63"
      },
      "source": [
        "# Passing the output of our encoder into the decoder attention\n",
        "decoder.attention_mechanism.setup_memory(sample_output)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0x6IbW2Jvzu",
        "outputId": "67a88d76-f2ae-42d7-f1f6-49c23ec17683"
      },
      "source": [
        "# Getting the initial state \n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE , [sample_h , sample_c] , tf.float32)\n",
        "initial_state[:2]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "  array([[-0.00332343,  0.00951187,  0.00872059, ...,  0.00271745,\n",
              "           0.00340977,  0.00204801],\n",
              "         [-0.0031446 ,  0.00791592,  0.00978961, ..., -0.00080166,\n",
              "          -0.00122762, -0.0031764 ],\n",
              "         [-0.00295936,  0.00692794,  0.00766462, ..., -0.00340543,\n",
              "          -0.00272821, -0.00033349],\n",
              "         ...,\n",
              "         [-0.00570125,  0.00837591,  0.00742682, ...,  0.00542367,\n",
              "           0.00557165,  0.00188923],\n",
              "         [-0.00610816,  0.00962569,  0.00756236, ...,  0.0036595 ,\n",
              "           0.00469161,  0.00099883],\n",
              "         [-0.002564  ,  0.00709025,  0.00763696, ...,  0.0014222 ,\n",
              "           0.00215575,  0.00052292]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "  array([[-0.00675011,  0.01907639,  0.01731787, ...,  0.0054781 ,\n",
              "           0.00679825,  0.00406175],\n",
              "         [-0.0063671 ,  0.01584223,  0.01940845, ..., -0.00161185,\n",
              "          -0.00245117, -0.00630689],\n",
              "         [-0.00598931,  0.01384946,  0.01521701, ..., -0.00685051,\n",
              "          -0.00545059, -0.00066173],\n",
              "         ...,\n",
              "         [-0.01157072,  0.01678718,  0.01475113, ...,  0.01094579,\n",
              "           0.01108454,  0.0037389 ],\n",
              "         [-0.01240768,  0.01929738,  0.01501421, ...,  0.00738835,\n",
              "           0.00934096,  0.00197862],\n",
              "         [-0.00519809,  0.01419957,  0.0151566 , ...,  0.00286173,\n",
              "           0.00429971,  0.00103716]], dtype=float32)>],\n",
              " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLS-vIUpK2h-",
        "outputId": "7366bcf1-8e0f-4e79-deac-44accf26e6c8"
      },
      "source": [
        "# Getting the decoder outputs \n",
        "sample_decoder_outputs = decoder(sample_x , initial_state)\n",
        "print(f'Decoder output shape: {sample_decoder_outputs.rnn_output.shape}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (64, 14, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ARJiLNxLG9U"
      },
      "source": [
        "### Define the optimizer and the loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B6z_z4qP7no"
      },
      "source": [
        "# We're going to use the Adam optimizer \n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEDr_UzuRjfD",
        "outputId": "82cb9e4c-b235-431c-ff5e-fcb8f6f8d2ed"
      },
      "source": [
        "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
        "steps_per_epoch "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVSNMfcgQAL7"
      },
      "source": [
        "# Creating a function for our loss function \n",
        "def loss_function(real , pred):\n",
        "  '''\n",
        "  Target shape = (Batch size , max_length) --> (64 , 15)\n",
        "  Prediction shape = (batch size , max_length , lab_vocab_size)\n",
        "  '''\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True , reduction = 'none')\n",
        "  loss = cross_entropy(y_true = real , y_pred = pred)\n",
        "  # Output 0 for y = 0 else output 1\n",
        "  mask = tf.logical_not(tf.math.equal(real , 0)) \n",
        "  mask = tf.cast(mask , dtype = loss.dtype)\n",
        "  loss = mask*loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBH8on1IR011"
      },
      "source": [
        "### Creating a checkpoint \n",
        "\n",
        "This checkpoint will keep track of, \n",
        "- Optimizer \n",
        "- Encoder \n",
        "- Decoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmJ1d2lSCpO"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir , 'ckpt')\n",
        "\n",
        "# Initializing our checkpoint\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer , \n",
        "                                 encoder = encoder , \n",
        "                                 decoder = decoder)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsH7ClRlSfAc"
      },
      "source": [
        "### One `train_step` operations \n",
        "\n",
        "Creating a customizable train function. \n",
        "\n",
        "Have to read this https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "C_s93OCDUAMq",
        "outputId": "84c9efb2-74a3-472a-a6ce-c3778191376c"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp , targ , enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output , enc_h , enc_c = encoder(inp , enc_hidden)\n",
        "    \n",
        "    # Decoder input\n",
        "    dec_input = targ[: , :-1] # Ignore <end> token\n",
        "    real = targ[: , 1:] # Ignore <start> token\n",
        "\n",
        "    # Set the Attention Mechanism object with encoder_outputs \n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder \n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE , [enc_h , enc_c] , tf.float32)\n",
        "    \n",
        "    # Getting the prediction from the decoder \n",
        "    pred = decoder(dec_input , decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real , logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss , variables)\n",
        "  optimizer.apply_gradients(zip(gradients , variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-83-0a2f4d9ebe7c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def train_step(input_text_vectorizer(inp) , output_text_vectorizer(targ) , enc_hidden):\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu_-dMV2Vj2M"
      },
      "source": [
        "## Train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C1O_9-xkVnxZ",
        "outputId": "7e1236eb-38a1-4c6a-8b25-f732ed8b9138"
      },
      "source": [
        "import time\n",
        "EPOCHS  = 10\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  print(enc_hidden[0].shape , enc_hidden[1].shape)\n",
        "\n",
        "  for (batch , (inp , targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp , targ , enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch{batch} Loss{batch_loss.numpy()}\\n')\n",
        "\n",
        "  # Saving the model checkpoints for every 2 epochs \n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix= checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch}\\n')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1024) (64, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-9775adb4f302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput_text_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  assertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [64 28] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [64 14]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at <ipython-input-60-8065f7b606bc>:8) ]] [Op:__inference_train_step_248535]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert:\n assert_equal_9/Assert/Assert (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:1406)\n\nFunction call stack:\ntrain_step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3TPNw-ZYS9T",
        "outputId": "f4843331-5f48-40e3-e884-eba6268c313b"
      },
      "source": [
        "test_enc_hidden = encoder.initialize_hidden_state()\n",
        "test_enc_hidden"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXy6l_sFXprT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qthexN64YmAV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "De-Scrambling_Text_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTgVM2ETPr5tKwjG64UvWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Experiments/De_Scrambling_Text_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu8GiMfGJ_7G"
      },
      "source": [
        "# Converting Scrambled sequence into a Unscrambled sequence using attention. \n",
        "\n",
        "Reference: https://www.tensorflow.org/text/tutorials/nmt_with_attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXrtEA9lGzCW",
        "outputId": "6b6a301a-be42-4781-f30e-b40d96dcd9f5"
      },
      "source": [
        "!pip install aicrowd-cli\n",
        "API_KEY = '' \n",
        "!aicrowd login --api-key $API_KEY\n",
        "\n",
        "# Downloading the Dataset\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "!aicrowd dataset download --challenge de-shuffling-text -j 3 -o data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting aicrowd-cli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/57/59b5a00c6e90c9cc028b3da9dff90e242ad2847e735b1a0e81a21c616e27/aicrowd_cli-0.1.7-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (7.1.2)\n",
            "Collecting rich<11,>=10.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/32/eb8aadb1ed791081e5c773bd1dfa15f1a71788fbeda37b12f837f2b1999b/rich-10.3.0-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 8.2MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt<1,>=0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml<1,>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (0.10.2)\n",
            "Collecting tqdm<5,>=4.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/d7/f357d98e9b50346bcb6095fe3ad205d8db3174eb5edb03edfe7c4099576d/tqdm-4.61.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.7MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[?25hCollecting gitpython<4,>=3.1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (3.7.4.3)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/92/dfd892312d822f36c55366118b95d914e5f16de11044a27cf10a7d71bbbf/commonmark-0.9.1-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: colorama, commonmark, rich, requests, requests-toolbelt, tqdm, smmap, gitdb, gitpython, aicrowd-cli\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed aicrowd-cli-0.1.7 colorama-0.4.4 commonmark-0.9.1 gitdb-4.0.7 gitpython-3.1.17 requests-2.25.1 requests-toolbelt-0.9.1 rich-10.3.0 smmap-4.0.0 tqdm-4.61.0\n",
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[32mSaved API Key successfully!\u001b[0m\n",
            "train.csv:   0% 0.00/7.00M [00:00<?, ?B/s]\n",
            "train.csv: 100% 7.00M/7.00M [00:00<00:00, 10.9MB/s]\n",
            "val.csv:   0% 0.00/714k [00:00<?, ?B/s]\n",
            "test.csv: 100% 1.83M/1.83M [00:00<00:00, 3.64MB/s]\n",
            "val.csv: 100% 714k/714k [00:00<00:00, 1.78MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVPH-jwHIONU"
      },
      "source": [
        "# Importing all the packages we need \n",
        "import tensorflow as tf \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ZJq4reN10H",
        "outputId": "b785561d-fc11-4795-e60b-334de391b9c7"
      },
      "source": [
        "# Importing the data \n",
        "\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "val_data = pd.read_csv('data/val.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Printing out all shapes of our data \n",
        "print(f'Shape of the train data: {train_data.shape}')\n",
        "print(f'Shape of the validation data: {val_data.shape}')\n",
        "print(f'Shape of the test data: {test_data.shape}')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train data: (40001, 2)\n",
            "Shape of the validation data: (4001, 2)\n",
            "Shape of the test data: (10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "aiYByVBPN6bS",
        "outputId": "9ae982c2-727f-45f4-a577-3ad5b1ef5673"
      },
      "source": [
        "# How does our train data looks like? \n",
        "train_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>presented here Furthermore, naive improved. im...</td>\n",
              "      <td>Furthermore, the naive implementation presente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vector a in a form vector multidimensional spa...</td>\n",
              "      <td>Those coefficients form a vector in a multidim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>compatible of The model with recent is model s...</td>\n",
              "      <td>The model is compatible with a recent model of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>but relevance outlined. hemodynamics its based...</td>\n",
              "      <td>The model is based on electrophysiology, but i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of transitions lever-like involve reorientatio...</td>\n",
              "      <td>Conformational transitions in macromolecular c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  presented here Furthermore, naive improved. im...  Furthermore, the naive implementation presente...\n",
              "1  vector a in a form vector multidimensional spa...  Those coefficients form a vector in a multidim...\n",
              "2  compatible of The model with recent is model s...  The model is compatible with a recent model of...\n",
              "3  but relevance outlined. hemodynamics its based...  The model is based on electrophysiology, but i...\n",
              "4  of transitions lever-like involve reorientatio...  Conformational transitions in macromolecular c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgk0Pc6-Woxl",
        "outputId": "37c8cb56-b9d3-4c54-f451-f9b8ff5de2fe"
      },
      "source": [
        "# Shuffling our train data \n",
        "train_data_shuffled = train_data.sample(frac = 1 , random_state = 42)\n",
        "train_data_shuffled.head() , train_data_shuffled.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                    text                                              label\n",
              " 32824  on work, supervised label image the segmentati...  In our work, we focus on the weakly supervised...\n",
              " 16298  we small of a for set work, In this features i...  In this work, we propose a small set of featur...\n",
              " 30180  ($G_h^{Der}$ to factors the contributes $\\tau_...  The increment of both factors ($G_h^{Der}$ and...\n",
              " 6689   new precise particular, for entailment. bounds...  In particular, we provide new precise analytic...\n",
              " 26893  a these causation Incorporating features, defi...  Incorporating these three features, a definiti...,\n",
              " (40001, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc3LZNjiN_Ck",
        "outputId": "548cf8c2-b15e-4d47-b364-895ecc57c095"
      },
      "source": [
        "# Splitting sentences and labels\n",
        "train_sentences = train_data_shuffled['text'].to_numpy()\n",
        "train_labels = train_data_shuffled['label'].to_numpy()\n",
        "\n",
        "val_sentences = val_data['text'].to_numpy()\n",
        "val_labels = val_data['label'].to_numpy()\n",
        "\n",
        "test_sentences = test_data['text'].to_numpy()\n",
        "test_labels = test_data['label'].to_numpy()\n",
        "\n",
        "\n",
        "# Checking the shapes \n",
        "print(f'Shape of the train sentences: {train_sentences.shape}')\n",
        "print(f'Shape of the validation sentences: {val_sentences.shape}')\n",
        "print(f'Shape of the train labels: {train_labels.shape}')\n",
        "print(f'Shape of the validation labels: {val_labels.shape}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train sentences: (40001,)\n",
            "Shape of the validation sentences: (4001,)\n",
            "Shape of the train labels: (40001,)\n",
            "Shape of the validation labels: (4001,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bz8V1gpa7sN",
        "outputId": "9702cdbe-3025-45ef-8184-224b46643f15"
      },
      "source": [
        "# Creating a tf.data.dataset of our sentences and labels \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences , train_labels)).shuffle(1000)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences , val_labels))\n",
        "\n",
        "# Adding a batch \n",
        "train_dataset = train_dataset.batch(64)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>,\n",
              " <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAWBS79bLVM",
        "outputId": "0d440542-771b-4a04-b3b0-f2d1bdb0a605"
      },
      "source": [
        "# Looking into our train_dataset just a batch (only 5 first texts in a batch)\n",
        "for scrambled_text , unscrambled_text in train_dataset.take(1):\n",
        "  print(f'Below is the Scrambled version:\\n {scrambled_text[:5]}')\n",
        "  print('\\n----------\\n')\n",
        "  print(f'Below is the Un-Scrambled version:\\n {unscrambled_text[:5]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below is the Scrambled version:\n",
            " [b'to impaired loss processing information whether a information of transfer. local leads Therefore, we asked'\n",
            " b'with common Our approach issues a within adaptive proposed deals framework. these regularized'\n",
            " b'a about consider model We causal system, look for a facts. arguments and physical for'\n",
            " b'similar curves. properties The new have curves to $q$-B$\\\\acute{e}$zier some'\n",
            " b'generate cGAN the the sharper feedback from helps The first second cGAN images.']\n",
            "\n",
            "----------\n",
            "\n",
            "Below is the Un-Scrambled version:\n",
            " [b'Therefore, we asked whether impaired local information processing leads to a loss of information transfer.'\n",
            " b'Our proposed adaptive regularized approach deals with these issues within a common framework.'\n",
            " b'We consider a causal model for a physical system, and look for arguments about facts.'\n",
            " b'The new curves have some properties similar to $q$-B$\\\\acute{e}$zier curves.'\n",
            " b'The feedback from the second cGAN helps the first cGAN generate sharper images.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MveFqAiWd3s7"
      },
      "source": [
        "# Creating text vectorization layer for the scrambled words \n",
        "max_vocab_length = 10000\n",
        "\n",
        "input_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2 , \n",
        "    max_tokens = max_vocab_length \n",
        ")\n",
        "\n",
        "# Fitting on our train sentences (scrambled words )\n",
        "input_text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eooVx9i0pw6P",
        "outputId": "497f3a3e-3d90-43fd-aaf0-93bd55021c32"
      },
      "source": [
        "# First 10 words from the vocabulary \n",
        "input_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TtNwlVnv8D"
      },
      "source": [
        "# Creating a text vectorization layer for the unscrambled words \n",
        "output_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2, \n",
        "    max_tokens = max_vocab_length\n",
        ")\n",
        "\n",
        "# Fitting on our train labels (unscrambled words)\n",
        "output_text_vectorizer.adapt(train_labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP-f3FPAqnjK",
        "outputId": "fddab936-cdb4-4c01-b21e-9b4848a95a8e"
      },
      "source": [
        "# First 10 words from the vocab \n",
        "output_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTRBmXyIqtmm",
        "outputId": "ec791f9f-6c64-4cdd-b98e-1bb4eaa503de"
      },
      "source": [
        "# Passing a scrambled text (strings) into our layer \n",
        "scrambled_tokens = input_text_vectorizer(scrambled_text)\n",
        "scrambled_tokens[:3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 29), dtype=int64, numpy=\n",
              "array([[   6, 6562,  346,  195,   66, 1549,    4,   66,    3,  545,  179,\n",
              "         685,  201,    9, 6696,    1,    1,    1,    1,    1,    1, 4742,\n",
              "        3118,    1,    1,    1,    1,    1,    1],\n",
              "       [  16,  313,   15,   31,  805,    4,  366,  594,   23, 2724,   64,\n",
              "          26, 3192,    1,    1, 2006,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    0,    0,    0,    0],\n",
              "       [   4,  317,  199,   28,    9,  968,   73, 3220,   10,    4, 4668,\n",
              "        2095,    8, 1074,   10,    1,    1,    1, 2119,    1,    1,    1,\n",
              "           1,  316,    1,    1,    1,    1,    1]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXjg8ZRRsJyk"
      },
      "source": [
        "In the above example we passed our text (strings) into our text vectorizer layer and it returns us a vector of token ID's of our sequence. \n",
        "\n",
        "Likewise we can get the corresponding sequence of a token ID, that is convert token ids back to text using `get_vocabulary()` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Kh1G5CsY77",
        "outputId": "5a15c943-5e18-4117-84bc-3b637087252c"
      },
      "source": [
        "# Creating a numpy array of the vocabulary\n",
        "input_vocab = np.array(input_text_vectorizer.get_vocabulary())\n",
        "input_vocab\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', ..., 'paper deep', 'packing', 'package the'],\n",
              "      dtype='<U26')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T56Lb4ctRv3",
        "outputId": "8ac5c216-37a6-4090-e634-555776f99a97"
      },
      "source": [
        "# Indexing our scrambled tokens into the array of vocbulary\n",
        "tokens = input_vocab[scrambled_tokens.numpy()]\n",
        "print(f'Actual sequence:\\n\\n {scrambled_text[:3]}\\n')\n",
        "print(f'\\nThe sequence in tokens:\\n\\n {tokens[:3]}') "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sequence:\n",
            "\n",
            " [b'to impaired loss processing information whether a information of transfer. local leads Therefore, we asked'\n",
            " b'with common Our approach issues a within adaptive proposed deals framework. these regularized'\n",
            " b'a about consider model We causal system, look for a facts. arguments and physical for']\n",
            "\n",
            "\n",
            "The sequence in tokens:\n",
            "\n",
            " [['to' 'impaired' 'loss' 'processing' 'information' 'whether' 'a'\n",
            "  'information' 'of' 'transfer' 'local' 'leads' 'therefore' 'we' 'asked'\n",
            "  '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' 'a information'\n",
            "  'information of' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]']\n",
            " ['with' 'common' 'our' 'approach' 'issues' 'a' 'within' 'adaptive'\n",
            "  'proposed' 'deals' 'framework' 'these' 'regularized' '[UNK]' '[UNK]'\n",
            "  'our approach' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '' '' '' '']\n",
            " ['a' 'about' 'consider' 'model' 'we' 'causal' 'system' 'look' 'for' 'a'\n",
            "  'facts' 'arguments' 'and' 'physical' 'for' '[UNK]' '[UNK]' '[UNK]'\n",
            "  'model we' '[UNK]' '[UNK]' '[UNK]' '[UNK]' 'for a' '[UNK]' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '[UNK]']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH15QmLVt8Ep"
      },
      "source": [
        "## Modelling Part \n",
        "\n",
        "Here we are going to build a seq2seq architecture from scratch we will start building from, \n",
        "- Encoder \n",
        "- Decoder \n",
        "- Attention Head "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZNvJICLuNP3"
      },
      "source": [
        "Since we are going to use a lot of low level API\n",
        "s where it's easy to get the shapes wrong, this `SpaceChecker` is used to check shapes throughout the tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEKdhaCB8-0f"
      },
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6fNLob2-3IS"
      },
      "source": [
        "# Defining needed constants for our model \n",
        "embedding_dim = 256 \n",
        "units = 1024 "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m9z9iU_ARb"
      },
      "source": [
        "#### Building our encoder layer \n",
        "The encoder, \n",
        "- Takes a list of token IDs (from input_text_vectorizer)\n",
        "- Looks up an embedding vector for each token (we will create that using `layers.Embedding`) \n",
        "- Processes the embeddings into a new sequences (using a `layers.GRU`)\n",
        "- **Returns**\n",
        "  - The processed sequence. This will be passed to the attention head.\n",
        "  - The internal state. This will be used to initialize the encoder. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccira5SaAOAU"
      },
      "source": [
        "# Building a Encoder layer \n",
        "\n",
        "class Encoder(tf.keras.layers.Layer): \n",
        "  def __init__(self ,input_vocab_size , embedding_dim , enc_units):\n",
        "    super(Encoder , self).__init__()\n",
        "    self.enc_units = enc_units \n",
        "    self.input_vocab_size = input_vocab_size \n",
        "\n",
        "    # This embedding layer converst tokens to vectors \n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size , \n",
        "                                             embedding_dim)\n",
        "  \n",
        "    # Using GRU layers to processes those vectors sequentially \n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units , \n",
        "                                 return_sequences = True , \n",
        "                                 return_state = True , \n",
        "                                 recurrent_initializer = 'glorot_uniform')\n",
        "  \n",
        "  def call(self , tokens , state = None):\n",
        "    shape_checker = ShapeChecker() \n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layers looks up the embedding for each token\n",
        "    vectors = self.embedding(tokens) # gives us the vectors for each token\n",
        "    shape_checker(vectors , ('batch' , 's' , 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence \n",
        "    #       output shape: (batch , s , enc_units)\n",
        "    #       state_shape: (batch , enc_units)\n",
        "    output , state = self.gru(vectors , initial_state = state)\n",
        "    shape_checker(output , ('batch' ,'s' , 'enc_units'))\n",
        "    shape_checker(state , ('batch' , 'enc_units'))\n",
        "\n",
        "    # 4. Return the new sequence and it's state \n",
        "    return output , state\n",
        "  \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13HASLcgF5QP"
      },
      "source": [
        "Alright that's complicated let's see how it works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNLXsz1LGGWq",
        "outputId": "b39a9a22-406c-458a-927d-274735440415"
      },
      "source": [
        "# Firstly conver the input text to token using Textvectorizer \n",
        "example_tokens = input_text_vectorizer(scrambled_text)\n",
        "example_tokens"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 29), dtype=int64, numpy=\n",
              "array([[   6, 6562,  346, ...,    1,    1,    1],\n",
              "       [  16,  313,   15, ...,    0,    0,    0],\n",
              "       [   4,  317,  199, ...,    1,    1,    1],\n",
              "       ...,\n",
              "       [   5,  489,   11, ...,    0,    0,    0],\n",
              "       [  17,    7,   73, ...,    1,    0,    0],\n",
              "       [ 456,   12,   53, ..., 3593,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2CdItXlGTEz"
      },
      "source": [
        "# Encode the input sequence (apply everything we wrote in our class)\n",
        "encoder = Encoder(input_vocab_size= input_text_vectorizer.vocabulary_size() , \n",
        "                  embedding_dim = embedding_dim, enc_units = units)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTBQdSOKGsgx"
      },
      "source": [
        "# Unravelling with each variable by applying on our example toke\n",
        "\n",
        "example_encoder_output , example_encoder_state = encoder(example_tokens)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osuKJ0sBG4nS",
        "outputId": "f9d3412d-387d-4192-ee66-a92b092f62b9"
      },
      "source": [
        "# Good, let's print them one by one \n",
        "print(f'Input batch, shape (batch): {scrambled_text.shape}\\n')\n",
        "print(f'Input batch tokens , shape (batch ,s): {example_tokens.shape}\\n')\n",
        "\n",
        "print(f'Encoder output , shape (batch, s , units): {example_encoder_output.shape}\\n')\n",
        "print(f'Encoder state , shape (batch, units): {example_encoder_state.shape}\\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input batch, shape (batch): (64,)\n",
            "\n",
            "Input batch tokens , shape (batch ,s): (64, 29)\n",
            "\n",
            "Encoder output , shape (batch, s , units): (64, 29, 1024)\n",
            "\n",
            "Encoder state , shape (batch, units): (64, 1024)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drENKdsdJuIV"
      },
      "source": [
        "##### **Attention Head** \n",
        "\n",
        "- The decoder uses attention to selectively focus on parts of the input sequence. \n",
        "- The attention takes a **sequence of vectors** for each example and returns an **`attention vector` for each example**.\n",
        "- This attention layer is similar to `layers.GlobalAveragePooling1D` but the attention layer performs a weighted average. \n",
        "\n",
        "The attention head equation, \n",
        "- Calculates the attention weights, as a softmax across the encoders output and sequence. \n",
        "- Calculates the context vector as the weighted sum of the encoder outputs. \n",
        "\n",
        "We use (Bahdanau's additive attention)[https://arxiv.org/pdf/1409.0473.pdf] and TensorFlow includes the implementations of both as `layers.Attention` and `layers.AdditiveAttention`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afEvgk3vZd44"
      },
      "source": [
        "# Building the Bahdanau's attention head \n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self , units):\n",
        "    super().__init__()\n",
        "    # Calculating the Bahadanau attention \n",
        "    self.W1 = tf.keras.layers.Dense(units , use_bias = False)\n",
        "    self.W2 = tf.keras.layers.Dense(units , use_bias = False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self , query , value , mask):\n",
        "    shape_checker = ShapeChecker() \n",
        "    shape_checker(query , ('batch' , 't' , 'query_units'))\n",
        "    shape_checker(value , ('batch' , 's' , 'value_units') )\n",
        "    shape_checker(mask , ('batch' , 's'))\n",
        "\n",
        "    # Calculating W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query , ('batch' , 't' , 'attn_units'))\n",
        "\n",
        "    # Calculating W2@ht \n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key , ('batch' , 's' , 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1] , dtype = bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    # Creating the context vector\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query ,value , w2_key] , \n",
        "        mask = [query_mask , value_mask], \n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector , ('batch' , 't' , 'value_units'))\n",
        "    shape_checker(attention_weights , ('batch' , 't' , 's'))\n",
        "\n",
        "    return context_vector , attention_weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DIXI0IG5BVL",
        "outputId": "11c46316-e242-43a9-ec92-7a94163b4892"
      },
      "source": [
        "# Testing the attention layer by creating a bahdanau attention layer \n",
        "attention_layer = BahdanauAttention(units)\n",
        "attention_layer"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BahdanauAttention at 0x7f1acc422550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x7sbnjA5LOK"
      },
      "source": [
        "The above Bahdanau layer will take 3 inputs,\n",
        "\n",
        "- `query`: this will be generated by decoder later\n",
        "- `value`: output of the encoder \n",
        "- `mask`: to exclude the padding, `example_tokens ! = 0`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shoN_lby5l79"
      },
      "source": [
        "The vectorized implementation of the attention layer let's you pass a batch of sequences of query vectors and a batch of sequence of value vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU8BvFvZ5pQJ",
        "outputId": "fdeed8a7-9409-4727-8814-3950355f9737"
      },
      "source": [
        "# Later, the decoder will generate this attention query \n",
        "example_attention_query = tf.random.normal(shape = [len(example_tokens) , 2 , 10])\n",
        "example_attention_query"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 2, 10), dtype=float32, numpy=\n",
              "array([[[ 0.33169854, -1.5331086 ,  0.36669376, ..., -0.51797646,\n",
              "         -0.2474743 , -1.7129605 ],\n",
              "        [-0.74607015,  0.4274798 ,  0.9494914 , ...,  0.94947517,\n",
              "         -0.12682684,  1.2155783 ]],\n",
              "\n",
              "       [[ 0.10700006, -0.72547966,  0.5217185 , ...,  0.05120929,\n",
              "         -0.68890065,  0.18639168],\n",
              "        [ 0.91531307,  0.73072165, -0.27121982, ...,  0.40275267,\n",
              "          0.2720137 , -0.57919276]],\n",
              "\n",
              "       [[ 0.2927214 ,  0.7324313 ,  0.51344985, ..., -0.49110648,\n",
              "          0.16187479,  0.11939314],\n",
              "        [-0.6117943 ,  0.5170564 , -0.08260116, ...,  0.18620656,\n",
              "         -0.59345615, -0.69135875]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 1.4346825 ,  0.9154264 , -0.04331203, ...,  0.5751158 ,\n",
              "         -0.8019399 , -0.22650287],\n",
              "        [-0.10573704, -1.1321943 ,  0.5941484 , ...,  0.62285924,\n",
              "          0.1760563 ,  1.3294592 ]],\n",
              "\n",
              "       [[ 0.5574216 ,  0.9416092 , -0.94621986, ..., -3.411975  ,\n",
              "         -0.27881563, -1.6692009 ],\n",
              "        [ 3.5096133 ,  1.41996   , -0.7618969 , ...,  0.6637473 ,\n",
              "         -0.5116972 , -0.25347266]],\n",
              "\n",
              "       [[-0.62037843,  0.23795591, -0.84277743, ...,  1.1235794 ,\n",
              "          0.23119287, -0.62272584],\n",
              "        [ 1.014053  ,  0.4888923 ,  0.6899584 , ...,  1.4154745 ,\n",
              "         -0.25430235,  0.81059486]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa110xb66WqJ",
        "outputId": "c3a78184-a199-4e5c-b5cd-930af0fc2618"
      },
      "source": [
        "# Passing this to our attention layer \n",
        "\n",
        "context_vector , attention_weights = attention_layer(\n",
        "    query = example_attention_query, \n",
        "    value = example_encoder_output, \n",
        "    mask = (example_tokens != 0)\n",
        ")\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units): (64, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grc5TjmA6qEU"
      },
      "source": [
        "The attention weights should sum to 1.0 for each response. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzEB38h564eL",
        "outputId": "e29abc8e-18aa-4dc9-e665-2dda7113344a"
      },
      "source": [
        "attention_weights.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 2, 29])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVVxLqEu7XYu",
        "outputId": "e8cad409-5102-412c-98fc-398797cf8b28"
      },
      "source": [
        "attention_slice = attention_weights[0 , 0].numpy()\n",
        "print(attention_slice)\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03397634 0.0342699  0.03430775 0.03433517 0.03486571 0.03445323\n",
            " 0.03445512 0.03489019 0.0348306  0.03448528 0.03397669 0.03426249\n",
            " 0.03424235 0.03431281 0.03400245 0.03428709 0.03450302 0.03463967\n",
            " 0.0347182  0.03476054 0.03478232 0.03471794 0.03415803 0.03435341\n",
            " 0.03452441 0.03464188 0.03471401 0.03475537 0.03477798]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "N-JuWoIF7OWp",
        "outputId": "86a91fe9-c80f-4eb8-b7d1-0199f286868c"
      },
      "source": [
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1ac93a6d50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd7_thH-7UDc"
      },
      "source": [
        "#### **Decoder**\n",
        "The decoder job is to generate predictions for the next output token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7YYBtP375Xk"
      },
      "source": [
        "This is getting long, so found another tutorial the difference it's in the tensorflow addons section so it manages alot of heavy lifting..\n",
        "\n",
        "Will continue on this later, but this was a good exercise, learnt alot along the way. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shvIMXp99man"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
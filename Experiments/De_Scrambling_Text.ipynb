{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "De-Scrambling_Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgL/MJ2B3ZtQvjzgra1JhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Experiments/De_Scrambling_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu8GiMfGJ_7G"
      },
      "source": [
        "# Converting Scrambled sequence into a Unscrambled sequence using attention. \n",
        "\n",
        "Reference: https://www.tensorflow.org/text/tutorials/nmt_with_attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXrtEA9lGzCW",
        "outputId": "966bd6aa-8481-49d1-b826-a3e20e7f6126"
      },
      "source": [
        "!pip install aicrowd-cli\n",
        "API_KEY = 'b37b516b0cf698701bd83f05f784aab5' \n",
        "!aicrowd login --api-key $API_KEY\n",
        "\n",
        "# Downloading the Dataset\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "!aicrowd dataset download --challenge de-shuffling-text -j 3 -o data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting aicrowd-cli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/57/59b5a00c6e90c9cc028b3da9dff90e242ad2847e735b1a0e81a21c616e27/aicrowd_cli-0.1.7-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
            "\u001b[?25hCollecting gitpython<4,>=3.1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (7.1.2)\n",
            "Collecting requests<3,>=2.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml<1,>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from aicrowd-cli) (0.10.2)\n",
            "Collecting requests-toolbelt<1,>=0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hCollecting rich<11,>=10.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/32/eb8aadb1ed791081e5c773bd1dfa15f1a71788fbeda37b12f837f2b1999b/rich-10.3.0-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 8.4MB/s \n",
            "\u001b[?25hCollecting tqdm<5,>=4.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/d7/f357d98e9b50346bcb6095fe3ad205d8db3174eb5edb03edfe7c4099576d/tqdm-4.61.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython<4,>=3.1.12->aicrowd-cli) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25.1->aicrowd-cli) (2020.12.5)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/92/dfd892312d822f36c55366118b95d914e5f16de11044a27cf10a7d71bbbf/commonmark-0.9.1-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hCollecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<11,>=10.0.0->aicrowd-cli) (2.6.1)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: smmap, gitdb, gitpython, requests, requests-toolbelt, commonmark, colorama, rich, tqdm, aicrowd-cli\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed aicrowd-cli-0.1.7 colorama-0.4.4 commonmark-0.9.1 gitdb-4.0.7 gitpython-3.1.17 requests-2.25.1 requests-toolbelt-0.9.1 rich-10.3.0 smmap-4.0.0 tqdm-4.61.0\n",
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[32mSaved API Key successfully!\u001b[0m\n",
            "val.csv:   0% 0.00/714k [00:00<?, ?B/s]\n",
            "val.csv: 100% 714k/714k [00:00<00:00, 3.03MB/s]\n",
            "\n",
            "train.csv: 100% 7.00M/7.00M [00:00<00:00, 17.0MB/s]\n",
            "test.csv: 100% 1.83M/1.83M [00:00<00:00, 5.91MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVPH-jwHIONU"
      },
      "source": [
        "# Importing all the packages we need \n",
        "import tensorflow as tf \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ZJq4reN10H",
        "outputId": "765b63cd-8165-424a-ba89-8918af7f36b3"
      },
      "source": [
        "# Importing the data \n",
        "\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "val_data = pd.read_csv('data/val.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Printing out all shapes of our data \n",
        "print(f'Shape of the train data: {train_data.shape}')\n",
        "print(f'Shape of the validation data: {val_data.shape}')\n",
        "print(f'Shape of the test data: {test_data.shape}')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train data: (40001, 2)\n",
            "Shape of the validation data: (4001, 2)\n",
            "Shape of the test data: (10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "aiYByVBPN6bS",
        "outputId": "99802db2-36ac-4c2a-e8de-a074e920ebba"
      },
      "source": [
        "# How does our train data looks like? \n",
        "train_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>presented here Furthermore, naive improved. im...</td>\n",
              "      <td>Furthermore, the naive implementation presente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vector a in a form vector multidimensional spa...</td>\n",
              "      <td>Those coefficients form a vector in a multidim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>compatible of The model with recent is model s...</td>\n",
              "      <td>The model is compatible with a recent model of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>but relevance outlined. hemodynamics its based...</td>\n",
              "      <td>The model is based on electrophysiology, but i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of transitions lever-like involve reorientatio...</td>\n",
              "      <td>Conformational transitions in macromolecular c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  presented here Furthermore, naive improved. im...  Furthermore, the naive implementation presente...\n",
              "1  vector a in a form vector multidimensional spa...  Those coefficients form a vector in a multidim...\n",
              "2  compatible of The model with recent is model s...  The model is compatible with a recent model of...\n",
              "3  but relevance outlined. hemodynamics its based...  The model is based on electrophysiology, but i...\n",
              "4  of transitions lever-like involve reorientatio...  Conformational transitions in macromolecular c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgk0Pc6-Woxl",
        "outputId": "102757f0-a1d7-49e0-b2c0-cfa94eae3e57"
      },
      "source": [
        "# Shuffling our train data \n",
        "train_data_shuffled = train_data.sample(frac = 1 , random_state = 42)\n",
        "train_data_shuffled.head() , train_data_shuffled.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                    text                                              label\n",
              " 32824  on work, supervised label image the segmentati...  In our work, we focus on the weakly supervised...\n",
              " 16298  we small of a for set work, In this features i...  In this work, we propose a small set of featur...\n",
              " 30180  ($G_h^{Der}$ to factors the contributes $\\tau_...  The increment of both factors ($G_h^{Der}$ and...\n",
              " 6689   new precise particular, for entailment. bounds...  In particular, we provide new precise analytic...\n",
              " 26893  a these causation Incorporating features, defi...  Incorporating these three features, a definiti...,\n",
              " (40001, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc3LZNjiN_Ck",
        "outputId": "86bedec6-54ce-4aaa-ca31-886cbf3f3390"
      },
      "source": [
        "# Splitting sentences and labels\n",
        "train_sentences = train_data_shuffled['text'].to_numpy()\n",
        "train_labels = train_data_shuffled['label'].to_numpy()\n",
        "\n",
        "val_sentences = val_data['text'].to_numpy()\n",
        "val_labels = val_data['label'].to_numpy()\n",
        "\n",
        "test_sentences = test_data['text'].to_numpy()\n",
        "test_labels = test_data['label'].to_numpy()\n",
        "\n",
        "\n",
        "# Checking the shapes \n",
        "print(f'Shape of the train sentences: {train_sentences.shape}')\n",
        "print(f'Shape of the validation sentences: {val_sentences.shape}')\n",
        "print(f'Shape of the train labels: {train_labels.shape}')\n",
        "print(f'Shape of the validation labels: {val_labels.shape}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the train sentences: (40001,)\n",
            "Shape of the validation sentences: (4001,)\n",
            "Shape of the train labels: (40001,)\n",
            "Shape of the validation labels: (4001,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bz8V1gpa7sN",
        "outputId": "b30ede31-767c-4a44-be69-310d1e24a286"
      },
      "source": [
        "# Creating a tf.data.dataset of our sentences and labels \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences , train_labels)).shuffle(1000)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences , val_labels))\n",
        "\n",
        "# Adding a batch \n",
        "train_dataset = train_dataset.batch(64)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>,\n",
              " <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAWBS79bLVM",
        "outputId": "5c609b94-5a4e-49f9-aa8b-f9116d1edc89"
      },
      "source": [
        "# Looking into our train_dataset just a batch (only 5 first texts in a batch)\n",
        "for scrambled_text , unscrambled_text in train_dataset.take(1):\n",
        "  print(f'Below is the Scrambled version:\\n {scrambled_text[:5]}')\n",
        "  print('\\n----------\\n')\n",
        "  print(f'Below is the Un-Scrambled version:\\n {unscrambled_text[:5]}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below is the Scrambled version:\n",
            " [b\"representation's mechanism physiological this unknown. However, details of remain many\"\n",
            " b'In Plug-and-Play convergence. we a fixed algorithm with paper, point this ADMM provable propose'\n",
            " b'those I information interpret show to how with theory. to statistical expressions respect'\n",
            " b'The code and publicly are datasets available.'\n",
            " b'on results R-CNN demonstrated impressive has recently The Faster detection benchmarks. various object']\n",
            "\n",
            "----------\n",
            "\n",
            "Below is the Un-Scrambled version:\n",
            " [b\"However, many details of this representation's physiological mechanism remain unknown.\"\n",
            " b'In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed point convergence.'\n",
            " b'I show how to interpret those statistical expressions with respect to information theory.'\n",
            " b'The datasets and code are publicly available.'\n",
            " b'The Faster R-CNN has recently demonstrated impressive results on various object detection benchmarks.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MveFqAiWd3s7"
      },
      "source": [
        "# Creating text vectorization layer for the scrambled words \n",
        "max_vocab_length = 10000\n",
        "\n",
        "input_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2 , \n",
        "    max_tokens = max_vocab_length \n",
        ")\n",
        "\n",
        "# Fitting on our train sentences (scrambled words )\n",
        "input_text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eooVx9i0pw6P",
        "outputId": "75c6dd19-ab47-4f4d-fa6b-eb439859152f"
      },
      "source": [
        "# First 10 words from the vocabulary \n",
        "input_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TtNwlVnv8D"
      },
      "source": [
        "# Creating a text vectorization layer for the unscrambled words \n",
        "output_text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize = 'lower_and_strip_punctuation' , \n",
        "    ngrams = 2, \n",
        "    max_tokens = max_vocab_length\n",
        ")\n",
        "\n",
        "# Fitting on our train labels (unscrambled words)\n",
        "output_text_vectorizer.adapt(train_labels)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP-f3FPAqnjK",
        "outputId": "d0f8037a-08da-4a95-f1b6-53e02be7a063"
      },
      "source": [
        "# First 10 words from the vocab \n",
        "output_text_vectorizer.get_vocabulary()[:10]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'of', 'a', 'in', 'to', 'is', 'and', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTRBmXyIqtmm",
        "outputId": "b48cb72c-2f16-4a54-e214-7e70231d5f54"
      },
      "source": [
        "# Passing a scrambled text (strings) into our layer \n",
        "scrambled_tokens = input_text_vectorizer(scrambled_text)\n",
        "scrambled_tokens[:3]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 29), dtype=int64, numpy=\n",
              "array([[ 332,  494, 3090,   11,  887,   40, 1170,    3, 1392,   74,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0],\n",
              "       [   5,    1,  666,    9,    4, 1311,   39,   16,   21,  373,   11,\n",
              "        6184,    1,   43,    1,    1,    1,  218,    1,    1, 6737, 5780,\n",
              "           1,    1,    1,    1,    1,    0,    0],\n",
              "       [ 520,  830,   66, 3831,   53,    6,  132,   16,  166,    6,  356,\n",
              "        1287, 1697,    1,    1,    1,    1, 5479, 4767,    1,    1, 7575,\n",
              "        9543,    1,    1,    0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXjg8ZRRsJyk"
      },
      "source": [
        "In the above example we passed our text (strings) into our text vectorizer layer and it returns us a vector of token ID's of our sequence. \n",
        "\n",
        "Likewise we can get the corresponding sequence of a token ID, that is convert token ids back to text using `get_vocabulary()` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Kh1G5CsY77",
        "outputId": "89ffa825-2e3d-4a54-a593-4c0e55c13b1d"
      },
      "source": [
        "# Creating a numpy array of the vocabulary\n",
        "input_vocab = np.array(input_text_vectorizer.get_vocabulary())\n",
        "input_vocab\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', ..., 'paper deep', 'packing', 'package the'],\n",
              "      dtype='<U26')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T56Lb4ctRv3",
        "outputId": "94c1cb41-2e47-47ca-abcb-784c598bb5b7"
      },
      "source": [
        "# Indexing our scrambled tokens into the array of vocbulary\n",
        "tokens = input_vocab[scrambled_tokens.numpy()]\n",
        "print(f'Actual sequence:\\n\\n {scrambled_text[:3]}\\n')\n",
        "print(f'\\nThe sequence in tokens:\\n\\n {tokens[:3]}') "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual sequence:\n",
            "\n",
            " [b\"representation's mechanism physiological this unknown. However, details of remain many\"\n",
            " b'In Plug-and-Play convergence. we a fixed algorithm with paper, point this ADMM provable propose'\n",
            " b'those I information interpret show to how with theory. to statistical expressions respect']\n",
            "\n",
            "\n",
            "The sequence in tokens:\n",
            "\n",
            " [['representations' 'mechanism' 'physiological' 'this' 'unknown'\n",
            "  'however' 'details' 'of' 'remain' 'many' '[UNK]' '[UNK]' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '' '' '' '' '' '' '' ''\n",
            "  '' '']\n",
            " ['in' '[UNK]' 'convergence' 'we' 'a' 'fixed' 'algorithm' 'with' 'paper'\n",
            "  'point' 'this' 'admm' '[UNK]' 'propose' '[UNK]' '[UNK]' '[UNK]' 'we a'\n",
            "  '[UNK]' '[UNK]' 'algorithm with' 'with paper' '[UNK]' '[UNK]' '[UNK]'\n",
            "  '[UNK]' '[UNK]' '' '']\n",
            " ['those' 'i' 'information' 'interpret' 'show' 'to' 'how' 'with' 'theory'\n",
            "  'to' 'statistical' 'expressions' 'respect' '[UNK]' '[UNK]' '[UNK]'\n",
            "  '[UNK]' 'show to' 'to how' '[UNK]' '[UNK]' 'theory to' 'to statistical'\n",
            "  '[UNK]' '[UNK]' '' '' '' '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH15QmLVt8Ep"
      },
      "source": [
        "## Modelling Part \n",
        "\n",
        "Here we are going to build a seq2seq architecture from scratch we will start building from, \n",
        "- Encoder \n",
        "- Decoder \n",
        "- Attention Head "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZNvJICLuNP3"
      },
      "source": [
        "Since we are going to use a lot of low level API\n",
        "s where it's easy to get the shapes wrong, this `SpaceChecker` is used to check shapes throughout the tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEKdhaCB8-0f"
      },
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6fNLob2-3IS"
      },
      "source": [
        "# Defining needed constants for our model \n",
        "embedding_dim = 256 \n",
        "units = 1024 "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m9z9iU_ARb"
      },
      "source": [
        "#### Building our encoder layer \n",
        "The encoder, \n",
        "- Takes a list of token IDs (from input_text_vectorizer)\n",
        "- Looks up an embedding vector for each token (we will create that using `layers.Embedding`) \n",
        "- Processes the embeddings into a new sequences (using a `layers.GRU`)\n",
        "- **Returns**\n",
        "  - The processed sequence. This will be passed to the attention head.\n",
        "  - The internal state. This will be used to initialize the encoder. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccira5SaAOAU"
      },
      "source": [
        "# Building a Encoder layer \n",
        "\n",
        "class Encoder(tf.keras.layers.Layer): \n",
        "  def __init__(self ,input_vocab_size , embedding_dim , enc_units):\n",
        "    super(Encoder , self).__init__()\n",
        "    self.enc_units = enc_units \n",
        "    self.input_vocab_size = input_vocab_size \n",
        "\n",
        "    # This embedding layer converst tokens to vectors \n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size , \n",
        "                                             embedding_dim)\n",
        "  \n",
        "    # Using GRU layers to processes those vectors sequentially \n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units , \n",
        "                                 return_sequences = True , \n",
        "                                 return_state = True , \n",
        "                                 recurrent_initializer = 'glorot_uniform')\n",
        "  \n",
        "  def call(self , tokens , state = None):\n",
        "    shape_checker = ShapeChecker() \n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layers looks up the embedding for each token\n",
        "    vectors = self.embedding(tokens) # gives us the vectors for each token\n",
        "    shape_checker(vectors , ('batch' , 's' , 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence \n",
        "    #       output shape: (batch , s , enc_units)\n",
        "    #       state_shape: (batch , enc_units)\n",
        "    output , state = self.gru(vectors , initial_state = state)\n",
        "    shape_checker(output , ('batch' ,'s' , 'enc_units'))\n",
        "    shape_checker(state , ('batch' , 'enc_units'))\n",
        "\n",
        "    # 4. Return the new sequence and it's state \n",
        "    return output , state\n",
        "  \n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13HASLcgF5QP"
      },
      "source": [
        "Alright that's complicated let's see how it works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNLXsz1LGGWq",
        "outputId": "65417d82-2c75-4d2c-e9b8-e3e2dff7343d"
      },
      "source": [
        "# Firstly conver the input text to token using Textvectorizer \n",
        "example_tokens = input_text_vectorizer(scrambled_text)\n",
        "example_tokens"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 29), dtype=int64, numpy=\n",
              "array([[ 332,  494, 3090, ...,    0,    0,    0],\n",
              "       [   5,    1,  666, ...,    1,    0,    0],\n",
              "       [ 520,  830,   66, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  61,  229,   30, ...,    0,    0,    0],\n",
              "       [6190,   15,  514, ..., 7691, 8055,    1],\n",
              "       [   2,   17,   18, ...,    1,    1,    1]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2CdItXlGTEz"
      },
      "source": [
        "# Encode the input sequence (apply everything we wrote in our class)\n",
        "encoder = Encoder(input_vocab_size= input_text_vectorizer.vocabulary_size() , \n",
        "                  embedding_dim = embedding_dim, enc_units = units)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTBQdSOKGsgx"
      },
      "source": [
        "# Unravelling with each variable by applying on our example toke\n",
        "\n",
        "example_encoder_output , example_encoder_state = encoder(example_tokens)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osuKJ0sBG4nS",
        "outputId": "47552e47-7f8d-492e-f460-2f66e8f80159"
      },
      "source": [
        "# Good, let's print them one by one \n",
        "print(f'Input batch, shape (batch): {scrambled_text.shape}\\n')\n",
        "print(f'Input batch tokens , shape (batch ,s): {example_tokens.shape}\\n')\n",
        "\n",
        "print(f'Encoder output , shape (batch, s , units): {example_encoder_output.shape}\\n')\n",
        "print(f'Encoder state , shape (batch, units): {example_encoder_state.shape}\\n')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input batch, shape (batch): (64,)\n",
            "\n",
            "Input batch tokens , shape (batch ,s): (64, 29)\n",
            "\n",
            "Encoder output , shape (batch, s , units): (64, 29, 1024)\n",
            "\n",
            "Encoder state , shape (batch, units): (64, 1024)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drENKdsdJuIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
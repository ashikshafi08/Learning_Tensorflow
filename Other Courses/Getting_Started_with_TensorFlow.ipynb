{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting_Started_with_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5CJ/MmC69PLkxGR5LBKyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Other%20Courses/Getting_Started_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXlTs6_UyEoZ"
      },
      "source": [
        "This notebook contains all the materials and notes for the Getting Started with TensorFlow 2 Course by Imperial College London. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9qTHcwp0YsP"
      },
      "source": [
        "# Importing TensorFlow \n",
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkpA5UTX-fzW"
      },
      "source": [
        "## The Sequential model API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPZeEPbm0fuP"
      },
      "source": [
        "### Build a Sequential Model \n",
        "\n",
        "It's really easy and intuitive way to construct a deeplearning models. Probably most of the neural networks that we work with, can be built using the Sequential Class. \n",
        "\n",
        "This will have the list of keras layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SdVnFi11AMH"
      },
      "source": [
        "# Importing the layers we're going to use \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten , Softmax "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XvboOUy1RXV"
      },
      "source": [
        "Build a feedforward neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kStX80FQ1Vfu"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  Flatten(input_shape = (28 , 28)) , # Explicitly specifying the input_shape (to build the model) \n",
        "  Dense(16 , activation = 'relu') , \n",
        "  Dense(10 , activation = 'relu'), \n",
        "  Dense(10 , activation= 'relu') , \n",
        "  #Dense(10 , activation = 'sigmoid')\n",
        "  tf.keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "# Getting the model summary \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfcgiANw25fS"
      },
      "source": [
        "### Convolutional and Pooling Layers in TensorFlow \n",
        "\n",
        "Previously we build our models with Feedforward networks, but now will use Convolutional layers to build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-Z2Wxw3kLo"
      },
      "source": [
        "# Importing the needed packages \n",
        "from tensorflow.keras.layers import Flatten, Dense , Conv2D , MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNvXB1HK4AD5"
      },
      "source": [
        "# Building a Convolutional Model \n",
        "model = tf.keras.Sequential([\n",
        "  Conv2D(filters= 16 , kernel_size= 3 , \n",
        "         activation = 'relu' , input_shape = (32 , 32 , 3)) , \n",
        "  MaxPooling2D(pool_size= 3) , \n",
        "  Flatten() , \n",
        "  Dense(64 ,  activation= 'relu'), \n",
        "  Dense(10 , activation= 'softmax')\n",
        "])\n",
        "\n",
        "# Getting the summary of the model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZBmPXkW7zOM"
      },
      "source": [
        "# Build the Sequential convolutional neural network model\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32 , kernel_size=3 , padding = 'SAME' , strides = 2 , input_shape = (224 , 224, 3)) , \n",
        "    MaxPooling2D(3), \n",
        "    Conv2D(16 , 3 , 2),\n",
        "    Flatten(),\n",
        "    Dense(30 , activation = 'relu'),\n",
        "    Dense(10 , activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Summary of the model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTQU3Ry9_kA4"
      },
      "source": [
        "### Weight and bias initializers \n",
        "Will discuss the different ways to intialize weights and biases in the layers of neural networks.\n",
        "\n",
        "#### Default weight and biases\n",
        "The models we've worked so far, we have not specified the **initial values of the weights and biases** in each layers. \n",
        "\n",
        "Tensorflow set's the default value depends upon what type of layer's we are using. \n",
        "\n",
        "For instance, \n",
        "- In `Dense` layer the **biases** are set to zero (`zeros`) by default. \n",
        "- While the **weights** are set according to the `glorot_uniform`, or the Glorot uniform initializer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI9eQ6Fy_rpg"
      },
      "source": [
        "#### Initializing your own weights and biases \n",
        "\n",
        "We can even initialize our own weights and biases, and TensorFlow makes the process quite straightforward. \n",
        "\n",
        "This can be achieved by using tweaking two optional arguments in each layer, \n",
        "- `kernel_initialiser` - for weights. \n",
        "- `bias_initialiser` - for the biases. \n",
        "\n",
        "Note: For `MaxPooling` layers we need not to specify the weights and biases. Will throw an error. \n",
        "\n",
        "Let's initialize the weights and biases by ourselves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41tOw85DNnX"
      },
      "source": [
        "# Importing again (to make a practice)\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Flatten, Dense , Conv2D , MaxPool2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYb4pJEwDmlD"
      },
      "source": [
        "# Constructing a model (with manual weight and bias initializer)\n",
        "\n",
        "model = Sequential([\n",
        "  Conv2D(16 , 3 , kernel_initializer='random_uniform' , \n",
        "         bias_initializer = 'zeros' , activation = 'relu' \n",
        "         , input_shape = (224 ,224 , 3)), \n",
        "  MaxPooling2D(3) , \n",
        "  Flatten(), \n",
        "  Dense(64, kernel_initializer= 'he_uniform' , bias_initializer='ones' , \n",
        "        activation = 'relu'), \n",
        "  \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDTrBBcZ6943"
      },
      "source": [
        "We can even instantiate initialisers in slightly different manner, allowing us to set optional arguments of the initalisation metod. \n",
        "\n",
        "- https://keras.io/api/layers/initializers/\n",
        "- https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_aMpl3q7fbP"
      },
      "source": [
        "# Slightly different method for more flexibility \n",
        "\n",
        "model = Sequential([\n",
        "  Conv2D(16 , 3 , kernel_initializer='random_uniform' , \n",
        "         bias_initializer = 'zeros' , activation = 'relu' \n",
        "         , input_shape = (224 ,224 , 3)), \n",
        "  MaxPooling2D(3) , \n",
        "  Flatten(), \n",
        "  Dense(64, kernel_initializer= 'he_uniform' , bias_initializer='ones' , \n",
        "        activation = 'relu'), \n",
        "  Dense(64 , kernel_initializer= tf.keras.initializers.TruncatedNormal(mean = 0.0 , stddev= 0.05) , \n",
        "        bias_initializer = tf.keras.initializers.Zeros() , \n",
        "        activation = 'relu') , \n",
        "        Dense(8 , kernel_initializer= tf.keras.initializers.Orthogonal(gain = 1.0 , seed = None) , \n",
        "              bias_initializer = tf.keras.initializers.Constant(value = 0.4) , \n",
        "              activation = 'relu')\n",
        "  \n",
        "])\n",
        "\n",
        "# Getting the summary of the model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fyiZMLQ8zi8"
      },
      "source": [
        "#### Custom weight and bias initializers \n",
        "\n",
        "It's also possible to define your own weight and bias initializers. \n",
        "\n",
        "But the initializers must take in two arguments, \n",
        "- the `shape` of the tensor to be initialized. \n",
        "- the `dtype`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH_sCgkw9P7X"
      },
      "source": [
        "import tensorflow.keras.backend as K "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO0SJwHj9cDE"
      },
      "source": [
        "# Define a custom initializer \n",
        "\n",
        "def my_init(shape , dtype = None):\n",
        "  return K.random_normal(shape , dtype = dtype)\n",
        "\n",
        "# Checking how our initializer works\n",
        "model = Sequential([\n",
        "  Flatten(input_shape = (28 , 28) ) , \n",
        "  Dense(64 , kernel_initializer= my_init)\n",
        "])\n",
        "\n",
        "# Summary of the model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze9cDgbT94Ee"
      },
      "source": [
        "model.weights[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24CC0J2K-WCi"
      },
      "source": [
        "We can even visualize the initialized weights and biases to see the effect of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qpr3F8P-bMy"
      },
      "source": [
        "# Filter our the pooling and flatten layers, because they don't have any weights\n",
        "weight_layers = [layer for layer in model.layers if len(layer.weights) > 0]\n",
        "\n",
        "weight_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGbBGPuk_HDJ"
      },
      "source": [
        "# Plot histograms of weight and bias values \n",
        "\n",
        "fig , axes = plt.subplots(5 , 2 , figsize = (12 , 16))\n",
        "fig.subplots_adjust(hspace= 0.5 , wspace = 0.5)\n",
        "\n",
        "for i , layer in enumerate(weight_layers):\n",
        "  for j in [0 , 1]:\n",
        "    axes[i , j].hist(layer.weights[j].numpy().flatten() , align = 'left')\n",
        "    axes[i , j].set_title(layer.weights[j].name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4eVBgot_iKr"
      },
      "source": [
        "### Compiling our model \n",
        "We saw how to build a model but to start training the model on our data we need to specify: \n",
        "- loss function \n",
        "- optimization function \n",
        "- a metric \n",
        "\n",
        "To do this we use `compile` to specify all the above 3 things in order to get our model ready for training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQXIxqzj-uHQ"
      },
      "source": [
        "# Let's build a simple binary classification model \n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64 , activation= 'elu' , input_shape = (32,)) , \n",
        "  Dense(1 , activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Now our important step is to compile our model \n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits= True) , \n",
        "              optimizer = tf.keras.optimizers.SGD(learning_rate= 0.001 , momentum= 0.9 , \n",
        "                                                  nesterov = True) , \n",
        "              metrics = [tf.keras.metrics.BinaryAccuracy(threshold= 0.7) , \n",
        "                         tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKF4MdeL_4FY"
      },
      "source": [
        "Writing in a object format, it gives us more flexibility to tweak the parameter values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwq7k6Sr_o16"
      },
      "source": [
        "# Getting the summary of our model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJRH9n0_uY-"
      },
      "source": [
        "# What's stored in the model \n",
        "print(model.optimizer)\n",
        "print(model.loss)\n",
        "print(model.metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN5gbYsrB-G2"
      },
      "source": [
        "# Printing the learning rate of the optimizer \n",
        "model.optimizer.lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLazwVO0CYlG"
      },
      "source": [
        "#### **Metrics in Keras**\n",
        " \n",
        "We will explore the different metrics in keras. \n",
        "\n",
        "One of the most common metrics used for classification problem in Keras is `accuracy`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYZ2uFf0CxhS"
      },
      "source": [
        "# Compile the model \n",
        "model.compile(loss = 'sparse_categorical_crossentropy' , \n",
        "              optimizer = 'Adam' , \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfuAlQL2DGOD"
      },
      "source": [
        "We now have a model that uses accuracy as a metric to judge the performance. \n",
        "\n",
        "But how is this metric actually calculated? We can break this in two cases. \n",
        "\n",
        "**Case 1 - Binary Classification with sigmoid activation function**\n",
        "\n",
        "We are training a model for binary classification problem with a sigmoid activation function in the output layer (Cat or Dog). \n",
        "\n",
        "- Given the input, the model will output a float between 0 and 1. \n",
        "- Based on whether the float is less than or greater than the `threshold` of the accuracy (default it's 0.5). \n",
        "- We round the float to get the predicted classification from the model (`y_pred`).\n",
        "\n",
        "This accuracy metric compares, \n",
        "- the value `y_pred` on each training examples. \n",
        "- with the true output `y_true` will be an one hot encoded vector. \n",
        "\n",
        "Atlast the accuracy computes the mean of 𝛿(𝑦(𝑖)𝑝𝑟𝑒𝑑,𝑦(𝑖)𝑡𝑟𝑢𝑒) over all training examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaI81olBEkf7"
      },
      "source": [
        "# Let's see in code how it's implemented (Sigmoid Function)\n",
        "\n",
        "y_true = tf.constant([0.0 , 1.0 , 1.0]) # the class \n",
        "y_pred = tf.constant([0.4 , 0.8 , 0.3]) # the prediction prob of y_true \n",
        "\n",
        "# Accuracy\n",
        "accuracy = K.mean(K.equal(y_true , K.round(y_pred))) \n",
        "print(f'The overall accuracy (taking mean over all the examples): {accuracy} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL7yUpk7FUGY"
      },
      "source": [
        "**Case 2 - Categorical Classification** \n",
        "\n",
        "Imagine we want to train a model for the classification problem which has more than 1 classes (Dog breeds), we use a activation function called `softmax` in the last layer. \n",
        "\n",
        "Given the training examples X(i) the model will output a tensor of probabilities 𝑝1,𝑝2,…𝑝𝑚 according to the model that x(i) falls into each class. \n",
        "\n",
        "Here the accuracy metric works a bit differetn rather comparing to the value of y_true, \n",
        "- it determines the largest argument in the `y_pred` tensor of one sample. \n",
        "- Then compares the index to the index of the maximum value of 𝑦(𝑖)𝑡𝑟𝑢𝑒 to determine the 𝛿(𝑦(𝑖)𝑝𝑟𝑒𝑑,𝑦(𝑖)𝑡𝑟𝑢𝑒). \n",
        "- It then computes the accuracy in the same way as for binary classification case. \n",
        "\n",
        "*Note*: The accuracy of binary classificaton problems is the same, no matter if we use a sigmoid or softmax activation function to obtain the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdAdCXETKLcZ"
      },
      "source": [
        "# Binary classification with softmax \n",
        "\n",
        "y_true = tf.constant([[0.0 , 1.0] , [1.0 , 0.0] , \n",
        "                      [1.0 , 0.0] , [0.0 , 1.0]])\n",
        "\n",
        "y_pred = tf.constant([[0.4 , 0.6] , [0.3 , 0.7] , \n",
        "                      [0.05 , 0.95] , [0.33 , 0.67]])\n",
        "\n",
        "accuracy = K.mean(K.equal(y_true , K.round(y_pred)))\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uevaddKLVm4"
      },
      "source": [
        "# Categorical classification when m > 2 (num_classes > 2)\n",
        "\n",
        "y_true = tf.constant([[0.0,1.0,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0]])\n",
        "y_pred = tf.constant([[0.4,0.6,0.0,0.0], [0.3,0.2,0.1,0.4], [0.05,0.35,0.5,0.1]])\n",
        "\n",
        "# We need to find the maximum index (argmax)\n",
        "accuracy = K.mean(K.equal(K.argmax(y_true , axis = -1) , K.argmax(y_pred , axis = -1)))\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF7ySDKvMDdN"
      },
      "source": [
        "Let's compile our model with different accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfpU9twLMQKi"
      },
      "source": [
        "# Compile the model with different accuracy \n",
        "\n",
        "# Binary Accuracy \n",
        "model.compile(optimizer = 'Adam' , \n",
        "              loss = 'sparse_categorical_crossentropy' , \n",
        "              metrics = [tf.keras.metrics.BinaryAccuracy(threshold = 0.5)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs9qStsvM42T"
      },
      "source": [
        "**Sparse Categorical Accuracy** \n",
        "\n",
        "Very similar metric to categorical accuracy with one major difference. \n",
        "\n",
        "That is the label `y_true` of each training examples is not expected to be a one hot encoded vector. \n",
        "\n",
        "But to be tensor consisting of single integer. This index is compared to the index of the maximum argument `y_pred`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBFbuc1GNC2x"
      },
      "source": [
        "# Using Sparse Categorical Accuracy \n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy' , \n",
        "              optimizer ='adam' , \n",
        "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVvxs4ANNi7u"
      },
      "source": [
        "**(Sparse) Top k - categorical accuracy**\n",
        "\n",
        "In top k-categorical accuracy, \n",
        "- Instead of computing how often the mdoel correctly predicts the label of a training example. \n",
        "- Here the metric computes how often the model has `y_true` in the top-k of if it's prediction. By default **k = 5**\n",
        "\n",
        "There is two version of it: \n",
        "- `tf.keras.metrics.SparseTopKCategoricalAccuracy()`\n",
        "- `tf.keras.metrics.TopKCategoricalAccuracy()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8TbxRsqPJy2"
      },
      "source": [
        "# Sparse Categorical \n",
        "def sparse_categorical_accuracy(y_true, y_pred ):\n",
        "    return K.cast( K.equal(K.max(y_true, axis=-1),\n",
        "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx()) ),\n",
        "                  K.floatx())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKUZn2aQ8_2"
      },
      "source": [
        "# Top k categorical\n",
        "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeYkzL_lP4de"
      },
      "source": [
        "# For the Sparse one \n",
        "\n",
        "def sparse_top_k_categorical_accuracy(y_true , y_pred , k= 5):\n",
        "  return K.mean(K.in_top_k(y_pred , K.cast(K.max(y_true , axis = -1), \n",
        "                                           'int32') , k) , axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyveNZz1RD39"
      },
      "source": [
        "Things to refer: \n",
        "- https://keras.io/metrics/\n",
        "- https://github.com/keras-team/keras/blob/master/keras/metrics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di7hAxhYRKBS"
      },
      "source": [
        "### The Fit method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLl4ck4rRgOl"
      },
      "source": [
        "# Let's build a model and fit it. \n",
        "model = Sequential([\n",
        "  Dense(64 , activation= 'elu' , input_shape = (32 ,)), \n",
        "  Dense(100 , activation= 'softmax')\n",
        "])\n",
        "\n",
        "# Compile the model \n",
        "model.compile(loss = 'categorical_crossentropy' , \n",
        "              optimizer = 'rmsprop' , \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the model \n",
        "# model.fit(X_train , y_train)\n",
        "\n",
        "# X_train (num_samples , num_features)\n",
        "# y_train (num_samples , num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIrrnnywSS8p"
      },
      "source": [
        "# Let's build a model and fit it. \n",
        "model = Sequential([\n",
        "  Dense(64 , activation= 'elu' , input_shape = (32 ,)), \n",
        "  Dense(100 , activation= 'softmax')\n",
        "])\n",
        "\n",
        "# Compile the model \n",
        "model.compile(loss = 'sparse_categorical_crossentropy' , \n",
        "              optimizer = 'rmsprop' , \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the model \n",
        "# history = model.fit(X_train , y_train , epochs = 10)\n",
        "\n",
        "# X_train (num_samples , num_features)\n",
        "# y_train (num_samples , ) # one dimensional array with lenght == num_samples\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqHSsUavSlUh"
      },
      "source": [
        "`history` --> TensorFlow history object which contains the record and progress of the model training. In terms of loss , metrics , epochs and whatever we pass in.\n",
        "\n",
        "It's time to work on a real data! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-T4Q4b6SlYw"
      },
      "source": [
        "# Load the Fashion-MNIST dataset \n",
        "\n",
        "fashion_mnist_data = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Splitting into sets \n",
        "(train_images , train_labels) , (test_images , test_labels) = fashion_mnist_data.load_data()\n",
        "\n",
        "\n",
        "# Checkin the shapes \n",
        "train_images.shape , train_labels.shape , test_images.shape , test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHvg6yQJSliU"
      },
      "source": [
        "# Building a Convolutional model \n",
        "\n",
        "model = Sequential([\n",
        "  Conv2D(16 , 3 , activation= 'relu' , input_shape = (28 , 28 , 1)), \n",
        "  MaxPooling2D(3) , \n",
        "  Flatten() , \n",
        "  Dense(10 , activation= 'softmax')\n",
        "])\n",
        "\n",
        "# Model summary \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHQ7P1otq8f6"
      },
      "source": [
        "# Compiling the model \n",
        "acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "\n",
        "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy() , \n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate= 0.005) , \n",
        "              metrics = [acc , mae] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkgx39FrON6"
      },
      "source": [
        "# Model Attributes \n",
        "print(model.loss)\n",
        "print(model.optimizer)\n",
        "print(model.metrics)\n",
        "print(model.optimizer.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAes7uNHrXq0"
      },
      "source": [
        "# Labels for our data \n",
        "\n",
        "labels = [\n",
        "    'T-shirt/top',\n",
        "    'Trouser',\n",
        "    'Pullover',\n",
        "    'Dress',\n",
        "    'Coat',\n",
        "    'Sandal',\n",
        "    'Shirt',\n",
        "    'Sneaker',\n",
        "    'Bag',\n",
        "    'Ankle boot'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae6blLDIr0hc"
      },
      "source": [
        "# Rescaling the values betwene 0 - 1 \n",
        "train_images = train_images / 255.\n",
        "test_images = test_images / 255. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eomNXzbDsIeE"
      },
      "source": [
        "# Display one of the images \n",
        "i = 8899\n",
        "img = train_images[i , : , :]\n",
        "plt.imshow(img)\n",
        "print(f'The labels of the image is: {labels[train_labels[i]]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koxRAbZ9s8hW"
      },
      "source": [
        "tf.rank(train_images), tf.rank(train_images[... , np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0-9wVDjtV5j"
      },
      "source": [
        "tf.rank(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wyheTgRtiKf"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMUvAC_JsOiE"
      },
      "source": [
        "# Fitting the model \n",
        "\n",
        "history = model.fit(train_images[... , np.newaxis] , train_labels , epochs = 10 , \n",
        "                    batch_size = 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snR5F481srdQ"
      },
      "source": [
        "# Our history dataframe \n",
        "\n",
        "import pandas as pd \n",
        "df = pd.DataFrame(history.history)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtiDxDs0u3um"
      },
      "source": [
        "# Make a plot for the loss \n",
        "\n",
        "loss_plot = df.plot(y = 'loss' , title = 'Loss vs Epochs' , legend = False)\n",
        "loss_plot.set(xlabel = 'Epochs' , ylabel = 'Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSiPyOsvWX6"
      },
      "source": [
        "### Evaluate and Predict methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYhuBR36S_ko"
      },
      "source": [
        "# Evaluate on the test dataset \n",
        "\n",
        "tets_loss , test_accuracy , test_mae =  model.evaluate(test_images[... , np.newaxis] , test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNRNIrnWh1c"
      },
      "source": [
        "test_images[np.newaxis,...,np.newaxis].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjwp40k7TPkq"
      },
      "source": [
        "# Make predictions from the model \n",
        "\n",
        "random_inx = np.random.choice(test_images.shape[0])\n",
        "\n",
        "test_image = test_images[random_inx]\n",
        "plt.imshow(test_image)\n",
        "plt.show()\n",
        "print(f'Label: {labels[test_labels[random_inx]]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcbmcD7OWAQI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaqsDDa_aKO"
      },
      "source": [
        "## Validation ,Regularizations and Callbacks \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKmnDxAb_a-l"
      },
      "source": [
        "### Validation Sets\n",
        "Validation set is used to measure how well our models performing outside the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djQyWOEX_i7j"
      },
      "source": [
        "# Loading the data \n",
        "\n",
        "from sklearn.datasets import load_diabetes \n",
        "diabetes_dataset = load_diabetes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i__O3Wr_AXwa"
      },
      "source": [
        "# Save the input and target variab;e \n",
        "\n",
        "inputs = diabetes_dataset['data']\n",
        "targets = diabetes_dataset['target']\n",
        "\n",
        "# Checking the shape \n",
        "inputs.shape , targets.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8iWdVrAAiFa"
      },
      "source": [
        "# Spread of targets \n",
        "min(targets) , max(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaFwd2NZAvKn"
      },
      "source": [
        "# Since there is huge spread in targets,normalizing the target data (will make clearer training curve )\n",
        "\n",
        "targets = (targets - targets.mean(axis = 0)) / targets.std() \n",
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4lC91ucBDUn"
      },
      "source": [
        "# Split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data , test_data , train_targets , test_targets = train_test_split(inputs , targets , test_size = 0.2)\n",
        "\n",
        "train_data.shape , train_targets.shape , test_data.shape , test_targets.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZbhD27zBGWH"
      },
      "source": [
        "**Train a feeforward neural network model** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22kqGXthBfqQ"
      },
      "source": [
        "# Importing the things we need \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def get_model():\n",
        "  model = Sequential([\n",
        "    Dense(128 , activation= 'relu' , input_shape = (train_data.shape[1], )),\n",
        "    Dense(128 , activation = 'relu'),\n",
        "    Dense(128 , activation= 'relu'), \n",
        "    Dense(128 , activation='relu'), \n",
        "    Dense(128 , activation='relu'), \n",
        "    Dense(128 , activation='relu'), \n",
        "    Dense(1)\n",
        "  ])\n",
        "\n",
        "  return model \n",
        "\n",
        "# Instantiating the model \n",
        "model = get_model()\n",
        "\n",
        "# SUmmary of the model \n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvB3_ftRDGv9"
      },
      "source": [
        "train_data.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0KRB3DnBrhq"
      },
      "source": [
        "# Compile the model \n",
        "model.compile(optimizer= 'adam' , \n",
        "              loss = 'mse' , \n",
        "              metrics = ['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiGpuWAgBtwr"
      },
      "source": [
        "# Train the model \n",
        "history = model.fit(train_data , train_targets , epochs = 100 , \n",
        "                    validation_split = 0.15 , batch_size = 64 , \n",
        "                    verbose = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "944ucf89CpKS"
      },
      "source": [
        "# Evaluate the model on test data \n",
        "model.evaluate(test_data , test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yuI9pZKD67i"
      },
      "source": [
        "# Plotting the learning curves \n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training' , 'Validation'] , loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8BevJAEXdz"
      },
      "source": [
        "Hmm.. our model is severly overfitting. Let's in next module how to overcome this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71bowrPoEoSV"
      },
      "source": [
        "### Regularizations \n",
        "\n",
        "Techniques used to avoid overfitting into the model training and have the effect of constraining the model capacity in preventing overfitting. \n",
        "\n",
        "We'll look at: \n",
        "- **L2 regularization** -> weight decay in neural network. \n",
        "- **L1 regularization** \n",
        "- How to use Dropouts in our models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU1bPtKcEtfm"
      },
      "source": [
        "# Creating model with regularization works for both Dense and Conv layers \n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64 , activation= 'relu' , \n",
        "        kernel_regularizer = tf.keras.regularizers.l2(0.001)), \n",
        "  \n",
        "])\n",
        "\n",
        "# Compile the model \n",
        "model.compile(loss = 'binary_crossentropy' , \n",
        "              optimizer = 'adadelta' , \n",
        "              metrics = ['acc'])\n",
        "\n",
        "# Fitting \n",
        "# model.fit(inputs ,targts , validation_split= 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnfRao-HiZH"
      },
      "source": [
        "# Let's look into l1 regularizers \n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64 , activation = 'relu' , \n",
        "        kernel_regularizer = tf.keras.regularizers.l1(0.005)) ,\n",
        "  Dense(1 , activation = 'sigmoid')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK08F7TkHigx"
      },
      "source": [
        "# Using both l1 and l2 regularizers\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64 , activation = 'relu' , \n",
        "        kernel_regularizer = tf.keras.regularizers.l1_l2(l1 = 0.005 , \n",
        "                                                         l2 = 0.001)) ,\n",
        "  Dense(1 , activation = 'sigmoid')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hoNbmzUHizz"
      },
      "source": [
        "**Regularizer for Bias** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGowJ3UJ9je"
      },
      "source": [
        "# Regularizer for bias\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64 , activation = 'relu' , \n",
        "        kernel_regularizer = tf.keras.regularizers.l1_l2(l1 = 0.005 ,l2 = 0.001) , \n",
        "        bias_regularizer = tf.keras.regularizers.l2(0.001)) ,\n",
        "  Dense(1 , activation = 'sigmoid')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z9mf8CoKapD"
      },
      "source": [
        "**Dropout layer**\n",
        "\n",
        "Dropout also has a regularizing effect on the neural network. We can add it as just like another layer. \n",
        "\n",
        "The dropout layer accepts an argument called `dropout_rate`, \n",
        "- the rate has been set to 0.5. \n",
        "- that mean that each weight connection between two dense layers is set to **zero** with probability 0.5. \n",
        "This is also known as **Bernoulli Dropout**, since the weights are effectively being multiplied by a bernoulli random variable. \n",
        "\n",
        "Each of the weights are randomly dropped out independently from one another and Dropout has also applied independently across each element in the batch at training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZQSifhUKnRo"
      },
      "source": [
        "# Dropout layer\n",
        "model = Sequential([\n",
        "  Dense(64 , activation = 'relu' , \n",
        "        kernel_regularizer = tf.keras.regularizers.l1_l2(l1 = 0.005 ,l2 = 0.001) , \n",
        "        bias_regularizer = tf.keras.regularizers.l2(0.001)) ,\n",
        "  Dropout(0.5),\n",
        "  Dense(1 , activation = 'sigmoid')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km2fzoLCRY_M"
      },
      "source": [
        "There are certain mode which comes when we use `Dropout` layer,\n",
        "- Training mode , with dropout. Here the weights will get dropped randomly during training .This happens during the `model.fit()` method.\n",
        "- Testing mode, no dropout. We stop dropping the weights randomly here. This happens during the methods, \n",
        "  - `model.evaluate()`\n",
        "  - `model.predict()`\n",
        "These two modes are handled behind the scenes. \n",
        "\n",
        "We can even control this two modes later in the course, by having more control over the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSE4auDzSg3z"
      },
      "source": [
        "# Getting back our overfitting model to fix it \n",
        "# Importing the things we need \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def get_model(wd , rate):\n",
        "  '''\n",
        "  wd --> Weight decay \n",
        "  rate --> dropout rate \n",
        "  '''\n",
        "  model = Sequential([\n",
        "    Dense(128 , activation= 'relu' ,\n",
        "          kernel_regularizer = regularizers.l2(wd), input_shape = (train_data.shape[1], )),\n",
        "    Dropout(rate),\n",
        "    Dense(128 , activation = 'relu' , kernel_regularizer = regularizers.l2(wd)),\n",
        "    Dropout(rate),\n",
        "    Dense(128 , activation= 'relu' , kernel_regularizer = regularizers.l2(wd)), \n",
        "    Dropout(rate),\n",
        "    Dense(128 , activation='relu', kernel_regularizer = regularizers.l2(wd)), \n",
        "    Dropout(rate),\n",
        "    Dense(128 , activation='relu' , kernel_regularizer = regularizers.l2(wd)), \n",
        "    Dropout(rate),\n",
        "    Dense(128 , activation='relu' , kernel_regularizer = regularizers.l2(wd)), \n",
        "    Dropout(rate),\n",
        "    Dense(1)\n",
        "  ])\n",
        "\n",
        "  return model \n",
        "\n",
        "# Instantiating the model \n",
        "model = get_model(1e-5 , 0.3)\n",
        "\n",
        "# SUmmary of the model \n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Un1RwwfTQKZ"
      },
      "source": [
        "# Compile the model \n",
        "model.compile(loss = 'mse' , \n",
        "              optimizer = 'adam' , \n",
        "              metrics = ['mae'])\n",
        "\n",
        "# Training the model and see the performance \n",
        "history = model.fit(train_data , train_targets , validation_split = 0.2 ,\n",
        "                    epochs = 100 , batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4fVZqLFUUMH"
      },
      "source": [
        "# Evaluate the performance of the model on test data \n",
        "model.evaluate(test_data , test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJHdBJUOUe9P"
      },
      "source": [
        "# Plotting the learning curves \n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training' , 'Validation'] , loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS9BCu33UiSO"
      },
      "source": [
        "Great!! Our loss is less compared to the un-regularized model. That's brilliant! \n",
        "\n",
        "Though the overfitting isn't completely fixed, but our regularizer played an significant effect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcleMnO0QlgI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
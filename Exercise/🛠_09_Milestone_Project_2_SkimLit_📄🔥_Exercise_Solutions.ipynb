{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ðŸ›  09. Milestone Project 2: SkimLit ðŸ“„ðŸ”¥ Exercise Solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Exercise/%F0%9F%9B%A0_09_Milestone_Project_2_SkimLit_%F0%9F%93%84%F0%9F%94%A5_Exercise_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG61Lb6RbNxR"
      },
      "source": [
        "# ðŸ›  09. Milestone Project 2: SkimLit ðŸ“„ðŸ”¥ Exercise Solutions.\n",
        "\n",
        "> **Note** The orders of the exercise is mixed. \n",
        "\n",
        "\n",
        "1. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
        "  - Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
        "  - It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.\n",
        "\n",
        "2. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?\n",
        "\n",
        "  - Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n",
        "  - Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf. \n",
        "\n",
        "3. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this effect model performance?\n",
        "  - Another example: `line_number=1` and total_lines=11 turns into `line_of_X=1_of_11`.\n",
        "\n",
        "4. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
        "  - `tf.keras.callbacks.ModelCheckpoint` to save the model's best weights only.\n",
        "  - `tf.keras.callbacks.EarlyStopping` to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n",
        "\n",
        "5. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:\n",
        "```\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "```\n",
        "You can find your own unstrcutured RCT abstract from PubMed or try this one from: [Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection.](https://pubmed.ncbi.nlm.nih.gov/22244707/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFYi9c4d1Zd"
      },
      "source": [
        "## Downloading the data and preprocessing it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKxWATW0h8fg"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsxPlSIiZ25",
        "outputId": "9c7b085b-4248-4869-861b-2d4110a15310"
      },
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pubmed-rct'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 30\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16hM2pBoic6t"
      },
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCqWBxzGiiq4",
        "outputId": "ea6706d5-0343-4dfe-9c53-685e01cb8131"
      },
      "source": [
        "# Check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjT1z2LxilII"
      },
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    return f.readlines()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPuqmqipipRL"
      },
      "source": [
        "# Creating a preprocessing function that returns a dictionary\n",
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract and what sentence number\n",
        "  the target line is.\n",
        "\n",
        "  Args:\n",
        "      filename: a string of the target text file to read and extract line data\n",
        "      from.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries each containing a line from an abstract,\n",
        "      the lines label, the lines position in the abstract and the total number\n",
        "      of lines in the abstract where the line is from. For example:\n",
        "\n",
        "      [{\"target\": 'CONCLUSION',\n",
        "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
        "        \"line_number\": 8,\n",
        "        \"total_lines\": 8}]\n",
        "  \"\"\"\n",
        "  input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "  \n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    \n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "  \n",
        "  return abstract_samples"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMc27jMgi29r",
        "outputId": "c47e0826-6fe1-4639-ac3b-891af29b0c91"
      },
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") \n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
        "\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 396 ms, sys: 88.1 ms, total: 484 ms\n",
            "Wall time: 484 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "lezgMjlQi9Ab",
        "outputId": "25ebefc0-095e-4294-81bd-4aedfc26fe1b"
      },
      "source": [
        "# Loading our data into a dataframe\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the outcome measures in rheumatology clinical ...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>emotional eating is associated with overeating...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>yet , empirical evidence for individual ( trai...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         target  ... total_lines\n",
              "0     OBJECTIVE  ...          11\n",
              "1       METHODS  ...          11\n",
              "2       METHODS  ...          11\n",
              "3       METHODS  ...          11\n",
              "4       METHODS  ...          11\n",
              "5       METHODS  ...          11\n",
              "6       RESULTS  ...          11\n",
              "7       RESULTS  ...          11\n",
              "8       RESULTS  ...          11\n",
              "9       RESULTS  ...          11\n",
              "10      RESULTS  ...          11\n",
              "11  CONCLUSIONS  ...          11\n",
              "12   BACKGROUND  ...          10\n",
              "13   BACKGROUND  ...          10\n",
              "\n",
              "[14 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QId1cyshjEEC",
        "outputId": "e25f785b-b89c-4ec3-f9d5-4fe4cce23967"
      },
      "source": [
        "# Convert abstract text lines into lists \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mBzQ4K_jHmP",
        "outputId": "e069815c-59d5-4131-ca19-f48c8d5ecffc"
      },
      "source": [
        "# One hot encoding the labels \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmqMBV3ejNjg",
        "outputId": "8393b395-9172-43ae-d9fe-e163d014da02"
      },
      "source": [
        "# Extract labels and encoder them into integers \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "\n",
        "label_encoder = LabelEncoder() \n",
        "\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HBTkxEKjdlu",
        "outputId": "e7870d5e-540c-454c-c214-c5e1023c31ce"
      },
      "source": [
        "# Get class names and number of classes from LabelEncoder instance \n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes , class_names"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQfKDXoXjzqX"
      },
      "source": [
        "\n",
        "### 1. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5P0kD6Cli7c",
        "outputId": "89d4d474-e847-4895-e07d-02e3dc26d640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loading the pre-trained embeddings \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-02 08:25:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-02 08:25:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-02 08:25:09--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.46MB/s    in 2m 44s  \n",
            "\n",
            "2021-09-02 08:27:53 (5.02 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5lsTdxjm3BY",
        "outputId": "58bb61e7-e55c-4145-fd90-8aa90f3dd89f"
      },
      "source": [
        "# Getting the path of the glove embedding (using 100D)\n",
        "import numpy as np \n",
        "glove_path = 'glove.6B.200d.txt'\n",
        "\n",
        "embedding_index = {}\n",
        "\n",
        "# Making dict of vector representtion of the words (s --> [8, 48......])\n",
        "with open(glove_path) as f:\n",
        "  for line in f:\n",
        "    \n",
        "    # Getting the words and coef in a variable \n",
        "    word , coefs = line.split(maxsplit = 1)\n",
        "    coefs = np.fromstring(coefs , 'f' , sep = ' ')\n",
        "    \n",
        "    # Adding the coefs to our embedding dict \n",
        "    embedding_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embedding_index)} word vectors')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BOJqK3Ynv0F"
      },
      "source": [
        "Great we loaded in the data and the next step will be creating a corresponding embedding matrix. So we can fit our `embedding_index` to our Embedding layer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_P-fPKCytqi"
      },
      "source": [
        "# Getting the sentences and characters \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "\n",
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKO4FwjpJDr"
      },
      "source": [
        "# Creatinga a text vectorizaiton layer (68k vocab size from the paper itself)\n",
        "from tensorflow.keras.layers import TextVectorization \n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens= 68000 , \n",
        "                                    output_sequence_length = 56)\n",
        "\n",
        "# Adapt our text vectorizer to training sentences\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olNGzXBmxyUt",
        "outputId": "6e1a8695-b33a-4948-bdf6-ea98ec4f62e2"
      },
      "source": [
        "# Getting the vocabulary of the vectorizer \n",
        "text_vocab = text_vectorizer.get_vocabulary()\n",
        "len(text_vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64841"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxYjS4u5n6q"
      },
      "source": [
        "# Getting the dict mapping word --> index \n",
        "word_index_text = dict(zip(text_vocab , range(len(text_vocab))))\n",
        "#word_index_char = dict(zip(char_vocab , range(len(char_vocab))))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLm2mfPHbt0v"
      },
      "source": [
        "# Vectorizer for the character level embedding\n",
        "\n",
        "# Get all keyboard characters for char-level embedding\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "\n",
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token\n",
        "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,  \n",
        "                                    output_sequence_length=290,\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    name=\"char_vectorizer\")\n",
        "\n",
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1SibA0_2zY0"
      },
      "source": [
        "char_vocab = char_vectorizer.get_vocabulary()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfDOTBaWfLom"
      },
      "source": [
        "# Creating a function that will give us a embedding matrix \n",
        "def get_glove_embedding_matrix(num_tokens , embedding_dim , word_index):\n",
        "\n",
        "  # Defining the hits and misses here \n",
        "  hits , misses = 0 , 0\n",
        "\n",
        "  # Prepare the embedding matrix \n",
        "  embedding_matrix = np.zeros((num_tokens , embedding_dim ))\n",
        "  for word , i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector \n",
        "      hits += 1 \n",
        "    else:\n",
        "      misses += 1 \n",
        "\n",
        "  return embedding_matrix , hits , misses"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kMAsx4idtuS",
        "outputId": "b208e1ae-eaf3-40db-f4aa-8e02825e2b25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using the above function to get the embedding matrix \n",
        "\n",
        "num_tokens_text = len(text_vocab) + 2 \n",
        "#num_tokens_char = len(char_vocab) + 2 \n",
        "embedding_dim = 100\n",
        "\n",
        "sentence_embedding_matrix , hits_ , misses_ = get_glove_embedding_matrix(num_tokens_text , embedding_dim, word_index_text)\n",
        "#char_embedding_matrix , hits , misses = get_glove_embedding_matrix(num_tokens_char , embedding_dim , word_index_char)\n",
        "\n",
        "\n",
        "print(f'Hits: {hits_} and Misses: {misses_} for the sentence embedding matrix')\n",
        "#print(f'Hits: {hits_} and Misses: {misses} for the character level embedding matrix')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hits: 29730 and Misses: 35111 for the sentence embedding matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoE_WjTp5UFO"
      },
      "source": [
        "# Adding the embedding matrix to our Embedding layer (Sentence and characters)\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "sen_embedding_layer = Embedding(num_tokens_text , \n",
        "                            embedding_dim , \n",
        "                            embeddings_initializer = tf.keras.initializers.Constant(sentence_embedding_matrix) , \n",
        "                            trainable = False )\n",
        "\n",
        "# char_embedding_layer = Embedding(num_tokens_char , \n",
        "#                                  embedding_dim , \n",
        "#                                  embeddings_initializer = tf.keras.initializers.Constant(char_embedding_matrix) , \n",
        "#                                  trainable = False)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4azUa3cWgt8y"
      },
      "source": [
        "Before making the datasets, we gotta convert our string's into numerical values with the help of the `vectorizer` layers we have created for our both sentence and characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5eKijpyg_Yn",
        "outputId": "e5c8be74-c34d-425e-c65e-f67bd238196a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating the datasets for our both sentences and chars  \n",
        "\n",
        "train_sen_vectors = text_vectorizer(np.array([[sen] for sen in train_sentences])).numpy()\n",
        "val_sen_vectors = text_vectorizer(np.array([[sen] for sen in val_sentences])).numpy()\n",
        "\n",
        "# train_char_vectors = char_vectorizer(np.array([[char] for char in train_chars])).numpy()\n",
        "# val_char_vectors = char_vectorizer(np.array([[char] for char in val_chars])).numpy()\n",
        "\n",
        "\n",
        "# # Converting the above into tf.data datasets \n",
        "# train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sen_vectors , train_char_vectors))\n",
        "# val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sen_vectors , val_char_vectors))\n",
        "\n",
        "# train_label_ds = tf.data.Dataset.from_tensor_slices(train_labels_encoded)\n",
        "# val_label_ds = tf.data.Dataset.from_tensor_slices(val_labels_encoded)\n",
        "\n",
        "# # Zipping the datasets\n",
        "# train_char_token_ds = tf.data.Dataset.zip((train_char_token_data , train_label_ds))\n",
        "# val_char_token_ds = tf.data.Dataset.zip((val_char_token_data , val_label_ds))\n",
        "\n",
        "# Training and validation dataset \n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_sen_vectors , train_labels_encoded))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_sen_vectors , val_labels_encoded))\n",
        "\n",
        "\n",
        "# Applying the batch size and prefetching (performance optimization )\n",
        "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "train_ds,  val_ds"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: ((None, 56), (None,)), types: (tf.int64, tf.int64)>,\n",
              " <PrefetchDataset shapes: ((None, 56), (None,)), types: (tf.int64, tf.int64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtbL1JJqho3L"
      },
      "source": [
        "Perfect! Now we're gonna build a model that will use glove embeddings as the core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-IoBavcRQkB",
        "outputId": "8265923d-88f3-4752-f2f1-9e7dadb2c7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the token inputs \n",
        "token_input = layers.Input(shape = () , dtype = tf.int64 , name = 'token_input_layer')\n",
        "# token_glove_embeddings = \n",
        "token_output = sen_embedding_layer(token_input)\n",
        "#tok_dim = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(token_glove_embeddings)\n",
        "#token_conv = layers.Conv1D(128 , 5 , activation = 'relu' )(tok_dim)\n",
        "#token_output = layers.MaxPooling1D(5 , padding = 'same')(token_conv)\n",
        "#token_output = layers.Dense(128 , activation= 'relu')(token_glove_embeddings)\n",
        "token_model = tf.keras.Model(inputs = token_input , \n",
        "                             outputs = token_output)\n",
        "\n",
        "# Set the char input model\n",
        "char_input = layers.Input(shape = () , dtype= tf.int64 , name = 'char_input_layer')\n",
        "#char_glove_embeddings = char_embedding_layer(char_input)\n",
        "char_output =  char_embedding_layer(char_input)\n",
        "# char_dim = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(char_glove_embeddings)\n",
        "# char_conv = layers.Conv1D(128 , 5 , activation= 'relu')(char_dim)\n",
        "# char_output = layers.MaxPooling1D(5 , padding= 'same')(char_conv)\n",
        "#char_output = layers.Dense(128 , activation= 'relu')(char_glove_embeddings)\n",
        "\n",
        "char_model = tf.keras.Model(char_input , char_output)\n",
        "\n",
        "# Concatenate token and char inputs (hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name = 'token_char_hybrid')([token_model.output , char_model.output])\n",
        "\n",
        "# Create the output layers \n",
        "dim = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(token_char_concat)\n",
        "conv = layers.Conv1D(128 , 5 , activation = 'relu' , padding = \"same\")(dim)\n",
        "pool = layers.MaxPooling1D(2 , padding = 'same')(conv)\n",
        "global_maxpool = layers.GlobalMaxPooling1D()(pool)\n",
        "x = layers.Dense(128 , activation= 'relu')(global_maxpool)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "output_layer = layers.Dense(len(class_names) , activation = 'softmax')(x)\n",
        "\n",
        "# Packing into a model \n",
        "glove_model = tf.keras.Model(inputs = [token_model.input , char_model.input] , \n",
        "                             outputs = output_layer , \n",
        "                             name = 'Glove_embedding_model') \n",
        "\n",
        "glove_model.summary()\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Glove_embedding_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "token_input_layer (InputLayer)  [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "char_input_layer (InputLayer)   [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         multiple             6484300     token_input_layer[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         multiple             3000        char_input_layer[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "token_char_hybrid (Concatenate) (None, 200)          0           embedding_1[19][0]               \n",
            "                                                                 embedding_2[11][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 1, 200)       0           token_char_hybrid[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 1, 128)       128128      lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling1D) (None, 1, 128)       0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 128)          0           max_pooling1d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          16512       global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 5)            645         dropout_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,632,585\n",
            "Trainable params: 145,285\n",
            "Non-trainable params: 6,487,300\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53yxu1PE2Yk9",
        "outputId": "92e25164-f7dc-4da9-cb35-574aa93dccf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_sen_vectors[0].shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56,)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsopqN9tkccx",
        "outputId": "79207844-cc63-4df9-d3af-b55aa3be623b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sample \n",
        "input = layers.Input(shape = (None,) , dtype = 'int64')\n",
        "glove_emb = sen_embedding_layer(input)\n",
        "#sample_emb = embedding_layer(sample_tokens)\n",
        "x = layers.Conv1D(128 , 5 , activation= 'relu' , padding = 'same')(glove_emb)\n",
        "x = layers.MaxPooling1D(5, padding = 'same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.MaxPooling1D(5 , padding ='same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(len(class_names) , activation= 'softmax')(x)\n",
        "\n",
        "glove_model = tf.keras.Model(input , output)\n",
        "glove_model.summary()\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        multiple                  6484300   \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 6,729,681\n",
            "Trainable params: 245,381\n",
            "Non-trainable params: 6,484,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgqA8gTnTWHM"
      },
      "source": [
        "Now we gotta convert our list of string data to Numpy arrays of integer indicees. Th"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqbLlB2CSxDx",
        "outputId": "aef94bcc-2404-478d-dd91-b5947580c460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compiling and fitting the model\n",
        "glove_model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy() , \n",
        "                     optimizer = tf.keras.optimizers.Adam(), \n",
        "                     metrics = ['accuracy'])\n",
        "\n",
        "glove_model.fit(train_ds,\n",
        "                 epochs = 3 , \n",
        "                 validation_data = val_ds)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5627/5627 [==============================] - 22s 4ms/step - loss: 0.6537 - accuracy: 0.7588 - val_loss: 0.5387 - val_accuracy: 0.8047\n",
            "Epoch 2/3\n",
            "5627/5627 [==============================] - 21s 4ms/step - loss: 0.5301 - accuracy: 0.8084 - val_loss: 0.5207 - val_accuracy: 0.8121\n",
            "Epoch 3/3\n",
            "5627/5627 [==============================] - 21s 4ms/step - loss: 0.4854 - accuracy: 0.8245 - val_loss: 0.5542 - val_accuracy: 0.8028\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fecc1af8a50>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0IUpoQ-hWsu"
      },
      "source": [
        "### 2. PubMed Bert \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyMvY8zH8Fhp",
        "outputId": "cbb5f2e8-c377-40bc-d080-d662cad351a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download by uncommenting the below command\n",
        "!pip install tensorflow_text "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.4 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.39.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.34.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klo8Oa4rhqbk"
      },
      "source": [
        "# Loading in the both encoder and the preprocessing models \n",
        "import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# preprocess_bert = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "# bert = hub.load('')\n",
        "\n",
        "preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3' ,\n",
        "                                     trainable = False , name = 'pubmed_bert_preprocessor')\n",
        "\n",
        "bert_layer = hub.KerasLayer('https://tfhub.dev/google/experts/bert/pubmed/2' ,\n",
        "                            trainable = False , \n",
        "                            name = 'bert_model_layer')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQRjGWgVyS5u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ndj9nB7cGm",
        "outputId": "199a7f97-8f93-480d-c71e-17f4a3aeb533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating a model out of it \n",
        "input = layers.Input(shape = [] , dtype = tf.string , name = 'input_sentences')\n",
        "bert_inputs = preprocessing_layer(input)\n",
        "bert_embedding =bert_layer(bert_inputs)\n",
        "print(f'bert embedding shape: {bert_embedding}')\n",
        "##x = layers.Lambda(lambda x: tf.expand_dims(x, axis = 1))(bert_embedding['pooled_output'])\n",
        "x = layers.Dense(128 , activation = 'relu')(bert_embedding['pooled_output'])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(len(class_names) , activation= 'softmax')(x)\n",
        "\n",
        "# Packing into a model\n",
        "pubmed_bert_model = tf.keras.Model(input , output)\n",
        "pubmed_bert_model.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert embedding shape: {'sequence_output': <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, 'default': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'bert_model_layer')>, 'pooled_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'bert_model_layer')>, 'encoder_outputs': [<KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>]}\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_sentences (InputLayer)    [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "pubmed_bert_preprocessor (Keras {'input_type_ids': ( 0           input_sentences[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bert_model_layer (KerasLayer)   {'sequence_output':  109482241   pubmed_bert_preprocessor[11][0]  \n",
            "                                                                 pubmed_bert_preprocessor[11][1]  \n",
            "                                                                 pubmed_bert_preprocessor[11][2]  \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 128)          98432       bert_model_layer[11][13]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 5)            645         dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,581,318\n",
            "Trainable params: 99,077\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU7dVhYl8ZrR",
        "outputId": "ab7b5a6b-8f5b-4dbc-d8ea-691dee08dfd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pubmed_bert_model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy() , \n",
        "                          optimizer = tf.keras.optimizers.Adam(), \n",
        "                          metrics =['accuracy'])\n",
        "\n",
        "pubmed_bert_model.fit(train_sen_ds ,\n",
        "                      steps_per_epoch = int(0.1 * len(train_sen_ds)),\n",
        "                      epochs = 3 , \n",
        "                      validation_data = val_sen_ds , \n",
        "                      validation_steps = int(0.1 * len(val_sen_ds)))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "562/562 [==============================] - 137s 241ms/step - loss: 0.6510 - accuracy: 0.7732 - val_loss: 0.4593 - val_accuracy: 0.8378\n",
            "Epoch 2/3\n",
            "562/562 [==============================] - 135s 240ms/step - loss: 0.5194 - accuracy: 0.8213 - val_loss: 0.4358 - val_accuracy: 0.8424\n",
            "Epoch 3/3\n",
            "562/562 [==============================] - 135s 240ms/step - loss: 0.4991 - accuracy: 0.8273 - val_loss: 0.4179 - val_accuracy: 0.8547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7febb2ab9290>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJzbPrT_8mDm"
      },
      "source": [
        "\n",
        "train_sen_ds = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
        "train_sen_ds = train_sen_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_sen_ds = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
        "val_sen_ds = val_sen_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5sDOzAL5Ss"
      },
      "source": [
        "### 3 : Boom "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7kRfrz2Vap6",
        "outputId": "8b5a7623-34bb-421b-8ee0-74260e6f25b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df[['line_number' , 'total_lines']].apply(lambda x: x[0].astype('int')  + x[1].astype('int'))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "line_number     1\n",
              "total_lines    22\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-dm3oSTVdJc",
        "outputId": "23f20f66-ad35-4a04-90e2-1f88f59a05f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        ""
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-0a0ec0c900b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1_of_12'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'astype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx8vJRgLYGo3",
        "outputId": "20d5667d-535b-4a8b-cf1d-99a3fc83645c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i , col in enumerate(train_df[['line_number' , 'total_lines']]):\n",
        "  print(i , col)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 line_number\n",
            "1 total_lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnvzXNktZ0-q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
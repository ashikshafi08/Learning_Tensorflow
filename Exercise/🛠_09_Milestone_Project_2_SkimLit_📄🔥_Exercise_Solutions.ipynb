{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ðŸ›  09. Milestone Project 2: SkimLit ðŸ“„ðŸ”¥ Exercise Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning_Tensorflow/blob/main/Exercise/%F0%9F%9B%A0_09_Milestone_Project_2_SkimLit_%F0%9F%93%84%F0%9F%94%A5_Exercise_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG61Lb6RbNxR"
      },
      "source": [
        "# ðŸ›  09. Milestone Project 2: SkimLit ðŸ“„ðŸ”¥ Exercise Solutions.\n",
        "\n",
        "> **Note** The orders of the exercise is mixed. \n",
        "\n",
        "\n",
        "1. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
        "  - Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
        "  - It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.\n",
        "\n",
        "2. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?\n",
        "\n",
        "  - Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n",
        "  - Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf. \n",
        "\n",
        "3. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this effect model performance?\n",
        "  - Another example: `line_number=1` and total_lines=11 turns into `line_of_X=1_of_11`.\n",
        "\n",
        "4. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
        "  - `tf.keras.callbacks.ModelCheckpoint` to save the model's best weights only.\n",
        "  - `tf.keras.callbacks.EarlyStopping` to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n",
        "\n",
        "5. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:\n",
        "```\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "PREDICTED_LABEL: SEQUENCE\n",
        "```\n",
        "You can find your own unstrcutured RCT abstract from PubMed or try this one from: [Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection.](https://pubmed.ncbi.nlm.nih.gov/22244707/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFYi9c4d1Zd"
      },
      "source": [
        "## Downloading the data and preprocessing it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKxWATW0h8fg"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsxPlSIiZ25",
        "outputId": "852eaa04-3609-4448-8f58-f6a3faaed0f9"
      },
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pubmed-rct'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 30\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16hM2pBoic6t"
      },
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCqWBxzGiiq4",
        "outputId": "659a8fae-768c-41b6-9987-baad0ca8898b"
      },
      "source": [
        "# Check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjT1z2LxilII"
      },
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    return f.readlines()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPuqmqipipRL"
      },
      "source": [
        "# Creating a preprocessing function that returns a dictionary\n",
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract and what sentence number\n",
        "  the target line is.\n",
        "\n",
        "  Args:\n",
        "      filename: a string of the target text file to read and extract line data\n",
        "      from.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries each containing a line from an abstract,\n",
        "      the lines label, the lines position in the abstract and the total number\n",
        "      of lines in the abstract where the line is from. For example:\n",
        "\n",
        "      [{\"target\": 'CONCLUSION',\n",
        "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
        "        \"line_number\": 8,\n",
        "        \"total_lines\": 8}]\n",
        "  \"\"\"\n",
        "  input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "  \n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    \n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "  \n",
        "  return abstract_samples"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMc27jMgi29r",
        "outputId": "602ceef0-8d4f-41e1-90c4-08c4723f4829"
      },
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") \n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
        "\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 488 ms, sys: 107 ms, total: 595 ms\n",
            "Wall time: 595 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "lezgMjlQi9Ab",
        "outputId": "be023cd3-c9cc-49fa-8cde-4b3b9b5124ad"
      },
      "source": [
        "# Loading our data into a dataframe\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the outcome measures in rheumatology clinical ...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>emotional eating is associated with overeating...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>yet , empirical evidence for individual ( trai...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         target  ... total_lines\n",
              "0     OBJECTIVE  ...          11\n",
              "1       METHODS  ...          11\n",
              "2       METHODS  ...          11\n",
              "3       METHODS  ...          11\n",
              "4       METHODS  ...          11\n",
              "5       METHODS  ...          11\n",
              "6       RESULTS  ...          11\n",
              "7       RESULTS  ...          11\n",
              "8       RESULTS  ...          11\n",
              "9       RESULTS  ...          11\n",
              "10      RESULTS  ...          11\n",
              "11  CONCLUSIONS  ...          11\n",
              "12   BACKGROUND  ...          10\n",
              "13   BACKGROUND  ...          10\n",
              "\n",
              "[14 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QId1cyshjEEC",
        "outputId": "6c987cd8-a5ee-41c2-d70d-f258dc918579"
      },
      "source": [
        "# Convert abstract text lines into lists \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mBzQ4K_jHmP",
        "outputId": "5311a9a4-d40c-4216-99f6-2a169c95f91d"
      },
      "source": [
        "# One hot encoding the labels \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmqMBV3ejNjg",
        "outputId": "77bf15d9-8def-4b70-fe3e-58cc97b8d209"
      },
      "source": [
        "# Extract labels and encoder them into integers \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "\n",
        "label_encoder = LabelEncoder() \n",
        "\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HBTkxEKjdlu",
        "outputId": "94894714-dc11-4f3e-fde2-be3b403cdbf5"
      },
      "source": [
        "# Get class names and number of classes from LabelEncoder instance \n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes , class_names"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQfKDXoXjzqX"
      },
      "source": [
        "\n",
        "### 1. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5P0kD6Cli7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39022b9-76e2-4456-d875-730d5af16aec"
      },
      "source": [
        "# Loading the pre-trained embeddings \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-11 22:38:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-11 22:38:12--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-11 22:38:12--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 41s  \n",
            "\n",
            "2021-09-11 22:40:53 (5.10 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5lsTdxjm3BY",
        "outputId": "b9259c15-9887-4665-e964-32f99e5ebf76"
      },
      "source": [
        "# Getting the path of the glove embedding (using 100D)\n",
        "import numpy as np \n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "\n",
        "embedding_index = {}\n",
        "\n",
        "# Making dict of vector representtion of the words (s --> [8, 48......])\n",
        "with open(glove_path) as f:\n",
        "  for line in f:\n",
        "    \n",
        "    # Getting the words and coef in a variable \n",
        "    word , coefs = line.split(maxsplit = 1)\n",
        "    coefs = np.fromstring(coefs , 'f' , sep = ' ')\n",
        "    \n",
        "    # Adding the coefs to our embedding dict \n",
        "    embedding_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embedding_index)} word vectors')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BOJqK3Ynv0F"
      },
      "source": [
        "Great we loaded in the data and the next step will be creating a corresponding embedding matrix. So we can fit our `embedding_index` to our Embedding layer. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_P-fPKCytqi"
      },
      "source": [
        "# Getting the sentences and characters \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "\n",
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKO4FwjpJDr"
      },
      "source": [
        "# Creatinga a text vectorizaiton layer (68k vocab size from the paper itself)\n",
        "from tensorflow.keras.layers import TextVectorization \n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens= 68000 , \n",
        "                                    output_sequence_length = 56)\n",
        "\n",
        "# Adapt our text vectorizer to training sentences\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olNGzXBmxyUt",
        "outputId": "cf37387e-87a4-4b24-b709-567bdb961010"
      },
      "source": [
        "# Getting the vocabulary of the vectorizer \n",
        "text_vocab = text_vectorizer.get_vocabulary()\n",
        "len(text_vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64841"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxYjS4u5n6q"
      },
      "source": [
        "# Getting the dict mapping word --> index \n",
        "word_index_text = dict(zip(text_vocab , range(len(text_vocab))))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfDOTBaWfLom"
      },
      "source": [
        "# Creating a function that will give us a embedding matrix \n",
        "def get_glove_embedding_matrix(num_tokens , embedding_dim , word_index):\n",
        "\n",
        "  # Defining the hits and misses here \n",
        "  hits , misses = 0 , 0\n",
        "\n",
        "  # Prepare the embedding matrix \n",
        "  embedding_matrix = np.zeros((num_tokens , embedding_dim ))\n",
        "  for word , i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector \n",
        "      hits += 1 \n",
        "    else:\n",
        "      misses += 1 \n",
        "\n",
        "  return embedding_matrix , hits , misses"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kMAsx4idtuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e313620-a06b-493f-f2b6-ba7270046608"
      },
      "source": [
        "# Using the above function to get the embedding matrix \n",
        "\n",
        "num_tokens_text = len(text_vocab) + 2 \n",
        "embedding_dim = 100\n",
        "\n",
        "sentence_embedding_matrix , hits_ , misses_ = get_glove_embedding_matrix(num_tokens_text , embedding_dim, word_index_text)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Hits: {hits_} and Misses: {misses_} for the sentence embedding matrix')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hits: 29730 and Misses: 35111 for the sentence embedding matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoE_WjTp5UFO"
      },
      "source": [
        "# Adding the embedding matrix to our Embedding layer (Sentence and characters)\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "sen_embedding_layer = Embedding(num_tokens_text , \n",
        "                            embedding_dim , \n",
        "                            embeddings_initializer = tf.keras.initializers.Constant(sentence_embedding_matrix) , \n",
        "                            trainable = False )\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4azUa3cWgt8y"
      },
      "source": [
        "Before making the datasets, we gotta convert our string's into numerical values with the help of the `vectorizer` layers we have created for our both sentence and characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5eKijpyg_Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2647f70e-3a4c-422a-c5c5-88ca7fd15590"
      },
      "source": [
        "# Creating the datasets for our both sentences and chars  \n",
        "\n",
        "train_sen_vectors = text_vectorizer(np.array([[sen] for sen in train_sentences])).numpy()\n",
        "val_sen_vectors = text_vectorizer(np.array([[sen] for sen in val_sentences])).numpy()\n",
        "\n",
        "# Training and validation dataset \n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_sen_vectors , train_labels_encoded))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_sen_vectors , val_labels_encoded))\n",
        "\n",
        "\n",
        "# Applying the batch size and prefetching (performance optimization )\n",
        "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "train_ds,  val_ds"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: ((None, 56), (None,)), types: (tf.int64, tf.int64)>,\n",
              " <PrefetchDataset shapes: ((None, 56), (None,)), types: (tf.int64, tf.int64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtbL1JJqho3L"
      },
      "source": [
        "Perfect! Now we're gonna build a model that will use glove embeddings as the core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53yxu1PE2Yk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa31618-662d-453e-8d01-9c1538786ec8"
      },
      "source": [
        "train_sen_vectors[0].shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsopqN9tkccx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32ae3f8-c709-4a58-a4ed-ca348fe7be69"
      },
      "source": [
        "# Sample \n",
        "input = layers.Input(shape = (None,) , dtype = 'int64')\n",
        "glove_emb = sen_embedding_layer(input)\n",
        "#sample_emb = embedding_layer(sample_tokens)\n",
        "x = layers.Conv1D(128 , 5 , activation= 'relu' , padding = 'same')(glove_emb)\n",
        "x = layers.MaxPooling1D(5, padding = 'same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.MaxPooling1D(5 , padding ='same')(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\" , padding = 'same')(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(len(class_names) , activation= 'softmax')(x)\n",
        "\n",
        "glove_model = tf.keras.Model(input , output)\n",
        "glove_model.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         6484300   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 6,729,681\n",
            "Trainable params: 245,381\n",
            "Non-trainable params: 6,484,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgqA8gTnTWHM"
      },
      "source": [
        "Now we gotta convert our list of string data to Numpy arrays of integer indicees. Th"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqbLlB2CSxDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b46d35-e728-4280-8466-80507c823a81"
      },
      "source": [
        "# Compiling and fitting the model\n",
        "glove_model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy() , \n",
        "                     optimizer = tf.keras.optimizers.Adam(), \n",
        "                     metrics = ['accuracy'])\n",
        "\n",
        "glove_model.fit(train_ds,\n",
        "                 epochs = 3 , \n",
        "                 validation_data = val_ds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5627/5627 [==============================] - 69s 7ms/step - loss: 0.6510 - accuracy: 0.7582 - val_loss: 0.5520 - val_accuracy: 0.7943\n",
            "Epoch 2/3\n",
            "5627/5627 [==============================] - 42s 8ms/step - loss: 0.5284 - accuracy: 0.8078 - val_loss: 0.5169 - val_accuracy: 0.8109\n",
            "Epoch 3/3\n",
            "5627/5627 [==============================] - 42s 7ms/step - loss: 0.4830 - accuracy: 0.8242 - val_loss: 0.5294 - val_accuracy: 0.8135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6e0c40f50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0IUpoQ-hWsu"
      },
      "source": [
        "### 2. PubMed Bert \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyMvY8zH8Fhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0c8698-7f66-4a04-d3dc-3684a2da528c"
      },
      "source": [
        "# Download by uncommenting the below command\n",
        "!pip install tensorflow_text "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.39.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.34.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klo8Oa4rhqbk"
      },
      "source": [
        "# Loading in the both encoder and the preprocessing models \n",
        "import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# preprocess_bert = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "# bert = hub.load('')\n",
        "\n",
        "preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3' ,\n",
        "                                     trainable = False , name = 'pubmed_bert_preprocessor')\n",
        "\n",
        "bert_layer = hub.KerasLayer('https://tfhub.dev/google/experts/bert/pubmed/2' ,\n",
        "                            trainable = False , \n",
        "                            name = 'bert_model_layer')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQRjGWgVyS5u"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ndj9nB7cGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863054d0-4605-4e78-ddb4-989dd7d9c205"
      },
      "source": [
        "# Creating a model out of it \n",
        "input = layers.Input(shape = [] , dtype = tf.string , name = 'input_sentences')\n",
        "bert_inputs = preprocessing_layer(input)\n",
        "bert_embedding =bert_layer(bert_inputs)\n",
        "print(f'bert embedding shape: {bert_embedding}')\n",
        "##x = layers.Lambda(lambda x: tf.expand_dims(x, axis = 1))(bert_embedding['pooled_output'])\n",
        "x = layers.Dense(128 , activation = 'relu')(bert_embedding['pooled_output'])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(len(class_names) , activation= 'softmax')(x)\n",
        "\n",
        "# Packing into a model\n",
        "pubmed_bert_model = tf.keras.Model(input , output)\n",
        "pubmed_bert_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert embedding shape: {'encoder_outputs': [<KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>], 'default': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'bert_model_layer')>, 'sequence_output': <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'bert_model_layer')>, 'pooled_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'bert_model_layer')>}\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_sentences (InputLayer)    [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "pubmed_bert_preprocessor (Keras {'input_word_ids': ( 0           input_sentences[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bert_model_layer (KerasLayer)   {'encoder_outputs':  109482241   pubmed_bert_preprocessor[0][0]   \n",
            "                                                                 pubmed_bert_preprocessor[0][1]   \n",
            "                                                                 pubmed_bert_preprocessor[0][2]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          98432       bert_model_layer[0][13]          \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 5)            645         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,581,318\n",
            "Trainable params: 99,077\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzG6QZyYAX5b"
      },
      "source": [
        "# Making datasets for the pubmed model\n",
        "\n",
        "train_sen_ds = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
        "train_sen_ds = train_sen_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_sen_ds = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
        "val_sen_ds = val_sen_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU7dVhYl8ZrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efd814f-fc11-4953-b197-3e70d6e5d10e"
      },
      "source": [
        "pubmed_bert_model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy() , \n",
        "                          optimizer = tf.keras.optimizers.Adam(), \n",
        "                          metrics =['accuracy'])\n",
        "\n",
        "pubmed_bert_model.fit(train_sen_ds ,\n",
        "                      steps_per_epoch = int(0.1 * len(train_sen_ds)),\n",
        "                      epochs = 3 , \n",
        "                      validation_data = val_sen_ds , \n",
        "                      validation_steps = int(0.1 * len(val_sen_ds)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "562/562 [==============================] - 432s 750ms/step - loss: 0.6410 - accuracy: 0.7789 - val_loss: 0.4614 - val_accuracy: 0.8361\n",
            "Epoch 2/3\n",
            "562/562 [==============================] - 420s 748ms/step - loss: 0.5197 - accuracy: 0.8195 - val_loss: 0.4484 - val_accuracy: 0.8268\n",
            "Epoch 3/3\n",
            "562/562 [==============================] - 420s 748ms/step - loss: 0.5009 - accuracy: 0.8255 - val_loss: 0.4161 - val_accuracy: 0.8567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa682747590>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5sDOzAL5Ss"
      },
      "source": [
        "### 3 : Creating new function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-dm3oSTVdJc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "79021c09-1290-4e6d-9e42-a0a018b05912"
      },
      "source": [
        "# Combining the total lines and line number into a new feature! \n",
        "train_df['line_number_total'] = train_df['line_number'].astype(str) + '_of_' + train_df['total_lines'].astype(str)\n",
        "val_df['line_number_total'] = val_df['line_number'].astype(str) + '_of_' + val_df['total_lines'].astype(str)\n",
        "\n",
        "train_df.head(10)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "      <th>line_number_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>3_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>4_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>5_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>6_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>7_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>8_of_11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>9_of_11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      target  ... line_number_total\n",
              "0  OBJECTIVE  ...           0_of_11\n",
              "1    METHODS  ...           1_of_11\n",
              "2    METHODS  ...           2_of_11\n",
              "3    METHODS  ...           3_of_11\n",
              "4    METHODS  ...           4_of_11\n",
              "5    METHODS  ...           5_of_11\n",
              "6    RESULTS  ...           6_of_11\n",
              "7    RESULTS  ...           7_of_11\n",
              "8    RESULTS  ...           8_of_11\n",
              "9    RESULTS  ...           9_of_11\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx8vJRgLYGo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafe5d56-d3d0-4f9f-a6ac-3fdffcf757fb"
      },
      "source": [
        "# Perform one hot encoding on the train and transform the validation dataframe \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Creating an instance \n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "# Fitting on the training dataframe \n",
        "one_hot_encoder.fit(np.expand_dims(train_df['line_number_total'] , axis = 1))\n",
        "\n",
        "# Transforming both train and val df \n",
        "train_line_number_total_encoded = one_hot_encoder.transform(np.expand_dims(train_df['line_number_total'] , axis =1))\n",
        "val_line_number_total_encoded  = one_hot_encoder.transform(np.expand_dims(val_df['line_number_total'] , axis= 1))\n",
        "\n",
        "# Checking the shapes \n",
        "train_line_number_total_encoded.shape , val_line_number_total_encoded.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((180040, 460), (30212, 460))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnvzXNktZ0-q"
      },
      "source": [
        "# Converting the sparse object to array \n",
        "train_line_number_total_encoded = train_line_number_total_encoded.toarray()\n",
        "val_line_number_total_encoded = val_line_number_total_encoded.toarray()\n",
        "\n",
        "# Converting the datatype to int \n",
        "train_line_number_total_encoded = tf.cast(train_line_number_total_encoded , dtype= tf.int32)\n",
        "val_line_number_total_encoded = tf.cast(val_line_number_total_encoded , dtype= tf.int32)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8fncMWlUkNQ"
      },
      "source": [
        "Now lets build a tribid model which use pubmed Bert as the embedding and model + our new `line_number_total` feature which is the combination of `line_number` and `total_lines`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBbgThiAWxos",
        "outputId": "1e590f6a-8fe9-4a2e-8280-cb1ef01e6d0d"
      },
      "source": [
        "# Making the performant datasets for our tribid model \n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sentences ,\n",
        "                                                 train_chars , \n",
        "                                                 train_line_number_total_encoded))\n",
        "\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(train_labels_encoded)\n",
        "\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val_sentences , \n",
        "                                               val_chars , \n",
        "                                               val_line_number_total_encoded))\n",
        "\n",
        "val_labels = tf.data.Dataset.from_tensor_slices(val_labels_encoded)\n",
        "\n",
        "# Zipping the data and labels \n",
        "train_dataset = tf.data.Dataset.zip((train_data , train_labels))\n",
        "val_dataset = tf.data.Dataset.zip((val_data , val_labels))\n",
        "\n",
        "# Applying batch and prefetching \n",
        "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset , val_dataset"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: (((None,), (None,), (None, 460)), (None,)), types: ((tf.string, tf.string, tf.int32), tf.int64)>,\n",
              " <PrefetchDataset shapes: (((None,), (None,), (None, 460)), (None,)), types: ((tf.string, tf.string, tf.int32), tf.int64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAdFeh1UX7l7",
        "outputId": "b92eefae-6eba-48fc-852d-28570df53c2e"
      },
      "source": [
        "# Buidling the tribid model \n",
        "\n",
        "input_token = layers.Input(shape = [] , dtype =tf.string)\n",
        "bert_inputs_token = preprocessing_layer(input_token)\n",
        "bert_embedding_char =bert_layer(bert_inputs_token)\n",
        "output_token = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
        "token_model = tf.keras.Model(input_token , output_token)\n",
        "\n",
        "input_char = layers.Input(shape = [] , dtype =tf.string)\n",
        "bert_inputs_char = preprocessing_layer(input_char)\n",
        "bert_embedding_char =bert_layer(bert_inputs_char)\n",
        "output_char = layers.Dense(64 , activation = 'relu')(bert_embedding_char['pooled_output'])\n",
        "char_model = tf.keras.Model(input_char , output_char)\n",
        "\n",
        "line_number_total_input = layers.Input(shape = (460,), dtype = tf.int32)\n",
        "dense = layers.Dense(32 , activation = 'relu')(line_number_total_input)\n",
        "total_line_number_model = tf.keras.Model(line_number_total_input , dense)\n",
        "\n",
        "# Concatenating the tokens amd chars output (Hybrid!!!)\n",
        "combined_embeddings = layers.Concatenate(name = 'token_char_hybrid_embedding')([token_model.output , \n",
        "                                                                                char_model.output])\n",
        "\n",
        "# Combining the line_number_total to our hybrid model (Time for Tribid!!)\n",
        "z = layers.Concatenate(name = 'tribid_embeddings')([total_line_number_model.output , \n",
        "                                                    combined_embeddings])\n",
        "\n",
        "# Adding a dense + dropout and creating our output layer \n",
        "dropout = layers.Dropout(0.5)(z)\n",
        "x = layers.Dense(128 , activation='relu')(dropout)\n",
        "output_layer = layers.Dense(5 , activation='softmax')(x)\n",
        "\n",
        "# Packing into a model\n",
        "tribid_model = tf.keras.Model(inputs = [token_model.input , \n",
        "                                        char_model.input , \n",
        "                                        total_line_number_model.input] , \n",
        "                              outputs = output_layer)\n",
        "\n",
        "tribid_model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "pubmed_bert_preprocessor (Keras {'input_word_ids': ( 0           input_9[0][0]                    \n",
            "                                                                 input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bert_model_layer (KerasLayer)   {'encoder_outputs':  109482241   pubmed_bert_preprocessor[5][0]   \n",
            "                                                                 pubmed_bert_preprocessor[5][1]   \n",
            "                                                                 pubmed_bert_preprocessor[5][2]   \n",
            "                                                                 pubmed_bert_preprocessor[6][0]   \n",
            "                                                                 pubmed_bert_preprocessor[6][1]   \n",
            "                                                                 pubmed_bert_preprocessor[6][2]   \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 460)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 64)           49216       bert_model_layer[5][13]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 64)           49216       bert_model_layer[6][13]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 32)           14752       input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "token_char_hybrid_embedding (Co (None, 128)          0           dense_12[0][0]                   \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tribid_embeddings (Concatenate) (None, 160)          0           dense_14[0][0]                   \n",
            "                                                                 token_char_hybrid_embedding[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 160)          0           tribid_embeddings[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          20608       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 5)            645         dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 109,616,678\n",
            "Trainable params: 134,437\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "UPHiC5AQYeq_",
        "outputId": "91e0232a-853f-4c94-8b14-9186871420ea"
      },
      "source": [
        "# Plotting the model structure \n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(tribid_model)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAANHCAYAAAABppglAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdeVwV9f4/8NdwWA6H5YAG4gIkYLnfMi0kTVMrzfSqqLjd1LREu2mulHa1W1oulX5zyevV/JWWbC65oeWK3pTMJXIB9y1UQBEQkOXw/v3R9VyPKPswLK/n48EfzpnzmdfMnIGX58zMUUREQERERERqiLDSOgERERFRdcayRURERKQili0iIiIiFbFsEREREanI+sEJBw4cwBdffKFFFqJyNWHCBLRt21aVsfv166fKuEQVqW3btpgwYYLWMYiqvQLvbF25cgWRkZFaZCEqN5GRkbhy5Yqq41+9elW18YnUdvDgQRw4cEDrGEQ1QoF3tu6JiIioyBxE5UpRFNWXMX78ePTv31/15RCpge/OElUcnrNFREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpiGWLiIiISEUsW0REREQqYtkiIiIiUlG5lK2tW7fCaDRi06ZN5TGcpnJzc/HJJ5/Az88Ptra2cHFxQfPmzXHx4sUSjXPw4EE0adIEVlZWUBQFderUwcyZM9UJXUpr166Fj48PFEWBoijw8PDAkCFDtI5VLVWXY6Rjx47m18uDP46OjiUai8cIEdUU1uUxiIiUxzCVQlBQEE6ePInvvvsOzzzzDJKSkhAcHIw7d+6UaBx/f3+cOnUKXbt2xfbt2xEfHw8XFxeVUpdOYGAgAgMD4efnh+TkZFy/fl3rSNVWdTpGHqVdu3Ylmp/HCBHVFOVStrp3747U1NTyGKrMsrKy0LlzZ/z8888lfm5oaCg2bNiA3377DS1atAAA1K1bFz/88EN5x9REWbYNlU11OUb0ej3S0tLg5ORkMT04OBj9+/cvr4ia4TFCRGqodudsrVixAomJiaV67ldffYVWrVqZi1Z1U5ZtQ9VHWV4H27ZtK1C0rly5guPHj6NTp07lEU9TPEaISA1lLlv79++Hl5cXFEXBokWLAABLliyBg4MDDAYDfvjhB3Tr1g3Ozs5o0KAB1qxZY37ul19+Cb1eD3d3dwQHB6Nu3brQ6/UICAhATEyMeb6xY8fC1tYWHh4e5mlvv/02HBwcoCgKkpOTAQDvvvsuJk6ciHPnzkFRFPj5+RV7PXJycnDw4EE89dRTRc67bds2ODs7Y9asWcUe/56quG3ut2/fPjRt2hRGoxF6vR4tWrTA9u3bAQAjR440n9vi6+uLo0ePAgCGDx8Og8EAo9GIjRs3AgBMJhOmT58OLy8v2Nvbo2XLlggLCwMAzJ07FwaDAU5OTkhMTMTEiRNRv359xMfHlyqz1qrLMfIos2fPxrhx4yym8RjhMUJE95EHhIWFyUMmF+rKlSsCQBYuXGieNm3aNAEgO3fulNTUVElMTJT27duLg4OD5OTkmOcbNWqUODg4yMmTJ+Xu3bty4sQJadOmjTg5Ocnly5fN8w0ePFjq1Kljsdx58+YJAElKSjJPCwwMFF9f3xLlFxG5cOGCAJCnnnpKOnbsKB4eHmJnZyeNGzeWRYsWSX5+vnnezZs3i5OTk3z00UdFjvvKK68IAElJSam028bX11eMRmOxtlNERIR8+OGHcuvWLbl586b4+/tL7dq1LZah0+nkjz/+sHjeoEGDZOPGjeZ/T5o0Sezs7CQyMlJSUlJk6tSpYmVlJYcOHbLYRuPGjZOFCxdKnz595NSpU8XKKCICQMLCwoo9f0mVdPzqcIw8zNWrV6Vp06ZiMpkspvMYqfzHSN++faVv377Fnp+ISi1c9Y8RAwIC4OzsDDc3NwwYMAAZGRm4fPmyxTzW1tZo0qQJ7Ozs0LRpUyxZsgTp6elYuXKl2vHM7p0A7+bmhlmzZuHEiRO4ceMGevXqhb///e/4/vvvzfN2794daWlp+Mc//lGmZVaVbXO/vn37YsaMGXB1dUWtWrXQs2dP3Lx5E0lJSQCA0aNHw2QyWeRLS0vDoUOH8OqrrwIA7t69iyVLlqB3794IDAyEi4sLPvjgA9jY2BRYr9mzZ+Pvf/871q5di8aNG1fcilagqvg6uGf27Nl45513YGVl+auExwiPESL6nwo9Z8vW1hbAn7dXKEzr1q1hMBgQFxdXEbEAAHZ2dgCAZs2aISAgALVq1YLRaMQ///lPGI1GLFu2TNXlV+ZtUxgbGxsAf37kAQCdOnXCE088ga+//tp8BV5oaCgGDBgAnU4HAIiPj0dmZiaaN29uHsfe3h4eHh6VZr20UpVeBwkJCdi4cSOGDRtWIcurStvmfjxGiKjSniBvZ2dn/p9gRahbty4AmM/fuMfW1hbe3t44d+5chWUpSkVvm/tt2bIFHTt2hJubG+zs7DBlyhSLxxVFQXBwMM6fP4+dO3cCAL799luMGDHCPE9GRgYA4IMPPrC4T9OlS5eQmZlZcStTxWn5OgCAOXPm4M0334Rer9csw6PwGCGiyqRSlq3c3Fzcvn0bDRo0qLBlOjo6olGjRjh58mSBx/Ly8mA0GissS2EqettER0dj/vz5AIDLly+jd+/e8PDwQExMDFJTUzFnzpwCzxk2bBj0ej2WL1+O+Ph4ODs7w9vb2/y4m5sbAGD+/PkQEYufAwcOVMh6VXVaHCP3u379Or7//nuMGTNGk+UXhscIEVU25XKfrfK2Z88eiAj8/f3N06ytrYv8+KCsgoKCMGvWLJw/fx4+Pj4AgMzMTFy6dAmvvfaaqssuroreNocPH4aDgwMA4Pfff0dubi7GjBlj3j6KohR4jqurK4KCghAaGgonJye8+eabFo97enpCr9fj2LFjqmSuCbQ6Ru6ZM2cOhgwZglq1alXI8kqCxwgRVTaV4p2t/Px8pKSkIC8vD7GxsXj33Xfh5eVlcS6In58fbt26hQ0bNiA3NxdJSUm4dOlSgbFq1aqFhIQEXLx4Eenp6SX6BTthwgR4e3tj2LBhuHz5Mm7evImQkBBkZWXhvffeM88XFRVV6svaS0qrbZObm4sbN25gz5495j8kXl5eAIAdO3bg7t27OHPmjMUl9vcbPXo0srOzsXnzZvTo0cPiMb1ej+HDh2PNmjVYsmQJ0tLSYDKZcPXqVVy7dq2km6hGqCzHCADcuHEDX3/9NcaPH//IeXiM8Bghovs8eH1iSW/9sHDhQvHw8BAAYjAYpGfPnrJ48WIxGAwCQBo1aiTnzp2TZcuWibOzswAQb29vOX36tIj8eem2jY2N1K9fX6ytrcXZ2Vl69eol586ds1jOzZs35cUXXxS9Xi8NGzaUd955RyZPniwAxM/Pz3yZ95EjR8Tb21vs7e2lXbt2cv369RJdn3nlyhUZOHCguLq6ip2dnTz77LMSFRVlMc/WrVvFyclJZs6c+chxDh48KM2aNRMrKysBIB4eHjJr1qxKtW2++uor8fX1FQCF/qxbt868rJCQEKlVq5a4uLhIv379ZNGiRQJAfH19LS61FxF5+umn5f3333/o9snOzpaQkBDx8vISa2trcXNzk8DAQDlx4oTMmTNH7O3tBYB4enrKqlWrir8D/wuV6NYP1e0YmTBhggwZMqTQeXiMVP5jhLd+IKow4eVyn62yGDVqlNSqVavClleVVPVt8+qrr8r58+c1WXZlKltlVdVfB2qq6ttGy2OEZYuowqh/n63iuHdJNBVUlbbN/R+5xMbGQq/Xo2HDhhomqj6q0uugolWlbcNjhKhmqhRlSy1xcXEWl00/6mfAgAFaR60WQkJCcObMGZw+fRrDhw/Hxx9/rHUkKgKPkYrFY4SoZtK0bE2dOhUrV65EamoqGjZsiMjIyHIdv3HjxgUum37YT2hoaLkutzyovW3UYDAY0LhxY3Tp0gUffvghmjZtqnWkKo/HyKPxGCGiqkIR+e8tjP8rPDwcQUFBeGAyUZWiKArCwsLQv3//Kjk+kdr69esHAIiIiNA4CVG1F1GtP0YkIiIi0hrLFhEREZGKWLaIiIiIVMSyRURERKQili0iIiIiFbFsEREREamIZYuIiIhIRSxbRERERCpi2SIiIiJSEcsWERERkYpYtoiIiIhUxLJFREREpCKWLSIiIiIVWT/qgXvfCE9VU35+PjIyMuDk5KR1lGpr/vz5iIiI0DpGhUtPT4eDgwOsrPh/tars4MGD8Pf31zoGUY1Q4Lelp6cn+vbtq0UWKkdxcXHYvXs3UlJStI6iib59+8LT01PV8Rs0aKDa+JVVSkoKdu/ejbi4OK2jUBn5+/ujbdu2WscgqhEUERGtQ1D5y87ORr9+/RAdHY2oqCj+UqUy279/P7p3747nn38ea9euhb29vdaRiIiqggh+DlBN2dnZITIyEi+++CJeeukl7N69W+tIVIXt3bsXr776Kjp06IB169axaBERlQDLVjVma2uL8PBwdO3aFa+99hp27NihdSSqgqKiotCtWzd0794da9euhV6v1zoSEVGVwrJVzdnY2CAsLAyBgYHo2bMnfvzxR60jURWyadMm9O7dG4GBgVi1ahVsbGy0jkREVOWwbNUAOp0OK1euRFBQEHr06IEffvhB60hUBYSGhqJPnz4YPnw4vvnmG1hbP/LiZSIiKgTLVg2h0+nw9ddfY+TIkejfvz/WrVundSSqxJYvX47Bgwdj/Pjx+Oqrr3ibByKiMuBv0BpEURQsWrQIwcHB6N+/P1avXq11JKqEvvrqK4waNQqTJ0/G3LlztY5DRFTl8XOBGkZRFCxYsAA6nQ7Dhg2DyWTC0KFDtY5FlcTcuXMREhKCjz/+GB988IHWcYiIqgWWrRpIURR88cUXcHR0xBtvvAGTyYQ33nhD61iksTlz5uD999/HggULMG7cOK3jEBFVGyxbNdhHH30EBwcHjBw5EhkZGXjnnXe0jkQaEBFMnjwZCxYswPLly1m8iYjKGctWDRcSEgJFUTBu3DiYTCa8++67WkeiCiQiGD9+PBYtWoSvv/4ar7/+utaRiIiqHZYtwpQpU6DT6TBhwgTcuXOH5+rUECaTCW+99RZWr16N8PBw9OnTR+tIRETVEssWAQAmTpwIBwcHvP322zCZTJgxY4bWkUhF987TCwsLQ0REBHr27Kl1JCKiaotli8yCg4Oh0+kQHByMrKwszJ49W+tIpIKcnBwMHDgQ27dvx+bNm9GlSxetIxERVWssW2ThzTffhIODA4YOHQqTyYR58+ZpHYnKUXZ2Nvr374+9e/fixx9/REBAgNaRiIiqPZYtKmDQoEHQ6XQYMmQIMjIysHjxYiiKonUsKqOMjAz06tULhw8fxvbt2/Hcc89pHYmIqEZg2aKHCgoKgk6nw6BBg2AymfiVLVVcamoqXn31VZw9exZ79uxBy5YttY5ERFRjsGzRI/Xt2xf29vbo27cvTCYTli1bxsJVBaWkpKBr1664cuUKdu3ahWbNmmkdiYioRmHZokJ1794d69evR58+fZCRkYFVq1bB2povm6rixo0bePnll5Gamop9+/bB19dX60hERDUO36agInXt2hVRUVHYvHkzBg8ejNzcXK0jUTFcu3YNnTt3xt27d1m0iIg0xLJFxdKhQwds3boVUVFR6NOnD7Kzs7WORIW4dOkS2rdvj/z8fOzevRuenp5aRyIiqrFYtqjY2rdvj6ioKERHR6N37964e/eu1pHoIeLj49GuXTs4OzsjOjoa9erV0zoSEVGNxrJFJfL8889j165d+OWXX/DXv/4VWVlZWkei+5w6dQqdOnWCh4cHduzYgccee0zrSERENR7LFpXYM888g59++glHjhxB165dkZ6ernUkAnDkyBG88MIL8PPzw65du1CrVi2tIxEREVi2qJSefvppREdH48yZM+jWrRvS0tK0jlSjHTp0CC+99BJat26Nbdu2wcnJSetIRET0XyxbVGpNmjTBrl27cOHCBXTq1Am3bt3SOlKNFB0djc6dOyMgIADr16+Hvb291pGIiOg+LFtUJo0bN8bu3btx48YNdOnSBcnJyVpHqlG2bduGrl27olu3bli3bh30er3WkYiI6AEsW1RmTzzxBPbv34/U1FR06dIFSUlJWkeqETZv3ozevXujT58++O6772BjY6N1JCIiegiWLSoX3t7e2L17NzIyMvDCCy8gISFB60jVWlhYGPr06YOhQ4fi22+/5V39iYgqMZYtKjdeXl7Yt28frKys8OKLL+Lq1ataR6qWvvvuOwwZMgRvvfUWvyCciKgK4G9pKlceHh7YtWsX7Ozs0L59e1y4cEHrSNXK0qVL8frrr2PixIlYtGgRFEXROhIRERWBZYvKXZ06dbBz504YjUZ07NgR586d0zpStfDZZ59hzJgxmDFjBmbPnq11HCIiKiaWLVKFm5sbdu/eDQ8PD7Rv3x4nT57UOlKVNmfOHEyZMgVffPEFpk+frnUcIiIqAZYtUo2rqyt+/PFHeHt7o1OnTjh+/LjWkaqk6dOn4/3338fChQvx7rvvah2HiIhKiGWLVGU0GrFjxw40bdoUnTt3Rmxs7EPn++OPP2rs+V2//vrrQ6eLCMaPH49PPvkEK1euxNtvv13ByYiIqDywbJHqHBwcsHnzZrRo0QIdO3bEoUOHLB6/fv06XnjhhRr58diRI0fQtm1bREZGWkzPz8/Hm2++iSVLliA0NBRDhw7VKCEREZWVIiKidQiqGTIzM9G7d2/88ssviIqKgr+/P5KSktCuXTucPXsWABAfHw8/Pz+Nk1ac7t27IyoqCjqdDuvXr8drr70Gk8mEESNGIDQ0FKGhoejVq5fWMYmIqPQi+M4WVRiDwYBNmzahQ4cOeOmll7B582Z06dIFFy5cQH5+PnQ6HT7++GOtY1aYw4cPIyoqCiICk8mE3r17Y9OmTQgKCkJERAQ2bdrEokVEVA3wnS2qcDk5OejTpw+OHDmC5ORk5Obmmh+zsrLCqVOn8MQTT2iYsGK88sor2L17t3n9raysYG1tDQ8PD3z33Xdo166dxgmJiKgc8J0tqni5ublITk4uULQAQKfT4ZNPPtEoWcX59ddf8dNPP1msf35+PvLy8pCcnAxbW1sN0xERUXniO1tUoTIzM/HKK6/g4MGDyMvLe+g8VlZWiIuLQ6NGjSo4XcXp0qULoqOjC5RN4M/CaW9vj71796JVq1YapCMionLEd7ao4mRlZaFr166IiYl5ZNEC/iwbs2bNqsBkFes///kPdu7c+dCiBQAmkwl3795Fly5dcOrUqQpOR0RE5Y1liyrM+vXrcejQIRT1Zmpubi5Wr15dbb/mZ+rUqbC2ti50HhFBSkoKxo8fj/z8/ApKRkREamDZogozaNAgXLlyBdOmTYPRaIROp3vkFylbWVlh5syZFZxQffv370d0dPQj39mzsbEBALRu3RobN25EVFQUrKx4mBIRVWU8Z4s0kZ2djbCwMMyYMQOXLl2CoigF3sHR6XQ4ffo0fHx8NEpZ/tq3b//Q89Wsra1hMpnQrVs3TJ8+Hc8995xGCYmIqJzxnC3Shp2dHV5//XWcO3cOP/zwA1q0aAEAFh+vVbd3t3bu3In9+/dbFC1ra2vo9Xq88cYbiIuLw5YtW1i0iIiqGb6zRZWCiCAqKgqffvop9u/fD1tbW+Tk5ECn0+HMmTNo2LCh1hHLrG3btvjll18gIlAUBS4uLpgwYQJGjx6NWrVqaR2PiIjUEaFZ2QoPD9disVQFnDlzBhs2bMDhw4chIujYsSNGjx6tdawyiY2NNV9h6e7ujl69euGFF14wn6NFdL/+/ftrHYGIyo92ZetRJ0YTEdV0/MCBqFqJKPz6c5WFhYXxf3BUpISEBFy5cqXKnsuUmJiIc+fOoW3btlpHoUouPDwcQUFBWscgonKmadkiKo569eqhXr16WscoNXd3d7i7u2sdg4iINMKrEYmIiIhUxLJFREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpiGWLiIiISEU1smy1adMGOp0OTz31lNZRLIwcORJOTk5QFAXHjh0r0XMr6zpR1bF27Vr4+PhAURQoigJPT0+sWLHC/PjevXtRv359KIoCDw8PLFu2rNJk9fDwwJAhQzTLQ0RUGEVERJMFKwrCwsLQv39/LRaPLl26IDk5ucSlRm2hoaEYOHAgjh49WuLiVFnXiaoWPz8/JCcn4/bt2xbTRQRvvfUWrKyssHTpUiiKolHC/3lU1qoqPDwcQUFB0OjXMhGpI6JGvrN1T2X4Y1He1FynrKwsBAQEqDY+VV75+fkYMWIEbGxsKk3RIiKqKmp02bKxsdE6QgFl/SOm5jqtWLECiYmJqo1PlVN+fj7eeOMNGAwGLFmyhEWLiKiEqkTZ+vLLL6HX6+Hu7o7g4GDUrVsXer0eAQEBiImJMc83duxY2NrawsPDwzzt7bffhoODAxRFQXJyssW4Z8+eRePGjeHg4AB7e3u0b98e+/fvNz++YMECODg4wMrKCs888wzq1KkDGxsbODg4oFWrVmjfvj08PT2h1+vh4uKCKVOmWIxvMpkwffp0eHl5wd7eHi1btkRYWJj5cRHBvHnz8OSTT8LOzg5GoxGTJ08u07Yqap2KyjV37lwYDAY4OTkhMTEREydORP369dGtWzdMnDgR586dg6Io8PPzK3am4u6/Ry07Pj6+0MzlMb6I4IsvvkCTJk1gZ2cHV1dX9OrVC3FxcQXWZ9WqVWjdujX0ej0cHBzw+OOP4+OPPy5y2wJ/nvf07LPPwmAwwNnZGS1atEBaWlqRjxUnX2Hrt23bNjg7O2PWrFnF3m/An0Vr2LBhMBqNWLRo0SPnK81rKj4+Hvv27UPTpk1hNBqh1+vRokULbN++vVjbq6QKW9bIkSPN53/5+vri6NGjAIDhw4fDYDDAaDRi48aNZVpXIqrBRCMAJCwsrNjzjxo1ShwcHOTkyZNy9+5dOXHihLRp00acnJzk8uXL5vkGDx4sderUsXjuvHnzBIAkJSWZp3Xu3Fl8fHzkwoULkpubK8ePH5fnnntO9Hq9nD592jzfjBkzBIDExMRIRkaGJCcnS9euXQWAbNmyRZKSkiQjI0PGjh0rAOTYsWPm506aNEns7OwkMjJSUlJSZOrUqWJlZSWHDh0SEZFp06aJoijy+eefS0pKimRmZsrixYsFgBw9erTE27S461ScXABk3LhxsnDhQunTp4+cOnVKAgMDxdfXt8S5RIq//x617KIyl3X86dOni62traxatUpu374tsbGx0qpVK3nsscfk+vXr5ufPnz9fAMinn34qN2/elFu3bsm//vUvGTx4cJHb9s6dO+Ls7Cxz5syRrKwsuX79uvTp00eSkpIKfUxEip3vUeu3efNmcXJyko8++qjIfeXr6ytGo1Hy8vJk8ODBYmNjI/Hx8YU+p7SvqYiICPnwww/l1q1bcvPmTfH395fatWuLiBS5Te7PWhyFLUtEJDAwUHQ6nfzxxx8Wzxs0aJBs3LixzOtaHGFhYaLhr2UiUkd4lSpbD/5SPXTokACQf/7zn+ZpJSlbf/nLXyzmi42NFQAyadIk87R7ZSs9Pd087ZtvvhEA8vvvv5un/fLLLwJAQkNDRUQkKytLDAaDDBgwwDxPZmam2NnZyZgxYyQzM1MMBoO89NJLFhnWrFlTprJV1DoVlUvkf38ssrKyLMYqa9kqzv572LKLk7ks42dmZoqjo6PF+CL/26f3CkpOTo64uLjIiy++aDFfXl6eLFiwoMicx48fFwCyefPmAtunsMeKm+9R61dSvr6+4uTkJAMHDpRWrVoJAGnWrJncuXPnofOX5TX1oE8++UQASGJiYqHb5P6sxS1bhS1LRGTHjh0CQGbOnGmeJzU1VRo1aiR5eXnlvq4Pw7JFVC2FV4mPER+ldevWMBgMD/2opzRatGgBo9GI2NjYQueztbUFAOTl5Zmn3TtXKjc3FwAQHx+PzMxMNG/e3DyPvb09PDw8EBcXh7NnzyIzMxOdO3cul+yP8uA6FZWrIhV3/5U2c3HHP3HiBO7cuYPWrVtbTG/Tpg1sbW3NH0XGxsbi9u3beOWVVyzm0+l0GDduXJE5fXx84O7ujiFDhuDDDz/ExYsXzfMV9lhx85WnzMxMdOjQAYcPH0bv3r1x4sQJjBw58qHzludr6t5xZDKZCt0m5eH+ZQFAp06d8MQTT+Drr782Xw0YGhqKAQMGQKfTAahcxw8RVR1VumwBgJ2dHZKSksptPBsbG3NhKouMjAwAwAcffGA+F0RRFFy6dAmZmZm4evUqAMDNza3MyyrK/etUVK6KVpz9V5bMxRn/3m0DHB0dCzzm4uKC9PR0ADCfK+Ti4lKqnPb29ti1axfatWuHWbNmwcfHBwMGDEBWVlahjxU3X3lydHTEqFGjAAArV66Ej48PQkNDMX/+/BKvd2G2bNmCjh07ws3NDXZ2dhbnPRa2TUqjsGUBf16cEhwcjPPnz2Pnzp0AgG+//RYjRowol3UlopqrSpet3Nxc3L59Gw0aNCiX8fLy8nDr1i14eXmVeax7JWr+/PkQEYufAwcOQK/XAwCys7PLvKzCPLhOReWqSMXdf6XNXNzx75Wnh5WW+59fr149AChwoUVJcjZr1gybNm1CQkICQkJCEBYWhs8++6zQx4qbTy1GoxERERHmghIdHW3xeGn3z+XLl9G7d294eHggJiYGqampmDNnjsU8hW2vokRHR5vLYXGWBQDDhg2DXq/H8uXLER8fD2dnZ3h7e5d5XYmoZqvSZWvPnj0QEfj7+5unWVtbl/qdqd27dyM/Px+tWrUqc7Z7Vyk+6gajzZs3h5WVFfbu3VvmZRXmwXUqKldFetj+e5jSZi7u+M2bN4ejoyN+/fVXi+kxMTHIycnBM888AwB4/PHHUatWLfz444+lypmQkICTJ08C+POP9qeffopWrVrh5MmThT5W3EQTeF0AACAASURBVHxqatWqFebPn4+8vDz0798fCQkJ5sdKu39+//135ObmYsyYMfDx8YFer7e4rURh26Q4Dh8+DAcHh2It6x5XV1cEBQVhw4YN+Oyzz/Dmm29aPF6Zjh8iqjqqVNnKz89HSkoK8vLyEBsbi3fffRdeXl4YNmyYeR4/Pz/cunULGzZsQG5uLpKSknDp0qWHjpeTk4PU1FTk5eXhyJEjGDt2LLy9vS3GKy29Xo/hw4djzZo1WLJkCdLS0mAymXD16lVcu3YNbm5uCAwMRGRkJFasWIG0tDTExsaW+StQilqnonIVplatWkhISMDFixeRnp5e4lJbnP33MMXNXJbxJ06ciHXr1mH16tVIS0vD77//jtGjR6Nu3brmj9Ps7OwwdepUREdHY+zYsfjjjz+Qn5+P9PR0nDx5ssicCQkJCA4ORlxcHHJycnD06FFcunQJ/v7+hT5W3HyFiYqKKtWtH+43evRoDBw4EDdu3EC/fv3M+7+0r6l777bu2LEDd+/exZkzZyzOPytsmxQmNzcXN27cwJ49e8xlq6hlPbie2dnZ2Lx5M3r06GHxWFmOHyKqwSrmRPyCUIqrEW1sbKR+/fpibW0tzs7O0qtXLzl37pzFfDdv3pQXX3xR9Hq9NGzYUN555x2ZPHmyABA/Pz/zbQBWrlwpL774ori7u4u1tbXUrl1bBg4cKJcuXTKPtWDBAjEYDAJAHn/8cdm3b5/Mnj1bjEajAJA6derId999J6GhoVKnTh0BIK6urrJmzRoREcnOzpaQkBDx8vISa2trcXNzk8DAQDlx4oSIiKSnp8vIkSOldu3a4ujoKO3atZPp06cLAGnQoIH89ttvJdqmxVmnonLNmTNH7O3tBYB4enrKqlWrzM87cuSIeHt7i729vbRr187ilgPlsf8KW3ZR27Ks4+fn58u8efOkUaNGYmNjI66urtK7d++H3vJg0aJF0qJFC9Hr9aLX6+Xpp5+WxYsXF5nz4sWLEhAQIK6urqLT6aRevXoybdo0ycvLK/Sx4uYrbP22bt0qTk5OFlfaPWjdunXi6+srAMyvwalTp1rMk56eLk8++aQAEHd3d1mxYkWR611YrpCQEKlVq5a4uLhIv379ZNGiRQJAfH19Zd++fY/cJg9mfdTPunXrirWs+28PIiLy9NNPy/vvv//Q7VTadS0OXo1IVC2FV5nvRgwODkZERARu3rypcjJSg9r7j68PKk/du3fHokWL0LBhwwpdLr8bkahaqlrfjXjvEm2qmtTef3x9UGnd/5F4bGws9Hp9hRctIqq+qlTZqmni4uIsLi9/1M+AAQOYjagMQkJCcObMGZw+fRrDhw83f/0SEVF5qBJla+rUqVi5ciVSU1PRsGFDREZGah2pQjRu3LjA5eUP+wkNDa3U2dTefzX19UHlx2AwoHHjxujSpQs+/PBDNG3aVOtIRFSNVJlztoiIqjues0VULVWtc7aIiIiIqhqWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpiGWLiIiISEUsW0REREQqstZy4QcOHNBy8URElQp/JxJVT4qIiCYLVhQtFktEVOlp9GuZiNQRodk7W/xlQpVF//79AQDh4eEaJyEiouqI52wRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpiGWLiIiISEUsW0REREQqYtkiIiIiUhHLFhEREZGKWLaIiIiIVMSyRURERKQili0iIiIiFbFsEREREamIZYuIiIhIRSxbRERERCpi2SIiIiJSEcsWERERkYpYtoiIiIhUxLJFREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpyFrrAEQVae/evTh48KDFtLi4OADAnDlzLKb7+/ujQ4cOFZaNiIiqJ0VEROsQRBXlp59+wssvvwwbGxtYWT38jd38/Hzk5ubixx9/xEsvvVTBCYmIqJqJYNmiGsVkMqFOnTq4efNmofO5uroiMTER1tZ885eIiMokgudsUY2i0+kwePBg2NraPnIeW1tb/O1vf2PRIiKicsGyRTXOwIEDkZOT88jHc3JyMHDgwApMRERE1Rk/RqQaydvbG5cvX37oYw0aNMDly5ehKEoFpyIiomqIHyNSzTRkyBDY2NgUmG5ra4uhQ4eyaBERUblh2aIaaciQIcjNzS0wPScnBwMGDNAgERERVVcsW1QjNWnSBE2aNCkwvXHjxmjevLkGiYiIqLpi2aIa6/XXX7f4KNHGxgZDhw7VMBEREVVHPEGeaqzLly/j8ccfx71DQFEUnD9/Ho8//ri2wYiIqDrhCfJUc3l5eaF169awsrKCoiho06YNixYREZU7li2q0V5//XVYWVlBp9Phb3/7m9ZxiIioGuLHiFSjJSUloW7dugCAP/74A3Xq1NE4ERERVTMR/D6S/woPD0dQUJDWMUhDHh4eWkcgDYSFhaF///5axyCiaoxl6wFhYWFaR6AKtnfvXiiKghdeeEHrKFTB+B8sIqoILFsP4P9wa56uXbsCAJydnTVOQhWNZYuIKgLLFtV4LFlERKQmXo1IREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpiGWLiIiISEUsW0REREQqYtkqpTZt2kCn0+Gpp57SOkqlMXLkSDg5OUFRFBw7dqxYz/nss8/g7u4ORVGwdOnSEi2vLM/V2tq1a+Hj4wNFUaAoCjw9PbFixQrz43v37kX9+vWhKAo8PDywbNmySpPVw8MDQ4YM0SwPEVFVY611gKrq0KFD6NKlC5KTk7WOUmksX74cXbp0wcCBA4v9nEmTJqFXr15o1KhRiZdXludqLTAwEIGBgfDz80NycjKuXLli8fgLL7yAV199FVZWVli6dCkURdEoacGs169f1ywLEVFVxHe2ykjNP4JZWVkICAhQbXyqnPLz8zFixAjY2NhoXrSIiKjsWLbKyMbGRrWxV6xYgcTERNXGVwOLQdnk5+fjjTfegMFgwJIlS7g9iYiqAZatMjp79iwaN24MBwcH2Nvbo3379ti/f7/FPCaTCdOnT4eXlxfs7e3RsmVLhIWFAQDmzp0Lg8EAJycnJCYmYuLEiahfvz66deuGiRMn4ty5c1AUBX5+fsXOtGDBAjg4OMDKygrPPPMM6tSpAxsbGzg4OKBVq1Zo3749PD09odfr4eLigilTplg8X0TwxRdfoEmTJrCzs4Orqyt69eqFuLi4AvPNmzcPTz75JOzs7GA0GjF58uQCeQpbfzXs27cPTZs2hdFohF6vR4sWLbB9+3YAf55Xdu/cI19fXxw9ehQAMHz4cBgMBhiNRmzcuLHI3I/ab/Hx8di2bRucnZ0xa9asEuXOz8/HsGHDYDQasWjRokfOV9pchW0X4M/zxJ599lkYDAY4OzujRYsWSEtLK9E63KP1PiAiqlSEREQkLCxMSro5OnfuLD4+PnLhwgXJzc2V48ePy3PPPSd6vV5Onz5tnm/SpEliZ2cnkZGRkpKSIlOnThUrKys5dOiQiIhMmzZNAMi4ceNk4cKF0qdPHzl16pQEBgaKr69vqdZnxowZAkBiYmIkIyNDkpOTpWvXrgJAtmzZIklJSZKRkSFjx44VAHLs2DHzc6dPny62trayatUquX37tsTGxkqrVq3ksccek+vXr5vnmzZtmiiKIp9//rmkpKRIZmamLF68WADI0aNHi73+Z86cEQDy1VdflXg9H/bciIgI+fDDD+XWrVty8+ZN8ff3l9q1a5sfDwwMFJ1OJ3/88YfFWIMGDZKNGzcWO/ej9tvmzZvFyclJPvrooyLz+/r6itFolLy8PBk8eLDY2NhIfHx8oc8pba7CtsudO3fE2dlZ5syZI1lZWXL9+nXp06ePJCUlFchaHFrvg+ICIGFhYcWen4ioFMJZtv6rtGXrL3/5i8W02NhYASCTJk0SEZGsrCwxGAwyYMAA8zyZmZliZ2cnY8aMEZH//cHIysqyGKs8ylZ6erp52jfffCMA5PfffzdP++WXXwSAhIaGmrM5Ojpa5L1/vnsFIjMzUwwGg7z00ksW861Zs8aibBVn/cu7bD3ok08+EQCSmJgoIiI7duwQADJz5kzzPKmpqdKoUSPJy8srdu5H7beS8PX1FScnJxk4cKC0atVKAEizZs3kzp07D52/PHPdv12OHz8uAGTz5s2FZi1u2SpsWSKVZx+wbBFRBQjnx4jlrEWLFjAajYiNjQUAxMfHIzMzE82bNzfPY29vDw8PjwIfy6nN1tYWAJCXl2eedu+cs9zcXADAiRMncOfOHbRu3driuW3atIGtrS1iYmIA/PnxaWZmJjp37lzoMivD+t9bR5PJBADo1KkTnnjiCXz99dcQEQBAaGgoBgwYAJ1OV+G5MzMz0aFDBxw+fBi9e/fGiRMnMHLkyIfOW5657t8uPj4+cHd3x5AhQ/Dhhx/i4sWLpV6fopYFVL59QESkJpYtFdjY2JjLS0ZGBgDggw8+MJ+noigKLl26hMzMTC1jPtTt27cBAI6OjgUec3FxQXp6OgDg6tWrAAA3N7dCx9Ni/bds2YKOHTvCzc0NdnZ2Bc5JUxQFwcHBOH/+PHbu3AkA+PbbbzFixAhNcjs6OmLUqFEAgJUrV8LHxwehoaGYP39+gXnLkquw7WJvb49du3ahXbt2mDVrFnx8fDBgwABkZWWVap2q2j4gIlITy1Y5y8vLw61bt+Dl5QXgf2Vk/vz5EBGLnwMHDmgZ9aFcXFwAwFyq7nf79m00aNAAAKDX6wEA2dnZhY5X0et/+fJl9O7dGx4eHoiJiUFqairmzJlTYL5hw4ZBr9dj+fLliI+Ph7OzM7y9vTXLfY/RaERERIS5oERHR1s8XtpcxdkuzZo1w6ZNm5CQkICQkBCEhYXhs88+K1bu6Ohoczms6vuAiKi8sWyVs927dyM/Px+tWrUCAPNVf8W9o7rWmjdvDkdHR/z6668W02NiYpCTk4NnnnnGPJ+VlRX27t1b6HgVvf6///47cnNzMWbMGPj4+ECv1z/09gmurq4ICgrChg0b8Nlnn+HNN9/UNPf9WrVqhfnz5yMvLw/9+/dHQkJCmXMVtV0SEhJw8uRJAH+WnE8//RStWrUyTyvK4cOH4eDgUKxl3VOZ9wERUXli2SqjnJwcpKamIi8vD0eOHMHYsWPh7e2NYcOGAfjzHaDhw4djzZo1WLJkCdLS0mAymXD16lVcu3at0LFr1aqFhIQEXLx4Eenp6eaPJtWk1+sxceJErFu3DqtXr0ZaWhp+//13jB49GnXr1jV/3OXm5obAwEBERkZixYoVSEtLQ2xsbIGvlSnL+pfGvXcUd+zYgbt37+LMmTPm88weNHr0aGRnZ2Pz5s3o0aNHueWOiooq1a0fHsw2cOBA3LhxA/369TPv+9LmKmq7JCQkIDg4GHFxccjJycHRo0dx6dIl+Pv7F5ozNzcXN27cwJ49e8xlqzLsAyKiSqUiT8evzEpzNeLKlSvlxRdfFHd3d7G2tpbatWvLwIED5dKlSxbzZWdnS0hIiHh5eYm1tbW4ublJYGCgnDhxQubMmSP29vYCQDw9PWXVqlXm5x05ckS8vb3F3t5e2rVrZ3HbhcIsWLBADAaDAJDHH39c9u3bJ7Nnzxaj0SgApE6dOvLdd99JaGio1KlTRwCIq6urrFmzRkRE8vPzZd68edKoUSOxsbERV1dX6d27d4FbEqSnp8vIkSOldu3a4ujoKO3atZPp06cLAGnQoIH89ttvRa7/559/bs7g4OAgffr0Kfb2f9RzQ0JCpFatWuLi4iL9+vWTRYsWCQDx9fWVy5cvW4zx9NNPy/vvv//Q8Uu737Zu3SpOTk4WV9o9aN26deLr6ysAzNtr6tSpBbbvk08+KQDE3d1dVqxYUaZchW2Xffv2SUBAgLi6uopOp5N69erJtGnTJC8vr0DWR/2sW7euWMuqiH1QXODViESkvnBF5L+XAtVw4eHhCAoKAjdHzdK9e3csWrQIDRs21DpKjaXlPlAUBWFhYejfv3+FL5uIaowIfoxINcr9H8XGxsZCr9ezaFUw7gMiqmlYtqqIuLg4i8vfH/UzYMAAraOWidrrGRISgjNnzuD06dMYPnw4Pv7443JeAyoK9wER1TTWWgeg4mncuHGN+IhT7fU0GAxo3Lgx6tevj8WLF6Np06aqLYsejvuAiGoanrP1Xzxni6jm4TlbRFQBeM4WERERkZpYtoiIiIhUxLJFREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFTEskVERESkIpYtIiIiIhWxbBERERGpyFrrAJWNoihaRyAiIqJqRBER0TpEZXD16lX8/PPPWsegQqSmpmLatGlwc3PD9OnTK3Ux3r59O1auXIkJEybg2Wef1ToOFSIgIAANGjTQOgYRVV8RLFtUJdy9exedOnXC9evXERMTAzc3N60jFemdd97BihUrsGvXLvj7+2sdh4iItBHBc7ao0hMRjBw5EvHx8YiKiqoSRQsAFixYgJdffhk9evTA2bNntY5DREQaYdmiSm/69OkIDw9HeHg4nnzySa3jFJtOp8Pq1avh6emJnj17IiUlRetIRESkAZYtqtTCwsIwa9YsLFy4EJ07d9Y6Tok5Ojpiy5YtyMjIQO/evZGdna11JCIiqmAsW1Rp7d+/H0OHDsWUKVMwatQoreOUWt26dbF161YcO3YMwcHBWschIqIKxrJFldKFCxcQGBiI7t2745NPPtE6Tpk1a9YMYWFhWL16NWbOnKl1HCIiqkC8GpEqnbS0NDz//POwtbVFdHQ0HBwctI5UbpYvX4633noL33zzDf72t79pHYeIiNQXwZuaUqWSm5uLwMBA3Lp1CzExMdWqaAEwX1U5cuRI1K9fH506ddI6EhERqYxliyqVsWPH4uDBg9i3b1+1vdHk3Llz8ccff6Bfv374+eefq9QVlkREVHI8Z4sqjXnz5mHZsmVYvXo1nnrqKa3jqEZRFKxYsQJPPvkkunXrhsTERK0jERGRili2qFLYsmUL3n//fXz++ef461//qnUc1dnb22Pjxo3Q6XR47bXXkJmZqXUkIiJSCcsWae7o0aMICgrCsGHD8O6772odp8I89thj2LRpE86ePYthw4YhPz9f60hERKQCli3SVEJCAnr27Innn38eS5cu1TpOhWvcuDE2bNiAjRs34oMPPtA6DhERqYAnyJNmMjMz0atXLzg5OSEsLAzW1jXz5fjCCy/g//2//4dBgwbB09MTo0eP1joSERGVo5r51400l5+fj0GDBuHChQs4cOAAXFxctI6kqQEDBiAuLg7jxo2Dr68vXn75Za0jERFROWHZIk1MnjwZ27dvx86dO+Hn56d1nEphxowZuHDhAvr27Yv9+/ejZcuWWkciIqJywDvIU4VbsWIF3nzzTaxatQqDBw/WOk6lkpubi27duiEuLg4HDx6stvcaIyKqQSJYtqhC7d27Fy+//DKmTZuG6dOnax2nUkpNTUW7du1ga2uLvXv3wtHRUetIRERUeixbVHHi4uIQEBCAl156CaGhoVAURetIldaFCxfQtm1bPPPMM+b7cRERUZUUwVs/UIW4efMmevTogaZNm+Lbb79l0SpCw4YNsWnTJuzZswdvv/221nGIiKgMWLZIdTk5Oejbty9MJhPWrVsHOzs7rSNVCW3atME333yDf//73/jyyy+1jkNERKXEskWqEhGMGDECR44cwcaNG+Hu7q51pCqlb9+++PTTTzF+/Hhs2LBB6zhERFQKvPUDqeqf//wnwsLCsGXLFjRv3lzrOFXSlClTcOnSJQwaNAi7d+/Gc889p3UkIiIqAZ4gT6qJiIhAUFAQFi9ezLuil5HJZELv3r3x66+/4sCBA/D29tY6EhERFQ+vRiR1HDp0CB07dsSYMWMwb948reNUC+np6Wjfvj1yc3Pxn//8p8bfdZ+IqIpg2aLyd/HiRfj7+/O2BSpISEjAc889Bz8/P2zfvh22trZaRyIiosLx1g9UvtLS0tCzZ0/Uq1cPYWFhLFrlrF69eti4cSN+/fVXfjRLRFRF8AR5KjcmkwmDBg1CcnIyYmJieOdzlTz99NMIDw9Hz5490ahRI7z33ntaRyIiokKwbFG5GTt2LHbt2oU9e/bA09NT6zjVWrdu3bB48WIEBwfD09OT3zFJRFSJsWxRuZg/fz6WLl2KtWvX4tlnn9U6To3w1ltv4dSpUxg5ciQef/xxPP/881pHIiKih+AJ8lRmUVFR6NGjB+bMmYOJEydqHadGyc/PR9++fREdHY2ff/4ZTzzxhNaRiIjIEq9GpLI5ceIEAgIC0LdvX6xYsULrODVSVlYWOnXqhKSkJBw4cABubm5aRyIiov9h2aLSu3btGp577jn4+Pjgxx9/5G0INJSUlIS2bduiXr16+Omnn/j9k0RElQdv/UCFO3/+PB7Wx7OystCrVy84ODhg/fr1LFoac3Nzw6ZNm3D8+HEMHTr0ofvMZDIhISFBg3RERDUbyxYVavjw4QgKCsLdu3fN0/Lz8zF48GCcO3cOGzduhKurq4YJ6Z4mTZpg/fr1WL9+PWbMmGHxWEZGBv7617/i/fff1ygdEVHNxasR6ZHOnz+Pffv2QVEUnD9/Hlu3boW7uzvee+89bNmyBdu3b0ejRo20jkn36dChA5YuXYo33ngD9evXx6hRo3Dt2jV069YNsbGxsLW1xf/93//xq36IiCoQyxY90sqVK2FtbY3c3FzExsaiZcuWGD16ND777DN888036Nixo9YR6SGGDx+Os2fP4p133oFOp8OMGTOQlJQEEUFeXh6+//57jBkzRuuYREQ1Bk+Qp4fKz89HgwYNcO3aNfM0a2trKIqCfv364bvvvtMwHRVFRPDqq69iz549yMvLQ15eHgBAURQ0btwYJ0+e1DghEVGNwRPk6eG2b99uUbQAmP9oh4aGYuHChRolo+JYuXIlfvrpJ+Tk5JiLFvBnCTt16hQOHz6sYToiopqFZYseavny5bCxsSkwXUSQn5+PcePG4a233rL4Q07aExHMmDEDI0aMgMlkQn5+foF5bGxssGzZMg3SERHVTPwYkQpITk5G3bp1i1WkevTogXXr1sHamqf/aS07OxtDhw5FWFhYkfPa29sjMTGRXxZORKQ+foxIBa1evbrIeRRFQcuWLfHee++xaFUStra26NmzJzw8PIrcJ9nZ2cUqZUREVHYsW1TAv//9b5hMpoc+ZmNjA2dnZ8yfPx9HjhxBQEBABaejR1EUBYMGDcK5c+cwc+ZM2NvbF3qz2a+++qoC0xER1VwsW2Th0KFDOHnyZIE7kFtbW8PKygrDhg3DuXPnMG7cOOh0Oo1SUmEMBgNCQkJw5swZ9OvXD4qiFHinKz8/H4cPH8Zvv/2mUUoiopqDZYssrFixwuLEeCsrKyiKgrZt2+LYsWNYtmwZHnvsMQ0TUnHVr18fq1evRkxMDJ5++mkoigJFUcyP29jY4Ouvv9YwIRFRzcAT5MksKysL7u7uuHPnDoA/382qXbs25s6di9dff13jdFQWIoLIyEiMHTsWycnJ5osfHB0dcePGDRgMBo0TEhFVWzxBnv4nMjISd+7cgZWVFWxtbfGPf/wDFy5cYNGqBu7djPb06dOYNGkSbGxsoNPpcOfOHaxfv17reERE1Vqlfmfriy++wIEDB7SOUWPs2bMHycnJqF+/Pv7yl79U+Xc7JkyYgLZt26oydr9+/VQZt6JkZGTgt99+Q0JCAh577DF+9VIFi4iI0DpCpcPf91RdPOT4rtzvbB04cAAHDx7UOkaNcOfOHeTm5qJDhw5o27ZtlS9akZGRuHLliqrjX716VbXx1ebg4ICAgAB06NABubm5SE9P1zpSjXD16lVERkZqHaNS4u97quoKO74r/Q2S/P39+b/ACnDt2jW4ublVm3tm3X8iuFrGjx+P/v37q74ctZlMJiQmJqJu3bpaR6n2wsPDERQUpHWMSou/76kqK+z4rh5/WanM+Ie25tLpdNz/REQqqtQfIxIRERFVdSxbRERERCpi2SIiIiJSEcsWERERkYpYtoiIiIhUxLJFREREpCKWLSIiIiIVsWwRERERqYhli4iIiEhFLFtEREREKmLZIiIiIlIRyxYRERGRili2iIiIiFRU7cvWyJEj4eTkBEVRcOzYMa3jlErHjh2hKMpDfxwdHUs01tq1a+Hj41NgHFtbW7i7u6Njx46YN28eUlJSVFobAqrH6xIAvv/+e7Rp0wZOTk7w9vbG8OHDcf369RKPw9cllcbWrVthNBqxadMmraOUWXkdSwcPHkSTJk1gZWUFRVFQp04dzJw5U4XEpffg8e7h4YEhQ4ZoHUtV1b5sLV++HP/+97+1jqGadu3alWj+wMBAnD9/Hr6+vjAajRAR5OfnIzExEeHh4WjYsCFCQkLQrFkz/PrrryqlpurwugwLC8PgwYPRr18/XL16FT/88AOio6PRrVs35OXllWgsvi6pNERE6wjlojyPJX9/f5w6dQovv/wyACA+Ph4ffPCBGrFL7cHj/fr161i9erXWsVRV7ctWdaDX65GWlgYRsfgZNWoUpkyZUubxFUWBi4sLOnbsiJUrVyI8PBw3btxA9+7dkZqaWg5rQNXRv/71L9SrVw+TJ0+G0WjEU089hQkTJuDYsWOIiYkp8/h8XVJR7r0WevTooXUUZGVlISAgoFTP+Ht3/AAAIABJREFUVftY0lpZtk11USPKlqIoWkcok23btsHJycli2pUrV3D8+HF06tSp3JfXt29fDBs2DImJiVi6dGm5j09/quqvyytXrqBu3boW6+Hp6QkAuHTpUrkvj69LqsxWrFiBxMTEUj23oo+lilaWbVNdVLuyJSKYN28ennzySdjZ2cFoNGLy5MkF5jOZTJg+fTq8vLxgb2+Pli1bIiwsDACwZMkSODg4wGAw4IcffkC3bt3g7OyMBg0aYM2aNRbj7N27F88++ywMBgOcnZ3RokULpKWlFbmMspo9ezbGjRtnMW3btm1wdnbGrFmzyjz+sGHDAABRUVHmaVV9m2mpOr4ufXx8CvwCvXeOiY+Pj3kaX5ekhv3798PLywuKomDRokUAir+/v/zyS+j1eri7uyM4OBh169aFXq9HQECAxTtJY8eOha2tLTw8PMzT3n77bTg4OEBRFCQnJwMA3n33XUycOBHnzp2Doijw8/Mr0bpUxLFUVbfNPfv27UPTpk1hNBqh1+vRokULbN++HcCf58DeO//L19cXR48eBQAMHz4cBoMBRqMRGzduBFD4sTx37lwYDAY4OTkhMTEREydORP369REfH1+qzBakEuvbt6/07du3RM+ZNm2aKIoin3/+uaSkpEhmZqYsXrxYAMjRo0fN802aNEns7OwkMjJSUlJSZOrUqWJlZSWHDh0yjwNAdu7cKampqZKYmCjt27cXBwcHycnJERGRO3fuiLOzs8yZM0eysrLk+vXr0qdPH0lKSirWMkrr6tWr0rRpUzGZTBbTN2/eLE5O/5+9O4+Lqtz/AP4ZBphhkUUlzQUV09wwF0RTuLlcI3JJRRZzSW+maSZlFl4zIzc0u2m5VK43d8DMtTS1XFNyDXHfcAnJFUFAGOD7+6OfcyW2GZzhzMDn/Xrxh2eec87neeacM1/PnHOmkkyaNKnEZdSvX19cXV2LfD01NVUASO3atfXTrGnMAEh0dLRR85hz+eVxu9y1a5fY2dnJl19+KampqZKQkCCNGzeWgICAfO24Xf5PdHS0WPhhVzGlOd5fu3ZNAMicOXP00wx5v0VEhg8fLk5OTnLq1Cl5+PChnDx5Utq0aSOVKlWSq1ev6tv1799fqlWrlm+9M2fOFAD67UNEJCgoSOrXr29st0XEPPtSQECAAJB79+7pp1na2JS0vz8uNjZWIiMj5e7du3Lnzh1p166dVKlSJd861Gq1/PHHH/nme/XVV2Xjxo36fxt6vAgPD5c5c+ZInz595PTp0wZlLGb/jrHovd7YnS8jI0McHR2la9eu+aavXr0634daZmamODo6SlhYWL55NRqNjBw5UkT+N+CZmZn6No8+HC9cuCAiIgkJCQJANm/eXCCLIesorVGjRslXX331RMswZCNXqVTi5uYmItY3ZpZUbJXn7XLChAkCQP9Xq1YtuXbtmtHLeaS8b5cstopm6mKruPdb5K+C4u/b2qFDhwSAfPLJJ/ppZVFsiZh+Xyqu2LKUsTGm2Pq7adOmCQC5efOmiIjs2LFDAMiUKVP0be7fvy8NGjSQnJwcESn98cJQxRVb5eprxAsXLiAjIwNdunQptt3Zs2eRkZGBZs2a6ac5ODigevXqOHPmTJHz2dvbAwB0Oh2Av07vPvXUUxgwYAAiIyORmJj4xOsoSVJSEjZu3Kj/OsVc0tPTISJwcXEBYN1jprTyul1++OGHWLBgAXbu3IkHDx7g0qVLaN++PZ5//nlcu3bNqGUZitsllcbf3++i+Pj4wNHRsczfVyX2pUcsfWyKYmdnB+CvrwUBoHPnzmjYsCGWLFmiv0t1zZo1CAsLg1qtBqDsvlyuiq3r168DADw8PIptl56eDgCYMGFCvmf6XLlyBRkZGQavz8HBAT///DP8/PwwdepUeHl5ISwsDJmZmSZbx9/NmDEDb7zxBrRabamXYYhz584BABo1agTAusdMaeVxu7xx4wZmzJiBYcOGoXPnznByckK9evWwcOFCJCUlYebMmQYvyxjcLsncNBoNbt26VWbrU2pfKo2yHpvHbdmyBR07doSHhwc0Gk2BO/FVKhXefPNNXLp0CTt37gQALFu2DK+//rq+jZL7crkqth4VIFlZWcW2e/ShN2vWrAKPUzhw4IBR62zatCk2bdqEpKQkREREIDo6Gp999plJ1/FIcnIyVq1ahZEjR5ZqfmNs3boVABAYGAjAesfMEpTH7fL8+fPIzc1FjRo18k13cXFB5cqVcfLkSaPyGorbJZmTTqdDSkoKatWqVWbrVGpfMlZZj82ePXswa9YsAMDVq1fRu3dvVK9eHXFxcbh//z5mzJhRYJ7BgwdDq9Vi0aJFOHv2LFxcXFCnTh3960ruy+Wq2GrWrBlsbGywe/fuYtvVrl0bWq32iZ/cnZSUhFOnTgH4602MiopCq1atcOrUKZOt43EzZszAgAEDULlyZZMtszDJycmYNWsWatWqhX/9618ArHfMLEF53C4fHXBv3LiRb3paWhru3r2rv23dlLhdkrnt2rULIoJ27drpp9na2pb4FduTUGJfKo2yHpsjR47AyckJAHDixAnodDqMHDkSXl5e0Gq1hT46x93dHaGhoVi/fj0+++wzvPHGG/leV3JfLlfFloeHB4KCgrB27VosXrwYqampiI+Px4IFC/K102q1GDJkCFavXo358+cjNTUVubm5uH79eoENvjhJSUl48803cebMGWRnZ+PYsWO4cuUK2rVrZ7J1PPLnn39iyZIlePfdd4ts8+OPPxp1W7CI4MGDB8jLy4OI4NatW4iOjkaHDh2gVquxfv16/bUx1jhmlqI8bpf16tVDp06dsHDhQuzZsweZmZm4du0ahg8fDgD5Tt1zuyRLlZeXh3v37iEnJwfx8fF455134Onpme+a2GeeeQZ3797F+vXrodPpcOvWrUKffVW5cmUkJSUhMTERaWlpBhch5tyXnoRSY6PT6fDnn39i165d+mLL09MTALBjxw48fPgQ58+fL/JhryNGjEBWVhY2b95c4GG3iu7LRl9uX4ZKc3dKWlqaDB06VKpUqSLOzs7i5+cnEydO1N/d8fvvv4uISFZWlkRERIinp6fY2tqKh4eHBAUFycmTJ2XevHni6OgoAKRBgwZy8eJFWbBggbi4uAgAqVOnjpw7d04SExOlffv24u7uLmq1WmrUqCEffvih/s6H4tZhrDFjxsiAAQOKbfPDDz9IpUqV8t2N8XcbN26U5s2bi6Ojo9jb24uNjY0A0N/h5evrK5MmTZI7d+4UmNeaxgwWdDeiSPncLm/fvi3vvPOOPPPMM6LRaMTZ2Vk6dOgg33//fb523C7/h3cjFs3Y4/2cOXOkevXqAkAcHR2lZ8+eBr/fIn/dcWdnZyc1a9YUW1tbcXFxkV69esnFixfzrefOnTvSqVMn0Wq1Uq9ePXn77bfl/fffFwDyzDPP6B+FcPToUalTp444ODiIn5+fJCcnG9wXU+5LBw8elKZNm+r3oerVq8vUqVMtamy++uorqV+/fr67Lwv7W7dunX5dERERUrlyZXFzc5Pg4GCZO3euAJD69evnexyFiEjLli3l3//+d6HjU9y+PGPGDHFwcNA/Xmb58uUGv4cixd+NqBKx3B+XCg4OBgDExsYqnISsjUqlQnR0NEJCQqxy+VQ+xcTEIDQ0tNz8pp8plfXx/s0330RsbCzu3LlTJuuzJtY+Nt26dcPcuXNRr169Ml1vMft3bLn6GpGIiMhQjx4bQAVZ09g8/rVkfHw8tFptmRdaJWGxpYAzZ87ku+20qL+wsDClo1IFwu2SyDS4L5WtiIgInD9/HufOncOQIUMwefJkpSMVYKt0gIqoUaNG/BqBLA63S6ooxo8fj6VLlyI7Oxv16tXDzJkz0bdvX5Mt35r3JXOPjTk4OjqiUaNGqFmzJubNm4cmTZooHakAntkiIqIKZdq0acjKyoKI4PLlyxZfTJQlaxybKVOmIDc3F1evXi1wB6KlYLFFREREZEYstoiIiIjMiMUWERERkRmx2CIiIiIyIxZbRERERGbEYouIiIjIjFhsEREREZkRiy0iIiIiM2KxRURERGRGLLaIiIiIzIjFFhEREZEZsdgiIiIiMiMWW0RERERmZKt0gJIcPHgQwcHBSseg/5eTk4OMjAy4uLgoHUVxs2bNQmxsrNIxzC47Oxv29vZKxygXrl+/rnQEi8bjPVmz4vZviy62nn/+eaUj0N+cPXsW58+fh6+vL2rUqKF0nCL17dsXtWvXNuvyK4rdu3ejZs2aaNKkidJRrF6tWrUq1LZjDB7vzW/jxo3w8fGx6GO3NStu/1aJiJRxHrJiOTk5ePfddzFv3jx88MEHiIqKgkqlUjoWmcm6devQt29fHD9+HM2bN1c6DhE9AZVKhejoaISEhCgdpaKJtegzW2R5bG1tMWfOHHh7e2PUqFG4fPkyli5dCkdHR6WjkRlERUWhT58+LLSIiJ4AL5CnUhk2bBh27tyJXbt2oUOHDrhy5YrSkcjENm7ciCNHjuDf//630lGIiKwaiy0qNX9/fxw4cAA5OTnw8fHB7t27lY5EJjRlyhS88soraN26tdJRiIisGosteiJeXl44cOAA/Pz8EBAQgCVLligdiUxgy5YtOHToEMaPH690FCIiq8dii56Ys7Mz1q1bh3HjxmHo0KEYPnw4dDqd0rHoCURFRaF79+5o06aN0lGIiKweL5Ank1CpVIiMjESTJk0wZMgQJCYmYs2aNXB3d1c6Ghlp27Zt2L9/P+Li4pSOQkRULvDMFplUSEgI9u/fj7Nnz8LX1xenTp1SOhIZaerUqQgMDISvr6/SUYiIygUWW2RyLVq0wKFDh1CjRg20a9cOGzduVDoSGWjHjh3Yu3cvr9UiIjIhFltkFh4eHvjpp5/Qt29f9OrVC5GRkUpHIgNMnjwZXbt2hZ+fn9JRiIjKDV6zRWaj0WiwZMkStGvXDm+99RbOnj2LJUuWwMHBQeloVIhdu3Zhz5492LNnj9JRiIjKFZ7ZIrMbNmwYtmzZgm3btqFDhw64du2a0pGoEJMmTULnzp3h7++vdBQionKFxRaViRdffBG//fYbsrKy0K5dO97pZmF+/fVX/PLLL/joo4+UjkJEVO6w2KIy88wzz+DgwYPw8fHBCy+8gG+//VbpSPT/IiMj0aFDB3Ts2FHpKERE5Q6LLSpTlSpVwrp16/DOO+9g8ODBCA8PR25urtKxKrSDBw9i+/bt+OSTT5SOQkRULrHYojKnVqsxffp0rFq1CgsXLkT37t2RkpKidKwK65NPPsHzzz+PLl26KB2FiKhcYrFFiunXrx/27duHkydPwtfXF2fOnFE6UoVz5MgRbNu2DR9//LHSUYiIyi0WW6SoVq1a4eDBg3B3d0f79u2xfft2pSNVKJGRkfD19UVAQIDSUYiIyi0WW6S4GjVqYM+ePejRowcCAwMxY8YMpSNVCMeOHcOWLVswceJEpaMQEZVrfKgpWQSNRoNvv/0WrVq1wpgxY3DixAksWrQIWq1W6Wjl1ieffIKWLVsiMDBQ6ShEROUaz2yRRQkPD8fmzZuxZcsWdOnSBcnJyUpHKpcSEhKwadMmfPzxx1CpVErHISIq11hskcUJDAxEXFwc7t69Cx8fHxw6dEjpSOVOZGQkmjdvjh49eigdhYio3GOxRRapYcOG2L9/Pxo3box//OMfWLFihdKRyo1Tp07h+++/x8SJE3lWi4ioDLDYIotVuXJlbN26FeHh4Rg0aBDGjRuHvLw8pWNZvU8++QSNGzfGK6+8onQUIqIKgRfIk0V79ABUb29vDB06FCdOnMCqVavg6uqqdDSrdPr0aaxduxZr1qyBjQ3/r0VEVBZ4tCWr0L9/f+zcuRNHjx6Fv78/Ll++rHQkqzRlyhQ0atQIQUFBSkchIqowWGyR1Wjfvj0OHz4MjUaDNm3a4Oeff1Y6klW5cOECYmJi8NFHH/GsFhFRGeIRl6xKzZo1sXfvXgQGBiIgIIAPQDXC5MmTUa9ePQQHBysdhYioQuE1W2R1tFotli1bhmbNmmH8+PG4dOkS5syZA3t7e6WjWayLFy9i1apV+O9//wu1Wq10HCKiCoVntsgqqVQqREREYOPGjVizZg26dOmCmzdvKh3LYk2dOhV169ZFaGio0lGIiCocFltk1bp164Z9+/bhjz/+gI+PD44ePap0JItz5coVrFy5EhMmTICtLU9mExGVNRZbZPW8vb1x6NAhNGjQAC+88ALWrVundCSLMnXqVNSqVQv9+/dXOgoRUYXEYovKhSpVqmDbtm3417/+hb59+/IBqP/v6tWr+Pbbb/Hhhx/yrBYRkUJYbFG5YWtriy+++AJff/01Pv/8c4SGhiI9PV3pWGUiLy8PI0eOLPD8saioKFSvXh0DBgxQKBkREalERJQOQWRq+/btQ1BQEKpXr44NGzagbt26Skcyq6SkJNSsWRNqtRoDBw7EhAkToNFo8Mwzz+DLL7/EsGHDlI5IRGVo4MCBOH78eL5piYmJ8PDwgJOTk36anZ0dNm3ahJo1a5Z1xIoklsUWlVvXrl1Dr169cPXqVcTGxqJjx44F2pw7dw7z58/H7Nmzyz6gCR04cADt27cH8NfBMzc3Fz4+Prh+/TouX77Mx2IQVTBTpkzBRx99VGK7Ro0a4fTp02WQqEKL5deIVG7Vrl0bu3fvhr+/PwICArB48eJ8r6ekpCAwMBBffvkl9uzZo1BK00hMTNQ/FV6n0yEvLw/Hjh3DjRs30K9fPx5MiSqYfv36QaVSFdvGzs4OgwcPLptAFRyLLSrXnJ2d8d1332HSpEkYNmwYhg8fDp1Oh9zcXISGhuLatWtQqVQYNmwYdDqd0nFL7erVq7Czs8s3TafTQUSwadMmNG3aFCEhITh58qRCCYmoLNWvXx8tW7Ys9qe5cnJy+Oy9MsLbk6jce/QA1Hr16mHIkCG4dOkSGjVqhJ07dyI3NxfAX78bOHv2bLz//vsKpy2dxMTEIu++fFRExsbGIiMjAxs3buRvIxJVAIMGDUJ8fHyhxwaVSgVfX99yfz2rpeARlyqMkJAQ7Nu3Dw8fPsTcuXP1hRYA5Obm4qOPPkJiYqJyAZ/ApUuXij0zp1ar8eKLL2Lt2rUstIgqiNDQ0CL/E2ZjY4NBgwaVcaKKi0ddqlDy8vLw22+/Ffnau+++W8aJTOPixYtFvmZra4uuXbti48aN0Gq1ZZiKiJRUvXp1+Pv7F/l7qEFBQWWcqOJisUUVxo0bN/Dyyy/nO6P1OJ1Oh/Xr12PLli1lnOzJ/fHHH4VOV6vV6NGjBzZu3AiNRlPGqYhIaQMHDiwwzcbGBp06dUK1atUUSFQxsdiiCiEzMxPdunXDvXv3iiy2gL8OQiNHjsTDhw/LMN2TuX37dqF51Wo1evXqhZiYmAIXzxNRxRAcHFzopQOFFWFkPiy2qEKYN29egQf8FSYvLw9JSUmIiooqg1SmceXKlQLTbGxsEBQUhDVr1vBneogqMBcXF7z00kv5jgNqtRqvvPKKgqkqHhZbVCGMHTsWiYmJmDx5MurUqQMART7oMycnB1FRUTh79mxZRiy1xMTEfM/TUavVCAkJwapVq1hoEREGDBigP6Nva2uLnj17wtXVVeFUFQuLLaowPD09ERERgcTERBw+fBjDhw+Hq6srVCpVoReQjhgxQoGUxrty5Yr+a8JHdxitXLmyyItiiahi6dmzJxwcHAD8ded1//79FU5U8bDYogqpdevW+PLLL5GcnIyYmBgEBgZCrVbD1tYWNjY20Ol0+OWXXxAbG6t01BJdvXoVOp0ONjY2eOONN7B48WI+3oGI9LRaLfr06QMAcHR0RGBgoMKJKh6r/Y4hJiZG6QhUjgwcOBC9evXC/v37sWvXLly+fBkA8MYbbyAjI0P/v0JLtG/fPogIAgIC0KlTJ6soEKliCwkJMctyr1+/jl9//dUsy7Z2tWvXBgC0adMGGzduVDiNZapduzaef/55syzban+IuqTffCIiIstkro+dmJgY/vwMlVrfvn3N9Z/VWKs9swUA0dHRZvsfEhHw192Jv/76K9q1a2exF5tPnz4d48aNUzoGUYnKqhiy0nMIZhcZGYkJEyZY7LFMScHBwWZdPkecqBg2Njbw8/NTOkaxWGgRkSFYaCmHV9ESERFVACy0lMNii4iIiMiMWGwRERERmRGLLSIiIiIzYrFFREREZEYstoiIiIjMiMUWERERkRmx2CIiIiIyIxZbRERERGbEYouIiIjIjFhsEREREZkRiy0iIiIiM2KxRURERGRGLLZKEBUVBVdXV6hUKhw/flzpOAYbMmQItFotVCoVHj58WG5ytGnTBmq1Gi1atCj1Mn744Qe4urpi06ZNRbYZOnQoKlWqZHHvuyn6XxRD+1xUO0PGtaydPXsWb7/9Npo2bYpKlSrB1tYWrq6uaNiwIbp164YDBw4oHZEsjKXu+8bQ6XSYOHEivLy8YG9vj5o1a2Ls2LHIzMw0elnfffcdvLy8oFKp8v3Z29vjqaeeQseOHTFz5kzcu3fPDD0pP1hsleDf//43vvnmG6VjGG3p0qUYO3as0jFMnuPQoUPo1KnTEy1DREpss2jRIixcuPCJ1mMOpuh/UQztc1HtDBnXsrR48WJ4e3sjPj4en3/+Oa5du4b09HQcO3YMkydPRkpKCk6cOKF0TLIwlrrvG+Odd97BzJkzMW3aNNy5cwcrV67EwoULMXToUKOXFRQUhEuXLqF+/fpwdXWFiCAvLw83b95ETEwM6tWrh4iICDRt2hSHDx82Q2/KhwpTbGVmZqJ9+/ZKxyATUalUpZ63W7duuH//Pnr06GHCRGXrSfpvLpY0rgcPHsTw4cPh7++PnTt3IiAgAG5ubtBoNPDy8kJoaCgmTpyI7OxspaMWScljFo+X1uvSpUv4+uuvMWjQIISFhaFSpUro2LEjRo8ejVWrVuH06dNPvA6VSgU3Nzd07NgRS5cuRUxMDP7880/9MYAKqjDF1uLFi3Hz5k2lYyjCUj6YTZnDzs7OZMsqiqWMW2HM1X9D+1wWYyMiiI2NxYIFC4yed8qUKcjNzUVUVBRsbW0LbRMQEIBRo0Y9aUyzUfKYVZGPl4Bl7/slOXToEPLy8tC2bdt801966SUAwLZt20y+zr59+2Lw4MG4efMmvv76a5MvvzyoEMXWO++8g/feew8XL16ESqXCM888A+Cvg/nnn3+Oxo0bQ6PRwN3dHb169cKZM2eKXd6ff/6JunXrwtbWVr8BA0Bubi4mTpwIT09PODg4oHnz5oiOjgYAzJ8/H05OTnB0dMSGDRsQGBgIFxcX1KpVC6tXry5135YvXw4fHx9otVo4OTmhbt26mDx5sv51GxsbbNmyBYGBgXB1dcXTTz+NJUuW5FvG3r170aRJE7i6ukKr1cLb21u/Q3766adwdHREpUqVcPPmTbz33nuoWbMmzp49a1TOknIMHTpUfy1A/fr1cezYMQB/XfPl6OgIV1dXbNy4Ud/+woULaNSoEZycnODg4AB/f3/s27dP/3pRuRcvXgxPT0+oVCrMnTtX315EMHPmTDz77LPQaDRwdXXF+++/b1QfH1fctjB79mw4OTnBxsYGrVu3RrVq1WBnZwcnJye0atUK/v7+qF27NrRaLdzc3PDBBx8UWH5J/S8pgzF9NqTdvn37CoyrMdt8bm4upk2bhmeffRYODg6oWrUq6tWrh2nTpiEkJETfbuvWrXBxccHUqVOLHPvs7Gzs3LkTVapUga+vb5HtCutnSccDY/fj4vbP4va7oo5ZpjrGmHrd1szQ/cBUY7979274+vrC0dERLi4u8Pb2RmpqaonrMJSNzV8f6w4ODvmmN2jQAADyndkyZH8y1ODBgwEAP/74o36atYxZmRArBUCio6MNbh8UFCT169fPN23ixIlib28vy5cvl5SUFImPj5dWrVpJ1apVJTk5Wd9u9erVAkCOHTsmIiLZ2dkSFBQkGzZsyLe8sWPHikajkbVr18q9e/dk/PjxYmNjI4cOHRIRkQ8//FAAyM6dO+X+/fty8+ZN8ff3FycnJ8nOzjZ6DGbNmiUAJCoqSu7cuSN3796Vb775Rvr3719gfSkpKXL37l15+eWXRaPRSHp6un45sbGxEhkZKXfv3pU7d+5Iu3btpEqVKvrXHy0nPDxc5syZI3369JHTp08bnNPQHEFBQaJWq+WPP/7IN/+rr74qGzdu1P+7S5cu4uXlJZcvXxadTicJCQnStm1b0Wq1cu7cuRJzX7t2TQDInDlz8rVVqVTyn//8R+7duycZGRkyb968fO+7MUraFj7++GMBIHFxcZKeni63b9+Wl156SQDIli1b5NatW5Keni6jR48WAHL8+HGj+2/I9mhInw1tV9S4GrLNT506VdRqtWzYsEEyMjLkyJEjUq1aNenYsWO+cd28ebNUqlRJJk2aVOTYnzt3TgBIu3btjHrPDD0eGNqnkvbPkva7wo5ZpjrGmGPdhoiOjhZzfuyUZvmGbt+mGPsHDx6Ii4uLzJgxQzIzMyU5OVn69Okjt27dMmgdhoiPjxcA8tFHH+WbnpOTIwCkd+/e+mmG7E+P1K9fX1xdXYt8PTU1VQBI7dq1rW7MRET69u0rffv2NWoeI8RU2GIrIyNDnJ2dJSwsLF+73377TQDk2/geL7Z0Op3069dPfvzxx3zzZWZmiqOjY77lZWRkiEajkZEjR4rI/zaszMxMfZtHO/WFCxcM7ovIXwWfm5ubdOrUKd/0nJwcmT17dpHrW7ZsmQCQhISEIpc9bdo0ASA3b94scjnGMDTHjh07BIBMmTJFP+3+/fvSoEEDycnJ0U/r0qWLPPfcc/nW8egAM3bs2GLXK1KwKMjIyBBHR0fp2rVrvnZ/L7INZci28KjYSktL07f59ttvBYCcOHFCP+3R9rhmzRqj+l9SBkP7bMzYFFdslbRW7CozAAAgAElEQVTNt2nTRnx9ffOtY9iwYWJjYyNZWVlijMOHDwsA+ec//2nwPMYcDwzpkyH759/9fb/7+zHLnMcYU6zbEJZWbBm6fZtq7BMSEgSAbN68uUAWU42xiMhLL70klStXlp07d0pmZqbcuHFDYmJiRKVSSffu3Y1a1iMlFVsiIiqVStzc3ETE+sbM3MVWhfgasTAnT57EgwcP4OPjk296mzZtYG9vj7i4uALz5Obm4tVXX8VTTz2V7+tD4K9bzDMyMtCsWTP9NAcHB1SvXr3YryXt7e0B/HWrrjHi4+ORkpKCgICAfNPVajXCw8OLnO/RtT7Fre9Rm9zcXKMyGaOwHJ07d0bDhg2xZMkS/Z1ta9asQVhYGNRqdbHL8/b2hqurK+Lj443OcuHCBWRkZKBLly5Gz1uYJ90WcnJy9NMMeb+Agv0vKYOhfTb12ACFb/MPHz4scDdjbm4u7OzsSnzv/87Z2RkAkJGRYfA8pTkePO7vfSrN/lnSfmfOY4y51m3pDN2+TTX2Xl5eeOqppzBgwABERkYiMTHxiddRmDVr1iA4OBiDBg1C5cqV0aFDB3z//fcQEVSpUsWoZRkqPT0dIgIXFxcA1jdm5lZhi62UlBQA/zswP87NzQ1paWkFpo8aNQrnz5/H119/jVOnTuV7LT09HQAwYcKEfM8iuXLlilEHfUM9+r7azc3tiZe1ZcsWdOzYER4eHtBoNIVeI1QWVCoV3nzzTVy6dAk7d+4EACxbtgyvv/66QfPb2dkZXbQCwPXr1wEAHh4eRs9bmLLeFh55vP8lZTC0z6Yem6K8/PLLOHLkCDZs2IDMzEwcPnwY69evR/fu3Y0uturWrQutVotz584ZPE9pjgfFMWT/NHa/M+V2peS6LYmh27ep+u/g4ICff/4Zfn5+mDp1Kry8vBAWFobMzEyTjrGrqyu+/vprXL9+HRkZGbh48SL+85//AABq1Khh1LIM9Wh/a9SoEQDrGzNzq7DF1qODYGEH0ZSUFNSqVavA9JCQEGzfvh1ubm4YNGhQvjMQj3bWWbNmQUTy/ZnjwYmPdpjbt28/0XKuXr2K3r17o3r16oiLi8P9+/cxY8YMU0QslcGDB0Or1WLRokU4e/YsXFxcUKdOnRLny8nJwd27d+Hp6Wn0OrVaLQAgKyvL6HkLU9bbAlCw/yVlMLTPph6bokRGRqJz584YPHgwXFxc0KdPH4SEhJTqeUcajQYBAQG4ffs29u/fX2S7u3fv6p87VJrjQXFK2j9Ls9+ZartSct2WxtDt25T9b9q0KTZt2oSkpCREREQgOjoan332mdnH+NChQwBgtuf0bd26FQAQGBgIoHyMmSlV2GKrWbNmcHZ2LvAQtri4OGRnZ6N169YF5unUqROqVq2KBQsW4MiRI5gyZYr+tUd3j5XVE4fr1q2LypUr46effnqi5Zw4cQI6nQ4jR46El5eX/mnvSnF3d0doaCjWr1+Pzz77DG+88YZB8/3yyy/Iy8tDq1atjF5ns2bNYGNjg927dxs9b2HKelsACva/pAyG9tnUY1OUkydP4uLFi7h16xZ0Oh2uXr2K+fPnw93dvVTLi4yMhEajwZgxY4p8anZCQoL+sRClOR4Up6T9szT7nam2KyXXbWkM3b5N1f+kpCT9tyIeHh6IiopCq1atcOrUKbOP8cKFC1GvXj288MILJl92cnIyZs2ahVq1auFf//oXgPIxZqZUYYqtypUrIykpCYmJiUhLS4NarcZ7772HdevWYcWKFUhNTcWJEycwYsQIPP300xg+fHiRy+rZsycGDx6MqVOn4siRIwD++h/SkCFDsHr1asyfPx+pqanIzc3F9evXcePGDZP3R6PRYPz48dizZw9Gjx6NP/74A3l5eUhLSyvwFWdxHp0J2bFjBx4+fIjz58+XeH2KuY0YMQJZWVnYvHlzkQ/IzM7Oxv3795GTk4OjR49i9OjRqFOnjv72Y2N4eHggKCgIa9euxeLFi5Gamor4+PhSPd8JKJttoaT+l5TB0D6bemyKMmrUKHh6euLBgwfFtvvxxx8NulW9RYsWWLlyJRISEuDv748ffvgB9+/fh06nw+XLl7Fw4UK8/vrr+muVtFptqY8HhSlp/zRkvyvsmGWK7UrJdVsaQ7dvU+3TSUlJePPNN3HmzBlkZ2fj2LFjuHLlCtq1a2fS44avry+uXLmCnJwcJCYmYuzYsdixYwcWL16svyYKMHx/ekRE8ODBA+Tl5UFEcOvWLURHR6NDhw5Qq9VYv369/potaxszszPXpffmBiPvRjx69KjUqVNHHBwcxM/PT5KTkyUvL09mzpwpDRo0EDs7O3F3d5fevXvL2bNn9fN999134u7uLgCkbt26cvPmTUlNTZXatWsLAHF2dpZly5aJiEhWVpZERESIp6en2NraioeHhwQFBcnJkydl3rx54ujoKACkQYMGcvHiRVmwYIG4uLgIAKlTp06+2/YNNXfuXPH29hatVitarVZatmwp8+bNkxkzZoiDg0O+9a1YsULfl1q1aunvBIyIiJDKlSuLm5ubBAcHy9y5cwWA1K9fX0aNGqVfTu3atWX58uVG5TMmx+Natmwp//73vwtd5tKlS6VTp07y1FNPia2trVSpUkX69esnV65cKXS9j+eeM2eOVK9eXQCIo6Oj9OzZU0RE0tLSZOjQoVKlShVxdnYWPz8/mThxoj7j77//blS/i9sWZs+erd8W6tatK3v37pXp06eLq6urAJBq1arJypUrZc2aNVKtWjUBIO7u7rJ69WqD+19SBmP6bEi7wsbVmG3+559/lipVqggA/Z+dnZ00btxYvvvuO32ffvjhB6lUqVK+O1aLc/XqVRk7dqx4e3uLs7OzqNVqcXNzk5YtW8rrr78u+/fv17c15Hhg7H5c1P4pUvx+d/Xq1UKPWaY6xph63YaytLsRRQzfD0wx9omJidK+fXtxd3cXtVotNWrUkA8//FB/t7UpxlhEpGvXruLm5ia2trbi7u4u3bp1K/RRCIbsTxs3bpTmzZuLo6Oj2Nvbi42NjQDQ33no6+srkyZNkjt37hSY15rGzNx3I6pELOwHzQykUqkQHR2d74GHVH5069YNc+fORb169ZSOQmVg/vz5OH/+PGbNmqWflp2djXHjxmH+/Pm4d+9egYc0kvWJiYlBaGio2X5H09zLp/IrODgYABAbG2uOxccW/jsWRGVMp9Ppv9KJj4+HVqtloVVBJCcnY/To0QWuu7C3t4enpyd0Oh10Oh2LLSKyWhXmmi1rcObMmXy3rxb1FxYWVu5yRkRE4Pz58zh37hyGDBmS7yeHLIG1vDfWyMHBAXZ2dli8eDH+/PNP6HQ6JCUlYdGiRZg4cSLCwsL014EQVUQ8/lg/ntmyII0aNbKK09/myOno6IhGjRqhZs2amDdvHpo0aWLS5T8pa3lvrJGrqyt++uknTJo0CQ0bNkR6ejqcnZ3RtGlTTJ8+HcOGDVM6IpGiePyxfiy2yCJMmTIl36M0qGLx9/fH9u3blY5BRGQW/BqRiIiIyIxYbBERERGZEYstIiIiIjNisUVERERkRiy2iIiIiMyIxRYRERGRGbHYIiIiIjIjFltEREREZsRii4iIiMiMWGwRERERmRGLLSIiIiIzYrFFREREZEYstoiIiIjMyFbpAE/iwIEDSkcgKtdycnJga2vVhwmyIGV1zI6JiSmT9RiC+5B1uH79OmrVqmW25atERMy2dDNSqVRKRyAiolIw18dOTEwMQkNDzbJsKv/69u2L2NhYcyw61mqLLSIyv9jYWAwfPhxPP/00VqxYgZYtWyodicjipaam4v3338eCBQswcOBAzJ8/H87OzkrHIuXE8potIipScHAwjh8/Dg8PD7Rt2xaRkZHIzc1VOhaRxfr111/RqlUrrF+/Hhs2bMCyZctYaBEvkCei4nl6euKXX37BzJkzERUVhX/84x+4dOmS0rGILEpWVhbGjRsHf39/PPfcczh58iR69uypdCyyECy2iKhEKpUK4eHhOHz4MB48eICWLVtiwYIFSscisggJCQlo164dvvrqK3z11Vf47rvvULVqVaVjkQVhsUVEBvP29kZcXBxGjBiBESNGICQkBHfv3lU6FpEiRARffPEFfHx84ODggCNHjmDYsGFKxyILxAvkiahUtm/fjiFDhsDGxgbffvstOnXqpHQkojJz5coVvPbaa/j1118xfvx4fPTRR1Cr1UrHIsvEC+SJqHS6du2KhIQE+Pn5oUuXLggPD0dWVpbSsYjMLjY2Fi1btsTt27cRFxeHyMhIFlpULBZbRFRqbm5uWLVqFf773/9i6dKlaN26NY4fP650LCKzuHXrFvr06YPQ0FAMHDgQR44c4eNQyCAstojoiQ0aNAjx8fGoXLkynn/+ecyYMQN5eXlKxyIymW3btqFFixY4cuQIdu7ciS+++AIajUbpWGQlWGwRkUnUrVsXv/zyCyIjIzFx4kS8+OKLuH79utKxiJ5IZmYmwsPDERgYiA4dOuD48eO8PpGMxmKLiExGrVYjIiIC+/btw/Xr19GsWTOsXLlS6VhEpfLbb7+hRYsWWLZsGZYvX46YmBi4u7srHYusEIstIjK5Nm3a4NixY3jttdcwcOBAhISE4N69e0rHIjJITk4OZsyYAT8/P9StWxcJCQno37+/0rHIivHRD0RkVtu2bcOQIUNgb2+Pb7/9Fi+88ILSkYiKdObMGQwcOBAnT55EVFQURo8eDZVKpXQssm589AMRmVdAQAB+//13PPfcc+jcuTPCw8ORnZ2tdCyifEQECxYsgI+PD1QqFY4dO4bw8HAWWmQSLLaIyOw8PDywYcMGLF26FEuWLIGPjw/i4+OVjkUEAEhOTkaPHj3w1ltvYdSoUdi/fz+effZZpWNROcJii4jKzKNHRLi4uKBt27Z8RAQpbu3atWjWrBlOnz6NXbt2Yfr06bCzs1M6FpUzLLaIqEzVq1cPu3btQmRkJD766CO89NJLSEpKUjoWVTD379/H8OHDERISgqCgIMTHx6NDhw5Kx6JyihfIE5Fi4uLiMHDgQNy7dw8LFy5Er169lI5EFcDOnTsxZMgQZGdnY9GiRejevbvSkah84wXyRKSctm3b4vjx43j11VfRu3dvDBo0CGlpaUrHonLq4cOHGDduHF588UX4+voiISGBhRaVCZ7ZIiKL8P3332PYsGFwdnbGsmXL4O/vr3QkKkcSEhIwYMAAXL58GTNnzsSwYcOUjkQVB89sEZFl6N27NxISEtCsWTN07twZ48aN4yMi6Inl5eXhiy++QOvWreHk5ISjR4+y0KIyxzNbRGRRRAQLFy7EmDFj0KRJE6xYsQINGzZUOhZZocTERLz22muIi4vDJ598gvfffx82NjzHQGWOZ7aIyLKoVCoMGzYMhw8fRm5uLlq0aIEvvvgC/H8hGWPZsmVo3rw57ty5g4MHDyIiIoKFFimGWx4RWaRGjRohLi4OH3zwAd577z0EBgbixo0bSsciC3fr1i307t0bgwcPxpAhQ3DkyBG0aNFC6VhUwbHYIiKLZWtri8jISOzduxcXLlxAixYtsHHjRqVjkYXaunUrnnvuORw7dgw///wzvvjiC2g0GqVjEbHYIiLL9/zzz+Po0aPo1asXXnnlFQwaNAgPHjxQOhZZiIyMDISHh+Pll1+Gn58fjh07ho4dOyodi0iPF8gTkVX57rvvMHz4cLi4uGD58uV86ncFd/DgQQwaNAgpKSn45ptv0Lt3b6UjEf0dL5AnIusSFBSEhIQENG7cGB07dsS4ceOg0+mUjkVlLCcnB5GRkfDz84OXlxeOHz/OQossFs9sEZFVevSIiHfffRfe3t5Yvnw5GjRooHQsKgOnT5/GwIEDcerUKURFRWH06NFQqVRKxyIqCs9sEZF1evSIiEOHDiE7OxutW7fGggULlI5FZiQiWLBgAXx8fKBWq3H8+HGEh4ez0CKLx2KLiKxakyZNEBcXhzFjxmDEiBEICgrC7du3lY5FJpacnIzu3bvjrbfewttvv419+/bxYbdkNVhsEZHVs7OzQ2RkJLZv345Dhw6hWbNm2Lx5s9KxyERiY2PRtGlTXLp0CQcOHMD06dNhZ2endCwig7HYIqJyo3Pnzjhx4gRefPFF9OzZE8OHD0d6errSsaiU7t+/j0GDBiE0NBR9+/bF4cOH4ePjo3QsIqPxAnkiKpdiY2Px5ptvolq1alixYgVatWqldCQywo4dOzBkyBDk5ORg0aJF6Natm9KRiEqLF8gTUfkUHByMY8eOoVq1amjbti0iIyORm5urdCwqwcOHDzFu3DgEBASgXbt2SEhIYKFFVo9ntoioXBMRfPnll4iIiECrVq2wfPly1K9fv8R5eIdb2Ttx4gQGDBiAK1eu4NNPP8WwYcOUjkRkCjyzRUTlm0qlQnh4OA4fPoyMjAy0atWq2EdErF27FtOmTSvDhOVfXl4eQkJCcO3atUJfz83NxYwZM+Dj44OqVavixIkTLLSoXGGxRUQVQrNmzXDw4EGMGDECI0aMQHBwMO7cuZOvzbVr1zBkyBBMnDgRBw8eVChp+fPZZ58hNjYWgwcPxt+/TElMTESnTp0QGRmJSZMmYfv27ahdu7ZCSYnMg8UWEVUYWq0W06dPx7Zt23DgwAE0a9YMP/74I4C/zr70798fWVlZUKlUCAkJQWpqqsKJrd+hQ4cwfvx4AMCuXbswd+5c/WvLli2Dt7c37t27h4MHDyIiIgI2NvxYovKH12wRUYWUkpKCt956C6tXr8Ybb7yBOnXqYMKECfozL3Z2dggNDcXy5csVTmq9Hjx4gObNm+PatWvIyckB8Ne47tixA59//jk2bdqEUaNGYebMmbC3t1c4LZHZxLLYIqIK7b///S/efvttZGZmFnq34ooVK9C/f38Fklm//v37IzY2Nt8Phdva2kKr1cLDwwPLly9Hhw4dFExIVCZ4gTwRVWz9+vVD9erVC737UKVSYfjw4bh8+bICyaxbTEwMVq1ala/QAoCcnBxkZmYiODiYhRZVGCy2iKhCi4iIQGJiov5rrseJCLKzs9G3b98CRQMV7dKlSxgyZEiRj8/Izc3FzJkzsXfv3jJORqQMFltEVGHt2LEDX375ZaGF1iM6nQ6///47pk6dWobJrFdOTg5CQkKQnZ1d4M7Dx9nY2KB///5IS0srw3REymCxRUQV0t27d/Haa68Z1DY3NxeTJ0/Gvn37zJzK+k2YMAHHjx8vtoAF/hrTa9euYezYsWWUjEg5LLaIqEKytbXF559/jkGDBqFq1aoAAHt7+yIfPaBSqRAaGoqUlJSyjGlVduzYgU8//bTIn0VSqVRQq9UAgJo1a2LEiBF4+eWXiz0DRlQe8G5EIiL8dZ3Rpk2b8P3332P//v3Izc2FnZ0dsrOz9W1sbW3Ro0cPrFu3TsGklunWrVto1qwZ7ty5k6/Y0mg0yMrKgkajQfv27REQEIB//vOfaN26tYJpicoUH/1ARPR3qamp2LlzJ7Zu3YotW7bgjz/+gJ2dHXJyciAiWLp0KQYPHqx0TIshIujWrRt+/PFH/ZlBEUGTJk3Qo0cPvPjii+jQoQOfpUUVFYstIksXHByMtWvXKh2DiKwMP94tRqyt0gmIqGTt2rXDu+++q3QMApCVlYVTp04hLS0N//jHP5SOo7iHDx9i586daNKkCerWrVvk4x6o7Bw4cACzZ89WOgY9hsUWkRWoVasWQkJClI5BVKhBgwYpHYH+hsWWZeHdiERERERmxGKLiIiIyIxYbBERERGZEYstIiIiIjNisUVERERkRiy2iIiIiMyIxRYRERGRGbHYIiIiIjIjFltEREREZsRii4iIiMiMWGwRERERmRGLLSIiIiIzYrFFREREZEYstoiIiIjMiMUWUQX3ww8/wNXVFZs2bQIAtGnTBmq1Gi1atDB63sIMHToUlSpVgkqlwvHjx02W+0kZ009jGdrnotoZMq6W5OzZs3j77bfRtGlTVKpUCba2tnB1dUXDhg3RrVs3HDhwQOmIRIpisUVUwYlIvn8fOnQInTp1KtW8hVm0aBEWLlxYqmzmZEw/jWVon4tqZ8i4WorFixfD29sb8fHx+Pzzz3Ht2jWkp6fj2LFjmDx5MlJSUnDixAmlYxIpylbpAERkepmZmejSpQt+/fXXEtt269YN9+/fLzBdpVKVel5rYkg/y5q1jOvBgwcxfPhwvPDCC9i2bRtsbf/3keLl5QUvLy+4ubnh/PnzCqYsnjH7SnlaN5UtFltE5dDixYtx8+bNJ1qGnZ2didJYZkHziCn7+ThD+1wWYyMiWLt2Le7du4dhw4aZbLlTpkxBbm4uoqKi8hVajwsICEBAQIDJ1mlqpthXrHHdVLb4NSJROfPOO+/gvffew8WLF6FSqfDMM8/g008/haOjIypVqoSbN2/ivffeQ82aNbF48WJ4enpCpVJh7ty5+ZZz4cIFNGrUCE5OTnBwcIC/vz/27dunf33fvn2FzisimDlzJp599lloNBq4urri/fffL3V/cnNzMXHiRHh6esLBwQHNmzdHdHQ0AGD27NlwcnKCjY0NWrdujWrVqsHOzg5OTk5o1aoV/P39Ubt2bWi1Wri5ueGDDz4osPyS+llSBmP6bEi7wsZ1/vz5cHJygqOjIzZs2IDAwEC4uLigVq1aWL16dYGs06ZNw7PPPgsHBwdUrVoV9erVw7Rp0xASEqJvt3v3bvj6+sLR0REuLi7w9vZGamoqAGDr1q1wcXHB1KlTi3xfsrOzsXPnTlSpUgW+vr5FtitsDD7//HM0btwYGo0G7u7u6NWrF86cOaNvY0x/AWD58uXw8fGBVquFk5MT6tati8mTJwMA9u7diyZNmsDV1RVarRbe3t7Ytm0bgML3lUdjWNT7bUw2U6+brJgQkUXr27ev9O3b16h5goKCpH79+vmmffjhhwJAwsPDZc6cOdKnTx85ffq0XLt2TQDInDlz9G27dOkiXl5ecvnyZdHpdJKQkCBt27YVrVYr586d07crbN4PP/xQVCqV/Oc//5F79+5JRkaGzJs3TwDIsWPHjO7/2LFjRaPRyNq1a+XevXsyfvx4sbGxkUOHDomIyMcffywAJC4uTtLT0+X27dvy0ksvCQDZsmWL3Lp1S9LT02X06NECQI4fP250P0vKYGifDW1X1LgCkJ07d8r9+/fl5s2b4u/vL05OTpKdna1vN3XqVFGr1bJhwwbJyMiQI0eOSLVq1aRjx476Ng8ePBAXFxeZMWOGZGZmSnJysvTp00du3bolIiKbN2+WSpUqyaRJk4p8X86dOycApF27dka9nxMnThR7e3tZvny5pKSkSHx8vLRq1UqqVq0qycnJRvd31qxZAkCioqLkzp07cvfuXfnmm2+kf//+IiISGxsrkZGRcvfuXblz5460a9dOqlSpop+/sH3FkPfbkGzmWLchoqOjhR/vFiWG7waRhTN1sZWZmZlvelHF1nPPPZevXXx8vACQsWPHFjlvRkaGODo6SteuXfPNu3r16lIVW5mZmeLo6ChhYWH6aRkZGaLRaGTkyJEi8r9iKy0tTd/m22+/FQBy4sQJ/bTffvtNAMiaNWuM6mdJGQztszFjU1yx9fj796hQu3Dhgn5amzZtxNfXN986hg0bJjY2NpKVlSUiIgkJCQJANm/eLKV1+PBhASD//Oc/DZ4nIyNDnJ2d842lyP/em8eLO0P6m52dLW5ubtKpU6d8y8vJyZHZs2cXmmHatGkCQG7evCkiBfcVQ7Y5Q98Lc6zbECy2LE4Mv0YkIoN4e3vD1dUV8fHxRba5cOECMjIy0KVLF5Os8+zZs8jIyECzZs300xwcHFC9evV8Xzv9nb29PQAgJydHP+3RtVk6na7Ydf69nyVlMLTPph4b4H/9fLxPDx8+LHA3Y25uLuzs7KBWqwH8dfH6U089hQEDBiAyMhKJiYlGr9vZ2RkAkJGRYfA8J0+exIMHD+Dj45Nveps2bWBvb4+4uLhi5/97f+Pj45GSklLgmjC1Wo3w8PBCl/FoO8jNzS309Sfd5orbvsy1brJ8LLaIyGB2dnbFfphcv34dAODh4WGS9aWnpwMAJkyYAJVKpf+7cuWKUR/yxnq8nyVlMLTPph6borz88ss4cuQINmzYgMzMTBw+fBjr169H9+7d9cWWg4MDfv75Z/j5+WHq1Knw8vJCWFgYMjMzDV5P3bp1odVqce7cOYPnSUlJAfC/Qu1xbm5uSEtLM3hZAPTXmLm5uRXZZsuWLejYsSM8PDyg0WgKvW7vcabc5pRcN1kWFltEZJCcnBzcvXsXnp6eRbbRarUAgKysLJOs81FhMmvWLIhIvj9zPSjz7/0sKYOhfTb12BQlMjISnTt3xuDBg+Hi4oI+ffogJCSkwPO8mjZtik2bNiEpKQkRERGIjo7GZ599ZvB6NBoNAgICcPv2bezfv7/Idnfv3sXQoUMB/K8oKqyoSklJQa1atQxePwDUqFEDAHD79u1CX7969Sp69+6N6tWrIy4uDvfv38eMGTOKXaaptjkl102Wh8UWERnkl19+QV5eHlq1alVkm2bNmsHGxga7d+82yTof3UlYlk+e/3s/S8pgaJ9NPTZFOXnyJC5evIhbt25Bp9Ph6tWrmD9/Ptzd3fVtkpKScOrUKQB/fcBHRUWhVatW+mmGioyMhEajwZgxY4o8K5aQkKB/LESzZs3g7OyMw4cP52sTFxeH7OxstG7d2qj1161bF5UrV8ZPP/1U6OsnTpyATqfDyJEj4eXlBa1WW+KjNky1zSm5brI8LLaIyqHKlSsjKSkJiYmJSEtLK/E6pcJkZ2fj/v37yMnJwdGjRzF69GjUqVMHgwcPLnIeDw8PBAUFYe3atVi8eDFSU1MRHx+PBQsWlKofWq0WQ4YMwerVqzF//nykpqYiNzcX169fx40bN0q1zL8rqZ8lZTC0z6Yem0GCNN8AACAASURBVKKMGjUKnp6eePDgQZFtkpKS8Oabb+LMmTPIzs7GsWPHcOXKFbRr1w4A8OOPP5b46AcAaNGiBVauXImEhAT4+/vjhx9+wP3796HT6XD58mUsXLgQr7/+uv5aJa1Wi/feew/r1q3DihUrkJqaihMnTmDEiBF4+umnMXz4cKP6qtFoMH78eOzZswejR4/GH3/8gby8PKSlpeHUqVP6s5M7duzAw4cPcf78+QLXhf19X1Gr1SbZ5pRcN1mgMr8mn4iMUpq7EY8ePSp16tQRBwcH8fPzkzFjxoiDg4MAkNq1a8vy5ctFRGTOnDlSvXp1ASCOjo7Ss2dPERFZunSpdOrUSZ566imxtbWVKlWqSL9+/eTKlSv6dRQ1b1pamgwdOlSqVKkizs7O4ufnJxMnThQAUqtWLfn999+N6ktWVpZERESIp6en2NraioeHhwQFBcnJkydl9uzZ4ujoKACkbt26snfvXpk+fbq4uroKAKlWrZqsXLlS1qxZI9WqVRMA4u7uLqtXrza4nyVlMKbPhrQrbFznzZun72eDBg3k4sWLsmDBAnFxcREAUqdOHf2jKn7++WepUqWKAND/2dnZSePGjeW7774TEZHExERp3769uLu7i1qtlho1asiHH34oOTk5IiLyww8/SKVKlWTKlCkGvUdXr16VsWPHire3tzg7O4tarRY3Nzdp2bKlvP7667J//35927y8PJk5c6Y0aNBA7OzsxN3dXXr37i1nz57VtzGmvyIic+fOFW9vb9FqtaLVaqVly5Yyb948ERGJiIiQypUri5ubmwQHB8vcuXMFgNSvX1+uXr1aYF9JTk4u9v02Jpup120o3o1ocWJUIlb0I1xEFVBwcDAAIDY2VuEkZA3mz5+P8+fPY9asWfpp2dnZGDduHObPn4979+7BwcFBwYRkbjExMQgNDbWq39gs52L5cz1EROVEcnIyRo8eXeCaH3t7e3h6ekKn00Gn07HYIipjvGaLiMrUmTNn8t3WXtRfWFiY0lGtjoODA+zs7LB48WL8+eef0Ol0SEpKwqJFizBx4kSEhYXBxcVF6ZhEFQ7PbBFRmWrUqBG/3jATV1dX/PTTT5g0aRIaNmyI9PR0ODs7o2nTppg+fbpJf4SaiAzHYouIqBzx9/fH9u3blY5BRI/h14hEREREZsRii4iIiMiMWGwRERERmRGLLSIiIiIzYrFFREREZEYstoiIiIjMiMUWERERkRmx2CIiIiIyIxZbRERERGbEYouIiIjIjFhsEREREZkRiy0iIiIiM2KxRURERGRGtkoHIKKSrV27FiqVSukYRERUCiy2iCzcmDFjEBwcrHQMskChoaF455138PzzzysdhYiKoRIRUToEEREZT6VSITo6GiEhIUpHIaKixfKaLSIiIiIzYrFFREREZEYstoiIiIjMiMUWERERkRmx2CIiIiIyIxZbRERERGbEYouIiIjIjFhsEREREZkRiy0iIiIiM2KxRURERGRGLLaIiIiIzIjFFhEREZEZsdgiIiIiMiMWW0RERERmxGKLiIiIyIxYbBERERGZEYstIiIiIjNisUVERERkRiy2iIiIiMyIxRYRERGRGbHYIiIiIjIjFltEREREZsRii4iIiMiMWGwRERERmRGLLSIiIiIzYrFFREREZEYstoiIiIjMiMUWERERkRmx2CIiIiIyIxZbRERERGbEYouIiIjIjFhsEREREZmRrdIBiIioZFeuXEFubm6B6X/++ScuXbqUb9rTTz8NBweHsopGRCVQiYgoHYKIiIoXGBiIrVu3ltjO1tYWycnJqFKlShmkIiIDxPJrRCIiKxAWFgaVSlVsGxsbG3Tt2pWFFpGFYbFFRGQF+vTpAzs7uxLbDRw4sAzSEJExWGwREVmBSpUqoXv37sUWXHZ2dujRo0cZpiIiQ7DYIiKyEv3790dOTk6hr9na2qJ3795wdnYu41REVBIWW0REVqJbt25wcnIq9LXc3Fz079+/jBMRkSFYbBERWQmNRoO+ffvC3t6+wGvOzs548cUXFUhFRCVhsUVEZEVeffVVZGdn55tmZ2eHsLCwQoswIlIeiy0iIivSpUsXVK1aNd80nU6HV199VaFERFQSFltERFbExsYGr776ar6zWB4eHvD391cwFREVh8UWEZGV6devn/6rRHt7ewwaNAhqtVrhVERUFBZbRERWpm3btqhduzYAIDs7G2FhYQonIqLisNgiIrIyKpUKgwYNAgDUqVMHPj4+Ciciov9r7+5jq6zv/4+/rt73wDktarHFgmsLwgaCOkUsfBmGuI0RmdoCRZBVgymwzXkTVjIUlcnMqNJlBmJA5hJN6mnBADJB/yB2WVLdyLhRWLldmayUIqKFttLSvr9/+LO/77HQG3ourp72+UiuP/j0fV2f9/mkqS+v8znX6UiM1w0A6HsqKiq0evVqr9vo0+rq6iRJAwYM0MyZMz3upm+766679OSTT3rdBiIYd7YAhN2nn36qjRs3et1GnxYIBJSUlKT09HSvW+nTPvzwQ1VUVHjdBiIcd7YAuKasrMzrFvq09957Tz/60Y+8bqNP464hwoE7WwAQoQhaQGQgbAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAiwhbAAAALiJsAQAAuIiwBQAA4CLCFgAAgIsIWwAAAC4ibAEAALiIsAWgV1qwYIH8fr8cx9GePXu8bsdTK1as0Pe+9z0FAgHFx8dr+PDh+vWvf63z5893+1qbNm1SZmamHMcJOeLi4jR48GBNmTJFRUVFOnv2rAuvBOifCFsAeqXXXntN69ev97qNXmHnzp36xS9+oaqqKn322Wf63e9+pz/84Q+aOXNmt6+Vk5OjY8eOKSsrS0lJSTIztba2qra2VqWlpcrIyFBhYaFGjx6tXbt2ufBqgP6HsAUAV0FjY6Oys7Ov6NyBAweqoKBA11xzjfx+v2bNmqX7779fO3bs0Kefftrj3hzHUXJysqZMmaLXX39dpaWlOnXqlKZPn64vv/yyx9f3Wk/WHggHwhaAXstxHK9bCJsNGzaotrb2is7dtm2boqOjQ8auu+46SVJDQ0OPe/u23Nxc5efnq7a2Vq+++mrYr3+19WTtgXAgbAHoFcxMRUVFGjlypOLj45WUlKQlS5aE1KxatUo+n09+v1+1tbV66qmndMMNN+jgwYMyM61evVrf/e53FR8fr0GDBum+++5TZWVl2/l//OMflZCQoMGDB2vhwoVKS0tTQkKCsrOz9dFHH7Xrp7PrPfbYY4qLi1Nqamrb2M9//nMNGDBAjuPos88+kyQ9/vjjeuqpp3T06FE5jqPhw4f3eL3++9//KjExURkZGW1jO3bsUCAQ0MqVK3t8/fz8fEnS9u3bJbH2QI8YAIRZMBi07v55WbZsmTmOYy+//LKdPXvWGhoabM2aNSbJdu/eHVInyX71q1/ZK6+8Yg888ID961//suXLl1tcXJy98cYb9sUXX9i+ffvstttus+uuu85qamrazi8oKLABAwbYgQMH7KuvvrL9+/fbHXfcYX6/3/7zn/+01XX1enPnzrXrr78+5LUUFRWZJDt9+nTbWE5OjmVlZXVrTS6nvr7e/H6/PfbYYyHj27ZtM7/fbytWrOj0GllZWZaUlHTZn9fV1ZkkGzp0aNtYf1z73Nxcy83NvaJzgf+nlLAFIOy6G7YaGhrM5/PZPffcEzJeUlJy2bDV2NgYcv7AgQMtLy8v5Py///3vJikkfBQUFLQLGf/4xz9Mkj3//PPdvp4XYWvZsmV20003WV1d3RVfo7OwZWbmOI4lJyeHzNvf1p6whTAojbnKN9IAoJ0jR46ooaFBU6dOvaLz9+/fr/Pnz+v2228PGb/jjjsUFxfX7m2qb7v99tvl8/na3qbq6fXc9Pbbb6u0tFTvv/++/H6/a/PU19fLzBQIBDqs609rD1wpwhYAz504cUKSlJKSckXnf/HFF5K+/tTetyUnJ+vcuXOdXiM+Pl6nT58O2/Xc8NZbb2n16tX64IMPNGTIEFfnOnTokCRp1KhRHdb1l7UHeoKwBcBzCQkJkqQLFy5c0fnJycmSdMn/EH/xxRdKT0/v8Pzm5uaQup5ezw2vvPKK3nvvPe3cufOSQSTcduzYIUmaNm1ah3X9Ye2BnuLTiAA8N2bMGEVFRam8vPyKzx84cGC7h3B+9NFHampq0ve///0Oz//ggw9kZpowYUK3rxcTE6Pm5uYr6rsrzEyFhYX6+OOPtXnz5qsStGpqalRcXKz09HQ98sgjHdb25bUHwoWwBcBzKSkpysnJ0caNG7VhwwbV1dVp3759WrduXZfOT0hI0FNPPaW3335bb775purq6vTxxx9r0aJFSktLU0FBQUh9a2urzp49q4sXL2rfvn16/PHHNWzYsLbHHXTnesOHD9fnn3+uzZs3q7m5WadPn9bx48fb9XjNNdeourpaVVVVOnfuXJdDwoEDB7Rq1SqtX79esbGx7b5m56WXXmqr3b59e7ce/WBmOn/+vFpbW2VmOn36tILBoCZOnKjo6Ght3ry50z1bfXntgbDxcns+gL7pSh79cO7cOVuwYIFde+21NnDgQJs0aZItX77cJFl6errt3bvXfv/731tiYmLbIwneeOONtvNbW1utqKjIRowYYbGxsTZo0CC7//777eDBgyHzFBQUWGxsrN1www0WExNjgUDA7rvvPjt69GhIXVevd+bMGbv77rstISHBMjIy7Je//KUtWbLEJNnw4cPbHmnwz3/+02688UZLTEy0SZMmhTzCoCMff/yxSbrsUVRU1Fb77rvvmt/vtxdeeOGy19u6dauNHTvWfD6fxcXFWVRUlElq++Th+PHjbcWKFXbmzJmQ8/rj2pvxaUSERaljZuZFyAPQd5WWlmr27NnqjX9eFi5cqLKyMp05c8brVvqdSFz7b75/sqyszONOEMHKeBsRQL/T0tLidQv9FmuP/oiwBQBXWWVlZbu9V5c68vLyvG4VQBgQtgD0G7/5zW/0+uuv68svv1RGRoY2btzoSR+jRo2SmXV6vPXWW57054besvaAF9izBSDsevOeLaA72LOFMGDPFgAAgJsIWwAAAC4ibAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAiwhbAAAALiJsAQAAuIiwBQAA4CLCFgAAgItivG4AQN81c+ZMr1sAeuTDDz/UhAkTvG4DEY47WwDCbujQocrNzfW6jT5v69atqq6u9rqNPm3ChAm66667vG4DEc4xM/O6CQBA9zmOo2AwqFmzZnndCoDLK+POFgAAgIsIWwAAAC4ibAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAiwhbAAAALiJsAQAAuIiwBQAA4CLCFgAAgIsIWwAAAC4ibAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAiwhbAAAALiJsAQAAuIiwBQAA4CLCFgAAgIsIWwAAAC4ibAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAixwzM6+bAAB07KGHHtKePXtCxqqqqpSSkqIBAwa0jcXGxuqdd97RDTfccLVbBHBpZTFedwAA6NzIkSP15ptvths/f/58yL9HjRpF0AJ6Gd5GBIAIMGfOHDmO02FNbGys8vPzr05DALqMsAUAESArK0u33nqroqIu/2f74sWLmj179lXsCkBXELYAIELMnz//smHLcRyNHz9e3/nOd65uUwA6RdgCgAgxe/Zstba2XvJnUVFRmj9//lXuCEBXELYAIEKkpqbqf/7nfxQdHX3Jn+fk5FzljgB0BWELACLIQw891G4sKipKd999t66//noPOgLQGcIWAESQmTNnXnLf1qVCGIDegbAFABEkEAjoxz/+sWJi/v9jEqOjo/XTn/7Uw64AdISwBQARZt68eWppaZEkxcTEaMaMGUpKSvK4KwCXQ9gCgAgzY8YMJSYmSpJaWlo0d+5cjzsC0BHCFgBEmISEBD3wwAOSJJ/Pp2nTpnncEYCO8N2IAHqN0tJSr1uIGEOHDpUk3XHHHdq6davH3USO7Oxspaene90G+hnHzMzrJgBAUqff/Qf0VDAY1KxZs7xuA/1LGW8jAuhVgsGgzIyjC8ezzz6r5uZmz/uIlAPwCmELACLU008/HfIICAC9E2ELACIUQQuIDIQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEWELQAAABcRtgAAAFxE2AIAAHARYQsAAMBFhC0AAAAXEbYAAABcRNgCAABwEWELAADARYQtAH3GggUL5Pf75TiO9uzZ43U7PdLa2qri4mJlZ2dftuZvf/ubJk6cKJ/Pp7S0NBUWFurChQvdnmvTpk3KzMyU4zghR1xcnAYPHqwpU6aoqKhIZ8+e7clLAvotwhaAPuO1117T+vXrvW6jxw4fPqzJkyfrySefVENDwyVr9u/frx/+8IeaOnWqTp8+rbffflt/+tOftGjRom7Pl5OTo2PHjikrK0tJSUkyM7W2tqq2tlalpaXKyMhQYWGhRo8erV27dvX05QH9DmELAHqRvXv3aunSpVq0aJFuueWWy9b99re/VWpqqp5//nkNGDBAd911lwoLC/XnP/9ZlZWVPe7DcRwlJydrypQpev3111VaWqpTp05p+vTp+vLLL3t8faA/IWwB6FMcx/G6hR4ZN26cNm3apLlz5yo+Pv6SNRcvXtRf/vIX/eAHPwh5vdOmTZOZacuWLWHvKzc3V/n5+aqtrdWrr74a9usDfRlhC0DEMjMVFRVp5MiRio+PV1JSkpYsWdKurqWlRcuXL9ewYcOUmJiosWPHKhgMSpLWrl2rAQMGyOfzacuWLZo2bZoCgYDS09NVUlIScp3y8nKNHz9ePp9PgUBAN998s+rq6jqdI9yOHTum8+fPa9iwYSHjWVlZkqR9+/a1je3YsUOBQEArV67s8bz5+fmSpO3bt7eN9bW1BdxA2AIQsZ555hkVFhaqoKBAp06dUk1NjZYuXdqubunSpVq1apWKi4t18uRJ3XvvvXrwwQe1a9cuLV68WE888YQaGxvl9/sVDAZ19OhRZWZm6tFHH1Vzc7Mkqb6+XjNmzFBubq4+//xzHT58WDfddJOampo6nSPcampqJEl+vz9kPCEhQYmJiTp16lTbWEtLi6SvN9z31Ddvax47dqxtrK+tLeAKA4BeQpIFg8Eu1TY0NJjP57N77rknZLykpMQk2e7du83MrLGx0Xw+n+Xl5YWcGx8fb4sXLzYzs2XLlpkka2xsbKtZs2aNSbIjR46Ymdknn3xikmzbtm3teunKHFfizjvvtHHjxrUbf//9902SrV69ut3PAoGAZWdnX9F8WVlZlpSU1GGN4ziWnJxsZpG3tt35/QLCqJQ7WwAi0pEjR9TQ0KCpU6d2WHfw4EE1NDRozJgxbWOJiYlKTU3tcCN5XFycJLXdfcnMzNTgwYM1b948Pffcc6qqqurxHFcqISFB0td7t76tqalJiYmJYZ9T+voOlJkpEAhI6ptrC7iBsAUgIp04cUKSlJKS0mFdfX29JOnpp58OeYbU8ePHL/tYhUtJTEzUzp07NWnSJK1cuVKZmZnKy8tTY2Nj2OboqtTUVElq29P0jYaGBn311VdKS0sL+5ySdOjQIUnSqFGjJPXNtQXcQNgCEJG+ubvT2UM8vwljxcXFMrOQo6Kioltzjh49Wu+8846qq6tVWFioYDCol156KaxzdEVGRob8fr+OHz8eMn7kyBFJ0tixY8M+p/T1Znvp6089Sn1zbQE3ELYARKQxY8YoKipK5eXlHdYNHTpUCQkJPX6ifHV1tQ4cOCDp65Dx4osv6rbbbtOBAwfCNkdXxcTE6Cc/+Yn++te/hmx83759uxzH0YwZM8I+Z01NjYqLi5Wenq5HHnlEUt9cW8ANhC0AESklJUU5OTnauHGjNmzYoLq6Ou3bt0/r1q0LqUtISNDDDz+skpISrV27VnV1dWppadGJEyd08uTJLs9XXV2thQsXqrKyUk1NTdq9e7eOHz+uCRMmhG2O7njmmWd06tQpPfvss6qvr1dFRYWKioqUn5+vkSNHttVt3769W49+MDOdP39era2tMjOdPn1awWBQEydOVHR0tDZv3ty2Z6uvri0Qdld5Rz4AXJa6+Wmxc+fO2YIFC+zaa6+1gQMH2qRJk2z58uUmydLT023v3r1mZnbhwgUrLCy0YcOGWUxMjKWkpFhOTo7t37/f1qxZYz6fzyTZiBEj7OjRo7Zu3ToLBAImyW688UY7dOiQVVVVWXZ2tg0aNMiio6NtyJAhtmzZMrt48WKnc3RHRUWFTZw40dLS0kySSbLU1FTLzs628vLykNry8nIbP368xcfHW1pami1ZssS++uqrkJp3333X/H6/vfDCC5edc+vWrTZ27Fjz+XwWFxdnUVFRJqntk4fjx4+3FStW2JkzZ9qdG0lr293fLyBMSh0zM6+CHgD8X47jKBgMatasWV63gj6I3y94pIy3EQEAAFxE2AIAF1VWVoY8suByR15entetAnBJjNcNAEBfNmrUKLFbA+jfuLMFAADgIsIWAACAiwhbAAAALiJsAQAAuIiwBQAA4CLCFgAAgIsIWwAAAC4ibAEAALiIsAUAAOAiwhYAAICLCFsAAAAuImwBAAC4iLAFAADgIsIWAACAi2K8bgAA/q+KigqvWwCAsHLMzLxuAgAkyXEcr1tAHxcMBjVr1iyv20D/UsadLQC9Bv/v1z2O4xAegAjAni0AAAAXEbYAAABcRNgCAABwEWELAADARYQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEWELQAAABcRtgAAAFxE2AIAAHARYQsAAMBFhC0AAAAXEbYAAABcRNgCAABwEWELAADARYQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEWELQAAABcRtgAAAFxE2AIAAHARYQsAAMBFhC0AAAAXEbYAAABcRNgCAABwEWELAADARYQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEWELQAAABfFeN0AAKBz69at09mzZ9uNb9myRf/+979DxvLz83X99ddfrdYAdMIxM/O6CQBAxwoKCrRu3TrFx8e3jZmZHMdp+/fFixeVlJSkmpoaxcbGetEmgPbKeBsRACLAnDlzJEkXLlxoO5qamkL+HRUVpTlz5hC0gF6GsAUAEWDy5MkaPHhwhzXNzc1toQxA70HYAoAIEBUVpXnz5ikuLu6yNWlpacrOzr6KXQHoCsIWAESIOXPmqKmp6ZI/i42N1fz580P2cAHoHQhbABAhbr/9dmVkZFzyZ7yFCPRehC0AiCDz58+/5Ab4zMxMjRs3zoOOAHSGsAUAEWTevHlqbm4OGYuNjdXDDz/sUUcAOkPYAoAIMnz4cN18880he7Oam5s1e/ZsD7sC0BHCFgBEmPnz5ys6OlqS5DiObr31Vo0YMcLjrgBcDmELACLMgw8+qJaWFklSdHS0fvazn3ncEYCOELYAIMIMGTJE2dnZchxHra2tmjlzptctAegAYQsAItBDDz0kM9PkyZM1ZMgQr9sB0AG+iBpAr1FaWspGb7gmNzdXZWVlXreB/qcsxusOAODbgsGg1y1EhJdfflkFBQUaOHCg1630esXFxV63gH6MsAWg15k1a5bXLUSE7Oxspaene91GROCOFrzEni0AiFAELSAyELYAAABcRNgCAABwEWELAADARYQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEWELQAAABcRtgAAAFxE2AIAAHARYQsAAMBFhC0AAAAXEbYAAABcRNgC0KcsWLBAfr9fjuNoz549XrfTI62trSouLlZ2dnaPajqzadMmZWZmynGckCMuLk6DBw/WlClTVFRUpLNnz17xHEB/RtgC0Ke89tprWr9+vddt9Njhw4c1efJkPfnkk2poaLjimq7IycnRsWPHlJWVpaSkJJmZWltbVVtbq9LSUmVkZKiwsFCjR4/Wrl27rngeoL+K8boBAECovXv3asWKFVq0aJHq6+tlZldU0xOO4yg5OVlTpkzRlClTNH36dM2ePVvTp0/XoUOHlJSUFNb5gL6MO1sA+hzHcbxuoUfGjRunTZs2ae7cuYqPj7/imnDKzc1Vfn6+amtr9eqrr7o+H9CXELYARDQzU1FRkUaOHKn4+HglJSVpyZIl7epaWlq0fPlyDRs2TImJiRo7dqyCwaAkae3atRowYIB8Pp+2bNmiadOmKRAIKD09XSUlJSHXKS8v1/jx4+Xz+RQIBHTzzTerrq6u0zm8smPHDgUCAa1cubLH18rPz5ckbd++vW2sv64r0B2ELQAR7ZlnnlFhYaEKCgp06tQp1dTUaOnSpe3qli5dqlWrVqm4uFgnT57UvffeqwcffFC7du3S4sWL9cQTT6ixsVF+v1/BYFBHjx5VZmamHn30UTU3N0uS6uvrNWPGDOXm5urzzz/X4cOHddNNN6mpqanTObzS0tIi6euN9D11yy23SJKOHTvWNtZf1xXoFgOAXiIYDFp3/iw1NDSYz+eze+65J2S8pKTEJNnu3bvNzKyxsdF8Pp/l5eWFnBsfH2+LFy82M7Nly5aZJGtsbGyrWbNmjUmyI0eOmJnZJ598YpJs27Zt7XrpyhxX4s4777Rx48b1uKYrsrKyLCkpqcMax3EsOTnZzCJrXXNzcy03N7fL9UAYlXJnC0DEOnLkiBoaGjR16tQO6w4ePKiGhgaNGTOmbSwxMVGpqamqrKy87HlxcXGS1HYHJjMzU4MHD9a8efP03HPPqaqqqsdzRJJvNuIHAgFJrCvQVYQtABHrxIkTkqSUlJQO6+rr6yVJTz/9dMhzpI4fP96tRyYkJiZq586dmjRpklauj0KwgAAAAxNJREFUXKnMzEzl5eWpsbExbHP0ZocOHZIkjRo1ShLrCnQVYQtAxEpISJAkXbhwocO6b8JYcXGxzCzkqKio6Naco0eP1jvvvKPq6moVFhYqGAzqpZdeCuscvdWOHTskSdOmTZPEugJdRdgCELHGjBmjqKgolZeXd1g3dOhQJSQk9PiJ8tXV1Tpw4ICkr4PGiy++qNtuu00HDhwI2xy9VU1NjYqLi5Wenq5HHnlEEusKdBVhC0DESklJUU5OjjZu3KgNGzaorq5O+/bt07p160LqEhIS9PDDD6ukpERr165VXV2dWlpadOLECZ08ebLL81VXV2vhwoWqrKxUU1OTdu/erePHj2vChAlhmyPctm/f3q1HP5iZzp8/r9bWVpmZTp8+rWAwqIkTJyo6OlqbN29u27PVn9cV6JarvCMfAC6ru59GNDM7d+6cLViwwK699lobOHCgTZo0yZYvX26SLD093fbu3WtmZhcuXLDCwkIbNmyYxcTEWEpKiuXk5Nj+/fttzZo15vP5TJKNGDHCjh49auvWrbNAIGCS7MYbb7RDhw5ZVVWVZWdn26BBgyw6OtqGDBliy5Yts4sXL3Y6R3dUVFTYxIkTLS0tzSSZJEtNTbXs7GwrLy/vco2Z2bvvvmt+v99eeOGFy863detWGzt2rPl8PouLi7OoqCiT1PbJw/Hjx9uKFSvszJkz7c6NlHXl04jwUKljFubveACAK1RaWqrZs2eH/atngJkzZ0qSysrKPO4E/VAZbyMCAAC4iLAFAC6rrKwMeWzB5Y68vDyvWwXgghivGwCAvm7UqFG8NQr0Y9zZAgAAcBFhCwAAwEWELQAAABcRtgAAAFxE2AIAAHARYQsAAMBFhC0AAAAXEbYAAABcRNgCAABwEWELAADARYQtAAAAFxG2AAAAXETYAgAAcBFhCwAAwEUxXjcAAN/mOI7XLaAPys3N9boF9FOELQC9RnZ2toLBoNdtoI8aOnSo1y2gn3LMzLxuAgAAoI8qY88WAACAiwhbAAAALiJsAQAAuChGUpnXTQAAAPRRH/4vCIK+qdpQbOsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV3YQM-oe_yW",
        "outputId": "dc044b3a-8667-446d-c903-61b0f928cd82"
      },
      "source": [
        "# Compiling and fitting the model (Fun Part!)\n",
        "tribid_model.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                     optimizer= tf.keras.optimizers.Adam() , \n",
        "                     metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the model for fewer epochs  (training only on 10% of the data)\n",
        "# To speed up the experimentation\n",
        "\n",
        "tribid_model.fit(train_dataset , \n",
        "                 steps_per_epoch = int(0.1 * len(train_dataset)), \n",
        "                 epochs = 3 , \n",
        "                 validation_steps = int(0.1 * len(val_dataset)),\n",
        "                 validation_data = val_dataset)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "281/281 [==============================] - 863s 3s/step - loss: 0.7086 - accuracy: 0.7498 - val_loss: 0.4427 - val_accuracy: 0.8421\n",
            "Epoch 2/3\n",
            "281/281 [==============================] - 857s 3s/step - loss: 0.4773 - accuracy: 0.8322 - val_loss: 0.3641 - val_accuracy: 0.8494\n",
            "Epoch 3/3\n",
            "281/281 [==============================] - 856s 3s/step - loss: 0.4101 - accuracy: 0.8564 - val_loss: 0.3330 - val_accuracy: 0.8813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6106b7590>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELuWgnVrqwJC",
        "outputId": "7f7dbdf6-8841-4d32-9bec-20c4fbcf8961"
      },
      "source": [
        "# Evaluating on the whole val data \n",
        "tribid_model.evaluate(val_dataset)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "473/473 [==============================] - 1233s 3s/step - loss: 0.3193 - accuracy: 0.8822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3193438947200775, 0.8822322487831116]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ6mxxeOfXeA"
      },
      "source": [
        "### 4. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiIhcDvrgTLX",
        "outputId": "c6cf5bbb-8fbc-4901-9eae-d4e9da93701a"
      },
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column \n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "\n",
        "# Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column \n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "\n",
        "# Download pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainable=False,\n",
        "                                        name=\"universal_sentence_encoder\")\n",
        "\n",
        "# Check shape and samples of total lines one-hot tensor\n",
        "train_total_lines_one_hot.shape, train_line_numbers_one_hot.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([180040, 20]), TensorShape([180040, 15]))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n98VUaay67M",
        "outputId": "702d269f-a02f-44e6-8918-96348b956c97"
      },
      "source": [
        "# Re-building the Model 5 \n",
        "\n",
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_embeddings)\n",
        "\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape= [], dtype=\"string\", name=\"char_inputs\")\n",
        "char_embeddings = tf_hub_embedding_layer(char_inputs)\n",
        "exp_layer = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(char_embeddings)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(exp_layer)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Line numbers inputs\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
        "                                   outputs=x)\n",
        "\n",
        "# 4. Total lines inputs\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_line_model = tf.keras.Model(inputs=total_lines_inputs,\n",
        "                                  outputs=y)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, \n",
        "                                                                              char_model.output])\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
        "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
        "                                                                total_line_model.output,\n",
        "                                                                z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
        "\n",
        "# 8. Put together model\n",
        "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                 total_line_model.input,\n",
        "                                 token_model.input, \n",
        "                                 char_model.input],\n",
        "                         outputs=output_layer)\n",
        "\n",
        "\n",
        "\n",
        "# Summary of the model \n",
        "model_5.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_36\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "token_inputs (InputLayer)       [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "char_inputs (InputLayer)        [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "universal_sentence_encoder (Ker (None, 512)          256797824   token_inputs[0][0]               \n",
            "                                                                 char_inputs[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 1, 512)       0           universal_sentence_encoder[13][0]\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 64)           139520      lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "token_char_hybrid_embedding (Co (None, 576)          0           universal_sentence_encoder[12][0]\n",
            "                                                                 bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "line_number_input (InputLayer)  [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "total_lines_input (InputLayer)  [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 256)          147712      token_char_hybrid_embedding[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 32)           512         line_number_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 32)           672         total_lines_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 256)          0           dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "token_char_positional_embedding (None, 320)          0           dense_34[0][0]                   \n",
            "                                                                 dense_35[0][0]                   \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "output_layer (Dense)            (None, 5)            1605        token_char_positional_embedding[0\n",
            "==================================================================================================\n",
            "Total params: 257,087,845\n",
            "Trainable params: 290,021\n",
            "Non-trainable params: 256,797,824\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGBD_B3v59xS",
        "outputId": "6464c947-2440-4adc-e253-1fd307dfd81b"
      },
      "source": [
        "# Create training and validation datasets (all four kinds of inputs)\n",
        "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, \n",
        "                                                                train_total_lines_one_hot, \n",
        "                                                                train_sentences, \n",
        "                                                                train_chars)) \n",
        "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) \n",
        "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) \n",
        "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
        "\n",
        "# Validation dataset\n",
        "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
        "                                                              val_total_lines_one_hot,\n",
        "                                                              val_sentences,\n",
        "                                                              val_chars))\n",
        "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
        "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
        "\n",
        "# Check input shapes\n",
        "train_pos_char_token_dataset, val_pos_char_token_dataset"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>,\n",
              " <PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.float64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtUn9dSO7KTc"
      },
      "source": [
        "`tf.keras.callbacks.ModelCheckpoint` to save the model's best weights only.\n",
        "\n",
        "`tf.keras.callbacks.EarlyStopping` to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMhZJNlD7UE2"
      },
      "source": [
        "# Creating the callbacks \n",
        "check_filepath = 'best_weights/checkpoint.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath= check_filepath , \n",
        "                                                               save_weights_only = True , \n",
        "                                                               save_best_only = True  , \n",
        "                                                               save_freq = 'epoch' , \n",
        "                                                               monitor = 'val_loss')\n",
        "\n",
        "early_stopping  = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss' , \n",
        "                                                   patience = 3 )\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, \n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, \n",
        "                                                 min_lr=1e-7)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqFqTdx088tR"
      },
      "source": [
        "Now while compile and fit the data on 100% of the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVRXVWux9fMj",
        "outputId": "dba7b68f-a54c-4be2-8677-6ce62b299211"
      },
      "source": [
        "model_5.compile(loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing= 0.2) ,\n",
        "                    optimizer = tf.keras.optimizers.Adam() ,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "history = model_5.fit(train_pos_char_token_dataset , \n",
        "                      epochs = 100 , \n",
        "                      validation_data = val_pos_char_token_dataset  , \n",
        "                      callbacks = [early_stopping , model_checkpoint_callback , \n",
        "                                   reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5627/5627 [==============================] - 291s 51ms/step - loss: 0.9706 - accuracy: 0.8139 - val_loss: 0.9244 - val_accuracy: 0.8427\n",
            "Epoch 2/100\n",
            "5627/5627 [==============================] - 284s 50ms/step - loss: 0.9313 - accuracy: 0.8435 - val_loss: 0.9124 - val_accuracy: 0.8513\n",
            "Epoch 3/100\n",
            "5627/5627 [==============================] - 283s 50ms/step - loss: 0.9228 - accuracy: 0.8502 - val_loss: 0.9095 - val_accuracy: 0.8549\n",
            "Epoch 4/100\n",
            "5627/5627 [==============================] - 284s 50ms/step - loss: 0.9182 - accuracy: 0.8533 - val_loss: 0.9052 - val_accuracy: 0.8598\n",
            "Epoch 5/100\n",
            "5627/5627 [==============================] - 284s 51ms/step - loss: 0.9142 - accuracy: 0.8578 - val_loss: 0.9048 - val_accuracy: 0.8592\n",
            "Epoch 6/100\n",
            "5627/5627 [==============================] - 285s 51ms/step - loss: 0.9102 - accuracy: 0.8601 - val_loss: 0.9039 - val_accuracy: 0.8593\n",
            "Epoch 7/100\n",
            "5627/5627 [==============================] - 284s 50ms/step - loss: 0.9078 - accuracy: 0.8631 - val_loss: 0.9039 - val_accuracy: 0.8594\n",
            "Epoch 8/100\n",
            "5627/5627 [==============================] - 281s 50ms/step - loss: 0.9050 - accuracy: 0.8643 - val_loss: 0.9015 - val_accuracy: 0.8610\n",
            "Epoch 9/100\n",
            "5627/5627 [==============================] - 278s 49ms/step - loss: 0.9029 - accuracy: 0.8654 - val_loss: 0.9018 - val_accuracy: 0.8611\n",
            "Epoch 10/100\n",
            "5627/5627 [==============================] - 281s 50ms/step - loss: 0.9004 - accuracy: 0.8678 - val_loss: 0.8997 - val_accuracy: 0.8621\n",
            "Epoch 11/100\n",
            "5627/5627 [==============================] - 282s 50ms/step - loss: 0.8988 - accuracy: 0.8688 - val_loss: 0.8986 - val_accuracy: 0.8640\n",
            "Epoch 12/100\n",
            "5627/5627 [==============================] - 284s 50ms/step - loss: 0.8964 - accuracy: 0.8712 - val_loss: 0.8984 - val_accuracy: 0.8629\n",
            "Epoch 13/100\n",
            "5627/5627 [==============================] - 285s 51ms/step - loss: 0.8955 - accuracy: 0.8718 - val_loss: 0.8987 - val_accuracy: 0.8616\n",
            "Epoch 14/100\n",
            "5627/5627 [==============================] - 284s 51ms/step - loss: 0.8934 - accuracy: 0.8733 - val_loss: 0.8986 - val_accuracy: 0.8628\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 15/100\n",
            "5627/5627 [==============================] - 285s 51ms/step - loss: 0.8838 - accuracy: 0.8800 - val_loss: 0.8936 - val_accuracy: 0.8658\n",
            "Epoch 16/100\n",
            "5627/5627 [==============================] - 286s 51ms/step - loss: 0.8816 - accuracy: 0.8829 - val_loss: 0.8936 - val_accuracy: 0.8658\n",
            "Epoch 17/100\n",
            "5627/5627 [==============================] - 288s 51ms/step - loss: 0.8804 - accuracy: 0.8838 - val_loss: 0.8931 - val_accuracy: 0.8660\n",
            "Epoch 18/100\n",
            "5627/5627 [==============================] - 291s 52ms/step - loss: 0.8790 - accuracy: 0.8841 - val_loss: 0.8934 - val_accuracy: 0.8656\n",
            "Epoch 19/100\n",
            "5627/5627 [==============================] - 285s 51ms/step - loss: 0.8783 - accuracy: 0.8855 - val_loss: 0.8936 - val_accuracy: 0.8664\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 20/100\n",
            "5627/5627 [==============================] - 287s 51ms/step - loss: 0.8757 - accuracy: 0.8871 - val_loss: 0.8924 - val_accuracy: 0.8670\n",
            "Epoch 21/100\n",
            " 149/5627 [..............................] - ETA: 4:02 - loss: 0.8849 - accuracy: 0.8891"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p9Sa8q0-OPn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}